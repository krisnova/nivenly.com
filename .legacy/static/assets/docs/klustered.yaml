apiVersion: v1
items:
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2021-05-03T13:28:23Z"
    generateName: cilium-
    labels:
      controller-revision-hash: d4589f574
      k8s-app: cilium
      pod-template-generation: "1"
    name: cilium-7jxs4
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 389969c7-854b-4159-ac1a-42689975152a
    resourceVersion: "2623"
    uid: a8848d32-d612-4b53-a573-3e7373798e3c
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_FLANNEL_MASTER_DEVICE
        valueFrom:
          configMapKeyRef:
            key: flannel-master-device
            name: cilium-config
            optional: true
      - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT
        valueFrom:
          configMapKeyRef:
            key: flannel-uninstall-on-exit
            name: cilium-config
            optional: true
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: docker.io/cilium/cilium:v1.8.5
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9876
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9876
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-token-jh2gk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      - name: CILIUM_WAIT_BPF_MOUNT
        valueFrom:
          configMapKeyRef:
            key: wait-bpf-mount
            name: cilium-config
            optional: true
      image: docker.io/cilium/cilium:v1.8.5
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-token-jh2gk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 420
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: cilium-token-jh2gk
      secret:
        defaultMode: 420
        secretName: cilium-token-jh2gk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4d1322e45a11ce6fd5d77e9e1747e18a6c57f4b8f7f263be0fa4831c1b4120be
      image: docker.io/cilium/cilium:v1.8.5
      imageID: docker.io/cilium/cilium@sha256:3cbade4c15e7deba6b9a770475beec5e102a9f8e347c28307383a24581ebb6c4
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:34Z"
    hostIP: 10.25.32.135
    initContainerStatuses:
    - containerID: containerd://7a85a16ccc94f6d3ddced1f058cc161f64842b35bfddcfa8e07de149d6eac08e
      image: docker.io/cilium/cilium:v1.8.5
      imageID: docker.io/cilium/cilium@sha256:3cbade4c15e7deba6b9a770475beec5e102a9f8e347c28307383a24581ebb6c4
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://7a85a16ccc94f6d3ddced1f058cc161f64842b35bfddcfa8e07de149d6eac08e
          exitCode: 0
          finishedAt: "2021-05-03T13:28:33Z"
          reason: Completed
          startedAt: "2021-05-03T13:28:33Z"
    phase: Running
    podIP: 145.40.93.189
    podIPs:
    - ip: 145.40.93.189
    qosClass: Burstable
    startTime: "2021-05-03T13:28:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2021-05-03T13:28:23Z"
    generateName: cilium-
    labels:
      controller-revision-hash: d4589f574
      k8s-app: cilium
      pod-template-generation: "1"
    name: cilium-c7pw9
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 389969c7-854b-4159-ac1a-42689975152a
    resourceVersion: "2584"
    uid: d373c560-bf2a-4062-960a-e84cf53f3bd2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_FLANNEL_MASTER_DEVICE
        valueFrom:
          configMapKeyRef:
            key: flannel-master-device
            name: cilium-config
            optional: true
      - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT
        valueFrom:
          configMapKeyRef:
            key: flannel-uninstall-on-exit
            name: cilium-config
            optional: true
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: docker.io/cilium/cilium:v1.8.5
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9876
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9876
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-token-jh2gk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      - name: CILIUM_WAIT_BPF_MOUNT
        valueFrom:
          configMapKeyRef:
            key: wait-bpf-mount
            name: cilium-config
            optional: true
      image: docker.io/cilium/cilium:v1.8.5
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-token-jh2gk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 420
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: cilium-token-jh2gk
      secret:
        defaultMode: 420
        secretName: cilium-token-jh2gk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:03Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:03Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9e0203db09809be9ac72bcfb682e0c9413fa7ba6af69bf24841459f8e9415c1c
      image: docker.io/cilium/cilium:v1.8.5
      imageID: docker.io/cilium/cilium@sha256:3cbade4c15e7deba6b9a770475beec5e102a9f8e347c28307383a24581ebb6c4
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:34Z"
    hostIP: 10.25.32.131
    initContainerStatuses:
    - containerID: containerd://105cd9079769eb8be582133ed67b43d4157418f440eceb046bfd7364fa1ec3c2
      image: docker.io/cilium/cilium:v1.8.5
      imageID: docker.io/cilium/cilium@sha256:3cbade4c15e7deba6b9a770475beec5e102a9f8e347c28307383a24581ebb6c4
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://105cd9079769eb8be582133ed67b43d4157418f440eceb046bfd7364fa1ec3c2
          exitCode: 0
          finishedAt: "2021-05-03T13:28:34Z"
          reason: Completed
          startedAt: "2021-05-03T13:28:34Z"
    phase: Running
    podIP: 145.40.93.185
    podIPs:
    - ip: 145.40.93.185
    qosClass: Burstable
    startTime: "2021-05-03T13:28:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:28:24Z"
    generateName: cilium-operator-648569fbb8-
    labels:
      io.cilium/app: operator
      name: cilium-operator
      pod-template-hash: 648569fbb8
    name: cilium-operator-648569fbb8-8npvm
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cilium-operator-648569fbb8
      uid: bfd77510-3944-4f42-a384-238db1943b68
    resourceVersion: "2662"
    uid: 2bebca7b-6686-4edb-ac18-987bb7583aff
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: io.cilium/app
              operator: In
              values:
              - operator
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --debug=$(CILIUM_DEBUG)
      command:
      - cilium-operator-generic
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_DEBUG
        valueFrom:
          configMapKeyRef:
            key: debug
            name: cilium-config
            optional: true
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            key: AWS_ACCESS_KEY_ID
            name: cilium-aws
            optional: true
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            key: AWS_SECRET_ACCESS_KEY
            name: cilium-aws
            optional: true
      - name: AWS_DEFAULT_REGION
        valueFrom:
          secretKeyRef:
            key: AWS_DEFAULT_REGION
            name: cilium-aws
            optional: true
      image: docker.io/cilium/operator-generic:v1.8.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 9234
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: cilium-operator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-operator-token-sh2sf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium-operator
    serviceAccountName: cilium-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: cilium-operator-token-sh2sf
      secret:
        defaultMode: 420
        secretName: cilium-operator-token-sh2sf
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://829caf45c5501666e32847f76e995d47a5df6d059042c74c476b9dd722da1c9c
      image: docker.io/cilium/operator-generic:v1.8.5
      imageID: docker.io/cilium/operator-generic@sha256:f20cde7e8be13a49882cfa32bcd7d07fec685fc80201818de5b12e1d81375807
      lastState: {}
      name: cilium-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:36Z"
    hostIP: 10.25.32.135
    phase: Running
    podIP: 145.40.93.189
    podIPs:
    - ip: 145.40.93.189
    qosClass: BestEffort
    startTime: "2021-05-03T13:28:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:28:24Z"
    generateName: cilium-operator-648569fbb8-
    labels:
      io.cilium/app: operator
      name: cilium-operator
      pod-template-hash: 648569fbb8
    name: cilium-operator-648569fbb8-lwj4l
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: cilium-operator-648569fbb8
      uid: bfd77510-3944-4f42-a384-238db1943b68
    resourceVersion: "2611"
    uid: 96f85cf3-b2f5-49ce-bda4-9359673f5ab2
  spec:
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: io.cilium/app
              operator: In
              values:
              - operator
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      - --debug=$(CILIUM_DEBUG)
      command:
      - cilium-operator-generic
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_DEBUG
        valueFrom:
          configMapKeyRef:
            key: debug
            name: cilium-config
            optional: true
      - name: AWS_ACCESS_KEY_ID
        valueFrom:
          secretKeyRef:
            key: AWS_ACCESS_KEY_ID
            name: cilium-aws
            optional: true
      - name: AWS_SECRET_ACCESS_KEY
        valueFrom:
          secretKeyRef:
            key: AWS_SECRET_ACCESS_KEY
            name: cilium-aws
            optional: true
      - name: AWS_DEFAULT_REGION
        valueFrom:
          secretKeyRef:
            key: AWS_DEFAULT_REGION
            name: cilium-aws
            optional: true
      image: docker.io/cilium/operator-generic:v1.8.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 9234
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 3
      name: cilium-operator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-operator-token-sh2sf
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium-operator
    serviceAccountName: cilium-operator
    terminationGracePeriodSeconds: 30
    tolerations:
    - operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: cilium-operator-token-sh2sf
      secret:
        defaultMode: 420
        secretName: cilium-operator-token-sh2sf
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:24Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9b2a7b4dc1b995c068f8a7f2d717af56a039f0e31f89d81b8d7c78822f3d757b
      image: docker.io/cilium/operator-generic:v1.8.5
      imageID: docker.io/cilium/operator-generic@sha256:f20cde7e8be13a49882cfa32bcd7d07fec685fc80201818de5b12e1d81375807
      lastState: {}
      name: cilium-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:36Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 145.40.93.185
    podIPs:
    - ip: 145.40.93.185
    qosClass: BestEffort
    startTime: "2021-05-03T13:28:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2021-05-03T13:28:23Z"
    generateName: cilium-
    labels:
      controller-revision-hash: d4589f574
      k8s-app: cilium
      pod-template-generation: "1"
    name: cilium-t4snh
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 389969c7-854b-4159-ac1a-42689975152a
    resourceVersion: "2577"
    uid: 97b0ed83-fbf3-4630-932a-93905d7b2a01
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_FLANNEL_MASTER_DEVICE
        valueFrom:
          configMapKeyRef:
            key: flannel-master-device
            name: cilium-config
            optional: true
      - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT
        valueFrom:
          configMapKeyRef:
            key: flannel-uninstall-on-exit
            name: cilium-config
            optional: true
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: docker.io/cilium/cilium:v1.8.5
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9876
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9876
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-token-jh2gk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      - name: CILIUM_WAIT_BPF_MOUNT
        valueFrom:
          configMapKeyRef:
            key: wait-bpf-mount
            name: cilium-config
            optional: true
      image: docker.io/cilium/cilium:v1.8.5
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-token-jh2gk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 420
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: cilium-token-jh2gk
      secret:
        defaultMode: 420
        secretName: cilium-token-jh2gk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9551d75bdb46aedc26712d4baca178e33dad73ff99f219a3a508c878eb68937d
      image: docker.io/cilium/cilium:v1.8.5
      imageID: docker.io/cilium/cilium@sha256:3cbade4c15e7deba6b9a770475beec5e102a9f8e347c28307383a24581ebb6c4
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:34Z"
    hostIP: 10.25.32.129
    initContainerStatuses:
    - containerID: containerd://f0a0f60ad6ab52856cc272fe14c1ee97b074ae369e16887fea73cedfddb6c411
      image: docker.io/cilium/cilium:v1.8.5
      imageID: docker.io/cilium/cilium@sha256:3cbade4c15e7deba6b9a770475beec5e102a9f8e347c28307383a24581ebb6c4
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://f0a0f60ad6ab52856cc272fe14c1ee97b074ae369e16887fea73cedfddb6c411
          exitCode: 0
          finishedAt: "2021-05-03T13:28:33Z"
          reason: Completed
          startedAt: "2021-05-03T13:28:33Z"
    phase: Running
    podIP: 145.40.93.163
    podIPs:
    - ip: 145.40.93.163
    qosClass: Burstable
    startTime: "2021-05-03T13:28:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2021-05-03T13:28:23Z"
    generateName: cilium-
    labels:
      controller-revision-hash: d4589f574
      k8s-app: cilium
      pod-template-generation: "1"
    name: cilium-tw4pj
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: cilium
      uid: 389969c7-854b-4159-ac1a-42689975152a
    resourceVersion: "2582"
    uid: 4d0aff94-4096-4fbc-8123-ff7f20ad803f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --config-dir=/tmp/cilium/config-map
      command:
      - cilium-agent
      env:
      - name: K8S_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: CILIUM_K8S_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CILIUM_FLANNEL_MASTER_DEVICE
        valueFrom:
          configMapKeyRef:
            key: flannel-master-device
            name: cilium-config
            optional: true
      - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT
        valueFrom:
          configMapKeyRef:
            key: flannel-uninstall-on-exit
            name: cilium-config
            optional: true
      - name: CILIUM_CLUSTERMESH_CONFIG
        value: /var/lib/cilium/clustermesh/
      - name: CILIUM_CNI_CHAINING_MODE
        valueFrom:
          configMapKeyRef:
            key: cni-chaining-mode
            name: cilium-config
            optional: true
      - name: CILIUM_CUSTOM_CNI_CONF
        valueFrom:
          configMapKeyRef:
            key: custom-cni-conf
            name: cilium-config
            optional: true
      image: docker.io/cilium/cilium:v1.8.5
      imagePullPolicy: IfNotPresent
      lifecycle:
        postStart:
          exec:
            command:
            - /cni-install.sh
            - --enable-debug=false
        preStop:
          exec:
            command:
            - /cni-uninstall.sh
      livenessProbe:
        failureThreshold: 10
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9876
          scheme: HTTP
        initialDelaySeconds: 120
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      name: cilium-agent
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 127.0.0.1
          httpHeaders:
          - name: brief
            value: "true"
          path: /healthz
          port: 9876
          scheme: HTTP
        initialDelaySeconds: 5
        periodSeconds: 30
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
          - SYS_MODULE
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /host/opt/cni/bin
        name: cni-path
      - mountPath: /host/etc/cni/net.d
        name: etc-cni-netd
      - mountPath: /var/lib/cilium/clustermesh
        name: clustermesh-secrets
        readOnly: true
      - mountPath: /tmp/cilium/config-map
        name: cilium-config-path
        readOnly: true
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-token-jh2gk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    initContainers:
    - command:
      - /init-container.sh
      env:
      - name: CILIUM_ALL_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-state
            name: cilium-config
            optional: true
      - name: CILIUM_BPF_STATE
        valueFrom:
          configMapKeyRef:
            key: clean-cilium-bpf-state
            name: cilium-config
            optional: true
      - name: CILIUM_WAIT_BPF_MOUNT
        valueFrom:
          configMapKeyRef:
            key: wait-bpf-mount
            name: cilium-config
            optional: true
      image: docker.io/cilium/cilium:v1.8.5
      imagePullPolicy: IfNotPresent
      name: clean-cilium-state
      resources:
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        capabilities:
          add:
          - NET_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /sys/fs/bpf
        mountPropagation: HostToContainer
        name: bpf-maps
      - mountPath: /var/run/cilium
        name: cilium-run
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cilium-token-jh2gk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cilium
    serviceAccountName: cilium
    terminationGracePeriodSeconds: 1
    tolerations:
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: DirectoryOrCreate
      name: cilium-run
    - hostPath:
        path: /sys/fs/bpf
        type: DirectoryOrCreate
      name: bpf-maps
    - hostPath:
        path: /opt/cni/bin
        type: DirectoryOrCreate
      name: cni-path
    - hostPath:
        path: /etc/cni/net.d
        type: DirectoryOrCreate
      name: etc-cni-netd
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - name: clustermesh-secrets
      secret:
        defaultMode: 420
        optional: true
        secretName: cilium-clustermesh
    - configMap:
        defaultMode: 420
        name: cilium-config
      name: cilium-config-path
    - name: cilium-token-jh2gk
      secret:
        defaultMode: 420
        secretName: cilium-token-jh2gk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:34Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:23Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://dbe7ff80085dbfe71e0399d229f402844dfdf839f17facb66504d37c7280431a
      image: docker.io/cilium/cilium:v1.8.5
      imageID: docker.io/cilium/cilium@sha256:3cbade4c15e7deba6b9a770475beec5e102a9f8e347c28307383a24581ebb6c4
      lastState: {}
      name: cilium-agent
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:34Z"
    hostIP: 10.25.32.133
    initContainerStatuses:
    - containerID: containerd://2704b70fa1569b1e86d3559abafade8ba897ed7dfd76defbee438c793afcdf6c
      image: docker.io/cilium/cilium:v1.8.5
      imageID: docker.io/cilium/cilium@sha256:3cbade4c15e7deba6b9a770475beec5e102a9f8e347c28307383a24581ebb6c4
      lastState: {}
      name: clean-cilium-state
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://2704b70fa1569b1e86d3559abafade8ba897ed7dfd76defbee438c793afcdf6c
          exitCode: 0
          finishedAt: "2021-05-03T13:28:34Z"
          reason: Completed
          startedAt: "2021-05-03T13:28:34Z"
    phase: Running
    podIP: 145.40.93.187
    podIPs:
    - ip: 145.40.93.187
    qosClass: Burstable
    startTime: "2021-05-03T13:28:23Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:28:24Z"
    generateName: hubble-relay-5f65cc9b96-
    labels:
      k8s-app: hubble-relay
      pod-template-hash: 5f65cc9b96
    name: hubble-relay-5f65cc9b96-cl9dn
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hubble-relay-5f65cc9b96
      uid: 7dfb07cd-a1a0-4ecb-9fec-703bcbd14ae8
    resourceVersion: "2504"
    uid: 6ce19c9c-2815-4467-bd22-594576e034e7
  spec:
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: k8s-app
              operator: In
              values:
              - cilium
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - serve
      - --peer-service=unix:///var/run/cilium/hubble.sock
      - --listen-address=:4245
      command:
      - hubble-relay
      image: docker.io/cilium/hubble-relay:v1.8.5
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: grpc
        timeoutSeconds: 1
      name: hubble-relay
      ports:
      - containerPort: 4245
        name: grpc
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        periodSeconds: 10
        successThreshold: 1
        tcpSocket:
          port: grpc
        timeoutSeconds: 1
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/cilium
        name: hubble-sock-dir
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: hubble-relay-token-dvg86
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: hubble-relay
    serviceAccountName: hubble-relay
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/run/cilium
        type: Directory
      name: hubble-sock-dir
    - name: hubble-relay-token-dvg86
      secret:
        defaultMode: 420
        secretName: hubble-relay-token-dvg86
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:06Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:27Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:27Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:06Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://aaabdb81e63ca29d1cdcdd28acce10aa14192d22e6ea027085b3a22d6b7c2851
      image: docker.io/cilium/hubble-relay:v1.8.5
      imageID: docker.io/cilium/hubble-relay@sha256:41a5a8fac5f1a8d167b77d11a3ce39204978c6de02367de2e35dd22d84536da8
      lastState: {}
      name: hubble-relay
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:22Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 192.168.1.46
    podIPs:
    - ip: 192.168.1.46
    qosClass: BestEffort
    startTime: "2021-05-03T13:29:06Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:28:24Z"
    generateName: hubble-ui-7854cf65dc-
    labels:
      k8s-app: hubble-ui
      pod-template-hash: 7854cf65dc
    name: hubble-ui-7854cf65dc-w89lp
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: hubble-ui-7854cf65dc
      uid: aa40d7e3-aeca-465d-98cd-e1fd04a181da
    resourceVersion: "2473"
    uid: 7960dc63-b904-4c32-99e1-128b895f7133
  spec:
    containers:
    - env:
      - name: NODE_ENV
        value: production
      - name: LOG_LEVEL
        value: info
      - name: HUBBLE
        value: "true"
      - name: HUBBLE_SERVICE
        value: hubble-relay
      - name: HUBBLE_PORT
        value: "80"
      image: quay.io/cilium/hubble-ui:v0.6.1
      imagePullPolicy: IfNotPresent
      name: hubble-ui
      ports:
      - containerPort: 12000
        name: http
        protocol: TCP
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: hubble-ui-token-jw89g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsUser: 1001
    serviceAccount: hubble-ui
    serviceAccountName: hubble-ui
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: hubble-ui-token-jw89g
      secret:
        defaultMode: 420
        secretName: hubble-ui-token-jw89g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0f38e3a9a016b898a5f1024a7dc9e4fe4afe3372763f454e86d1df284c1eb22f
      image: quay.io/cilium/hubble-ui:v0.6.1
      imageID: quay.io/cilium/hubble-ui@sha256:e4c599595dbab5b6fd7a182f9992ad03af8ff091d76008215eb4ec20c50eb876
      lastState: {}
      name: hubble-ui
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:21Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 192.168.0.13
    podIPs:
    - ip: 192.168.0.13
    qosClass: BestEffort
    startTime: "2021-05-03T13:29:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:28:45Z"
    generateName: klustered-69b868776d-
    labels:
      app: klustered
      pod-template-hash: 69b868776d
    name: klustered-69b868776d-28ck5
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: klustered-69b868776d
      uid: 44c5a3e5-c83e-47f0-8da8-75f4811b983b
    resourceVersion: "2512"
    uid: 855a7d75-5ad1-4938-a49a-6057e85e411d
  spec:
    containers:
    - image: ghcr.io/rawkode/klustered:v1
      imagePullPolicy: Always
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 1
      name: klustered
      ports:
      - containerPort: 8080
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 2
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          cpu: 500m
          memory: 128Mi
        requests:
          cpu: 500m
          memory: 128Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-rn99c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: default-token-rn99c
      secret:
        defaultMode: 420
        secretName: default-token-rn99c
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4f9e1a076e6b4b486f4237780c93f502b8215c74ebb0d4aee5fd3a726ca4cb7e
      image: ghcr.io/rawkode/klustered:v1
      imageID: ghcr.io/rawkode/klustered@sha256:ef9ccec3e02668be9b7ee5450d3f964aedb7ad0aa68b55ca2483596ffa5dc2ac
      lastState: {}
      name: klustered
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:26Z"
    hostIP: 10.25.32.135
    phase: Running
    podIP: 192.168.7.183
    podIPs:
    - ip: 192.168.7.183
    qosClass: Guaranteed
    startTime: "2021-05-03T13:29:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:28:46Z"
    generateName: postgresql-
    labels:
      app: postgresql
      controller-revision-hash: postgresql-fff6bd679
      statefulset.kubernetes.io/pod-name: postgresql-0
    name: postgresql-0
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: StatefulSet
      name: postgresql
      uid: 2c013aca-3caa-4ea4-8527-389419186036
    resourceVersion: "2452"
    uid: 0363a28e-454e-4733-a2e6-d82f174c4e46
  spec:
    containers:
    - env:
      - name: POSTGRES_USER
        value: postgres
      - name: POSTGRES_DB
        value: klustered
      - name: POSTGRES_PASSWORD
        value: postgresql123
      image: postgres:13-alpine
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 2
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      name: postgresql
      ports:
      - containerPort: 5432
        name: psql
        protocol: TCP
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
        failureThreshold: 2
        initialDelaySeconds: 5
        periodSeconds: 5
        successThreshold: 1
        timeoutSeconds: 5
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /docker-entrypoint-initdb.d
        name: init
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-rn99c
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostname: postgresql-0
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    subdomain: postgresql
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        name: postgresql
      name: init
    - name: default-token-rn99c
      secret:
        defaultMode: 420
        secretName: default-token-rn99c
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:02Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:17Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:17Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:02Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c06f2fcd2e2ed700bdd2a13dd2e0eb4ad74a4e9c3984acc1d6c3cb8f6cb7d5f6
      image: docker.io/library/postgres:13-alpine
      imageID: docker.io/library/postgres@sha256:c4c7a1585974706b5f72b8ab595e47399b23b2e03d93bbf75c1b0904be1803dc
      lastState: {}
      name: postgresql
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:10Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 192.168.0.11
    podIPs:
    - ip: 192.168.0.11
    qosClass: BestEffort
    startTime: "2021-05-03T13:29:02Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:15:07Z"
    generateName: coredns-74ff55c5b-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 74ff55c5b
    name: coredns-74ff55c5b-r8swd
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-74ff55c5b
      uid: 8b04fffe-675d-4f90-8a47-289ed5e44bec
    resourceVersion: "2382"
    uid: b1943de0-f1b3-4f56-8125-bce677c7c708
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: k8s.gcr.io/coredns:1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: coredns-token-jhjft
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: coredns-token-jhjft
      secret:
        defaultMode: 420
        secretName: coredns-token-jhjft
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:06Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:06Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://deea8b63e4ea9cfc812fc65d873782174599d90b29e83e4959eef224eabc3b55
      image: k8s.gcr.io/coredns:1.7.0
      imageID: k8s.gcr.io/coredns@sha256:73ca82b4ce829766d4f1f10947c3a338888f876fbed0540dc849c89ff256e90c
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:56Z"
    hostIP: 10.25.32.129
    phase: Running
    podIP: 192.168.3.89
    podIPs:
    - ip: 192.168.3.89
    qosClass: Burstable
    startTime: "2021-05-03T13:28:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:15:07Z"
    generateName: coredns-74ff55c5b-
    labels:
      k8s-app: kube-dns
      pod-template-hash: 74ff55c5b
    name: coredns-74ff55c5b-t59rw
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: coredns-74ff55c5b
      uid: 8b04fffe-675d-4f90-8a47-289ed5e44bec
    resourceVersion: "2399"
    uid: 309ae956-c587-4fc7-be00-b48ef1388fe6
  spec:
    containers:
    - args:
      - -conf
      - /etc/coredns/Corefile
      image: k8s.gcr.io/coredns:1.7.0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 5
        httpGet:
          path: /health
          port: 8080
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 5
      name: coredns
      ports:
      - containerPort: 53
        name: dns
        protocol: UDP
      - containerPort: 53
        name: dns-tcp
        protocol: TCP
      - containerPort: 9153
        name: metrics
        protocol: TCP
      readinessProbe:
        failureThreshold: 3
        httpGet:
          path: /ready
          port: 8181
          scheme: HTTP
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      resources:
        limits:
          memory: 170Mi
        requests:
          cpu: 100m
          memory: 70Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_BIND_SERVICE
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/coredns
        name: config-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: coredns-token-jhjft
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000000000
    priorityClassName: system-cluster-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: coredns
    serviceAccountName: coredns
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: Corefile
          path: Corefile
        name: coredns
      name: config-volume
    - name: coredns-token-jhjft
      secret:
        defaultMode: 420
        secretName: coredns-token-jhjft
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:07Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:07Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1330f1a77b97d6ac1f34c297c8d66d0dedabe66cf5d8e80210652d485ed3840d
      image: k8s.gcr.io/coredns:1.7.0
      imageID: k8s.gcr.io/coredns@sha256:73ca82b4ce829766d4f1f10947c3a338888f876fbed0540dc849c89ff256e90c
      lastState: {}
      name: coredns
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:56Z"
    hostIP: 10.25.32.129
    phase: Running
    podIP: 192.168.2.112
    podIPs:
    - ip: 192.168.2.112
    qosClass: Burstable
    startTime: "2021-05-03T13:28:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/etcd.advertise-client-urls: https://145.40.93.163:2379
      kubernetes.io/config.hash: 03855081e61d9bb9a0ccc48bc7f72570
      kubernetes.io/config.mirror: 03855081e61d9bb9a0ccc48bc7f72570
      kubernetes.io/config.seen: "2021-05-03T13:14:57.122047578Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2021-05-03T13:15:03Z"
    labels:
      component: etcd
      tier: control-plane
    name: etcd-kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
      uid: 962e1cf3-2712-486b-a1d1-f3fa6170dbad
    resourceVersion: "2503"
    uid: af90d7fe-2379-4f1a-82f0-2fb91f8a7aa5
  spec:
    containers:
    - command:
      - etcd
      - --advertise-client-urls=https://145.40.93.163:2379
      - --cert-file=/etc/kubernetes/pki/etcd/server.crt
      - --client-cert-auth=true
      - --data-dir=/var/lib/etcd
      - --initial-advertise-peer-urls=https://145.40.93.163:2380
      - --initial-cluster=kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7=https://145.40.93.163:2380
      - --key-file=/etc/kubernetes/pki/etcd/server.key
      - --listen-client-urls=https://127.0.0.1:2379,https://145.40.93.163:2379
      - --listen-metrics-urls=http://127.0.0.1:2381
      - --listen-peer-urls=https://145.40.93.163:2380
      - --name=kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
      - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
      - --peer-client-cert-auth=true
      - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key
      - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      - --snapshot-count=10000
      - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
      image: k8s.gcr.io/etcd:3.4.13-0
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /health
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: etcd
      resources:
        requests:
          cpu: 100m
          ephemeral-storage: 100Mi
          memory: 100Mi
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /health
          port: 2381
          scheme: HTTP
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/etcd
        name: etcd-data
      - mountPath: /etc/kubernetes/pki/etcd
        name: etcd-certs
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/pki/etcd
        type: DirectoryOrCreate
      name: etcd-certs
    - hostPath:
        path: /var/lib/etcd
        type: DirectoryOrCreate
      name: etcd-data
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:16:15Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:16:15Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1fe4925602e11de65e5b76701b586de74877bebd5f082393ea80aa43f1292fb6
      image: k8s.gcr.io/etcd:3.4.13-0
      imageID: k8s.gcr.io/etcd@sha256:4ad90a11b55313b182afc186b9876c8e891531b8db4c9bf1541953021618d0e2
      lastState: {}
      name: etcd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:14:36Z"
    hostIP: 10.25.32.129
    phase: Running
    podIP: 145.40.93.163
    podIPs:
    - ip: 145.40.93.163
    qosClass: Burstable
    startTime: "2021-05-03T13:15:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 145.40.93.163:6443
      kubernetes.io/config.hash: cca1556e6d54b02a72e085d883ae3ed9
      kubernetes.io/config.mirror: cca1556e6d54b02a72e085d883ae3ed9
      kubernetes.io/config.seen: "2021-05-03T13:14:57.122053980Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2021-05-03T13:15:03Z"
    labels:
      component: kube-apiserver
      tier: control-plane
    name: kube-apiserver-kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
      uid: 962e1cf3-2712-486b-a1d1-f3fa6170dbad
    resourceVersion: "2517"
    uid: 6e2d5055-4de5-4538-ab96-5b3e0162f233
  spec:
    containers:
    - command:
      - kube-apiserver
      - --advertise-address=145.40.93.163
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --cloud-provider=external
      - --enable-admission-plugins=NodeRestriction
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --insecure-port=0
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
      - --requestheader-allowed-names=front-proxy-client
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --requestheader-extra-headers-prefix=X-Remote-Extra-
      - --requestheader-group-headers=X-Remote-Group
      - --requestheader-username-headers=X-Remote-User
      - --secure-port=6443
      - --service-account-issuer=https://kubernetes.default.svc.cluster.local
      - --service-account-key-file=/etc/kubernetes/pki/sa.pub
      - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=172.26.0.0/16
      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
      image: k8s.gcr.io/kube-apiserver:v1.20.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 145.40.93.163
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-apiserver
      readinessProbe:
        failureThreshold: 3
        httpGet:
          host: 145.40.93.163
          path: /readyz
          port: 6443
          scheme: HTTPS
        periodSeconds: 1
        successThreshold: 1
        timeoutSeconds: 15
      resources:
        requests:
          cpu: 250m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 145.40.93.163
          path: /livez
          port: 6443
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/pki
        name: etc-pki
        readOnly: true
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/pki
        type: DirectoryOrCreate
      name: etc-pki
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://96c364cc490f6895bc67c4d0e4699bd7ee22cef8fa7b1ccb6fc81267d47fe5b9
      image: k8s.gcr.io/kube-apiserver:v1.20.4
      imageID: k8s.gcr.io/kube-apiserver@sha256:adef5d31ea2fcf9c523e47bbcc6a955f3c247abbd8a9a97f4a26fdeb18f9b4b8
      lastState: {}
      name: kube-apiserver
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:14:46Z"
    hostIP: 10.25.32.129
    phase: Running
    podIP: 145.40.93.163
    podIPs:
    - ip: 145.40.93.163
    qosClass: Burstable
    startTime: "2021-05-03T13:15:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: b6b3e36e2688445b54405872d9b15f81
      kubernetes.io/config.mirror: b6b3e36e2688445b54405872d9b15f81
      kubernetes.io/config.seen: "2021-05-03T13:14:57.122055673Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2021-05-03T13:15:03Z"
    labels:
      component: kube-controller-manager
      tier: control-plane
    name: kube-controller-manager-kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
      uid: 962e1cf3-2712-486b-a1d1-f3fa6170dbad
    resourceVersion: "2636"
    uid: 7198fe60-b078-468c-bed6-ba4f143a1295
  spec:
    containers:
    - command:
      - kube-controller-manager
      - --allocate-node-cidrs=true
      - --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf
      - --bind-address=127.0.0.1
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --cloud-provider=external
      - --cluster-cidr=192.168.0.0/16
      - --cluster-name=kluster-022-krisnova-kubecon-eu-21
      - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt
      - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key
      - --controllers=*,bootstrapsigner,tokencleaner
      - --kubeconfig=/etc/kubernetes/controller-manager.conf
      - --leader-elect=true
      - --port=0
      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
      - --root-ca-file=/etc/kubernetes/pki/ca.crt
      - --service-account-private-key-file=/etc/kubernetes/pki/sa.key
      - --service-cluster-ip-range=172.26.0.0/16
      - --use-service-account-credentials=true
      image: k8s.gcr.io/kube-controller-manager:v1.20.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-controller-manager
      resources:
        requests:
          cpu: 200m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10257
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ssl/certs
        name: ca-certs
        readOnly: true
      - mountPath: /etc/ca-certificates
        name: etc-ca-certificates
        readOnly: true
      - mountPath: /etc/pki
        name: etc-pki
        readOnly: true
      - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        name: flexvolume-dir
      - mountPath: /etc/kubernetes/pki
        name: k8s-certs
        readOnly: true
      - mountPath: /etc/kubernetes/controller-manager.conf
        name: kubeconfig
        readOnly: true
      - mountPath: /usr/local/share/ca-certificates
        name: usr-local-share-ca-certificates
        readOnly: true
      - mountPath: /usr/share/ca-certificates
        name: usr-share-ca-certificates
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/ssl/certs
        type: DirectoryOrCreate
      name: ca-certs
    - hostPath:
        path: /etc/ca-certificates
        type: DirectoryOrCreate
      name: etc-ca-certificates
    - hostPath:
        path: /etc/pki
        type: DirectoryOrCreate
      name: etc-pki
    - hostPath:
        path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
        type: DirectoryOrCreate
      name: flexvolume-dir
    - hostPath:
        path: /etc/kubernetes/pki
        type: DirectoryOrCreate
      name: k8s-certs
    - hostPath:
        path: /etc/kubernetes/controller-manager.conf
        type: FileOrCreate
      name: kubeconfig
    - hostPath:
        path: /usr/local/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-local-share-ca-certificates
    - hostPath:
        path: /usr/share/ca-certificates
        type: DirectoryOrCreate
      name: usr-share-ca-certificates
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:16:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:16:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://80d1fe012bde86253cbf9009ddfed8f02be7ebbb90e1ec560d2292aedc0081d7
      image: k8s.gcr.io/kube-controller-manager:v1.20.4
      imageID: k8s.gcr.io/kube-controller-manager@sha256:37a251ee07e626fb34dfcd40957a92cf1711e22fade74295f03de8cb6a842845
      lastState: {}
      name: kube-controller-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:14:17Z"
    hostIP: 10.25.32.129
    phase: Running
    podIP: 145.40.93.163
    podIPs:
    - ip: 145.40.93.163
    qosClass: Burstable
    startTime: "2021-05-03T13:15:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:15:07Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 9978ddf98
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-bg2tf
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: e8921f46-1f52-4fc1-a44c-ff042c7d442f
    resourceVersion: "2377"
    uid: a10176e9-72b8-491f-b83d-77ea61aee6ed
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/kube-proxy:v1.20.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-2zf6f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-2zf6f
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-2zf6f
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:13Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:14Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:14Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:13Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cace322c2cff41c91f7a750ccc4203f8159714322078e382c302edbbcb85ae73
      image: k8s.gcr.io/kube-proxy:v1.20.4
      imageID: k8s.gcr.io/kube-proxy@sha256:454050a55141f5f77ef30a6d2aa8ee41c6dc03e307f4f2558498099493087cc4
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:15:13Z"
    hostIP: 10.25.32.129
    phase: Running
    podIP: 145.40.93.163
    podIPs:
    - ip: 145.40.93.163
    qosClass: BestEffort
    startTime: "2021-05-03T13:15:13Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:19:42Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 9978ddf98
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-bq96g
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: e8921f46-1f52-4fc1-a44c-ff042c7d442f
    resourceVersion: "2465"
    uid: 1d266c3e-3a91-4fb8-b514-90c26919cd03
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/kube-proxy:v1.20.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-2zf6f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-2zf6f
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-2zf6f
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:42Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:48Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:48Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:42Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://302a2e7198c91b596786e82fdd3ea574913f74f75e8e647cf8bad9638af18c92
      image: k8s.gcr.io/kube-proxy:v1.20.4
      imageID: k8s.gcr.io/kube-proxy@sha256:454050a55141f5f77ef30a6d2aa8ee41c6dc03e307f4f2558498099493087cc4
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:19:47Z"
    hostIP: 10.25.32.135
    phase: Running
    podIP: 145.40.93.189
    podIPs:
    - ip: 145.40.93.189
    qosClass: BestEffort
    startTime: "2021-05-03T13:19:42Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:19:50Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 9978ddf98
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-s9q47
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: e8921f46-1f52-4fc1-a44c-ff042c7d442f
    resourceVersion: "2657"
    uid: 5dcd1741-3c68-43a7-a2df-49e09a884b9a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/kube-proxy:v1.20.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-2zf6f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-2zf6f
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-2zf6f
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:56Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:56Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://b29a9f9518d55e3b53f0e5afc08c4860dc86ce456ce7284f1ba2c1402c2e754d
      image: k8s.gcr.io/kube-proxy:v1.20.4
      imageID: k8s.gcr.io/kube-proxy@sha256:454050a55141f5f77ef30a6d2aa8ee41c6dc03e307f4f2558498099493087cc4
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:19:55Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 145.40.93.187
    podIPs:
    - ip: 145.40.93.187
    qosClass: BestEffort
    startTime: "2021-05-03T13:19:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:19:50Z"
    generateName: kube-proxy-
    labels:
      controller-revision-hash: 9978ddf98
      k8s-app: kube-proxy
      pod-template-generation: "1"
    name: kube-proxy-ztm89
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: kube-proxy
      uid: e8921f46-1f52-4fc1-a44c-ff042c7d442f
    resourceVersion: "2689"
    uid: f480ba72-94d7-4c32-8c22-69ccbf0a45df
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    containers:
    - command:
      - /usr/local/bin/kube-proxy
      - --config=/var/lib/kube-proxy/config.conf
      - --hostname-override=$(NODE_NAME)
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/kube-proxy:v1.20.4
      imagePullPolicy: IfNotPresent
      name: kube-proxy
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/kube-proxy
        name: kube-proxy
      - mountPath: /run/xtables.lock
        name: xtables-lock
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: kube-proxy-token-2zf6f
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: kube-proxy
    serviceAccountName: kube-proxy
    terminationGracePeriodSeconds: 30
    tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - configMap:
        defaultMode: 420
        name: kube-proxy
      name: kube-proxy
    - hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
      name: xtables-lock
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - name: kube-proxy-token-2zf6f
      secret:
        defaultMode: 420
        secretName: kube-proxy-token-2zf6f
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:51Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:19:50Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5283e8d5e0493099e9a42a776e3c3f46ec68d3eab1d26aed3cb323141cff7906
      image: k8s.gcr.io/kube-proxy:v1.20.4
      imageID: k8s.gcr.io/kube-proxy@sha256:454050a55141f5f77ef30a6d2aa8ee41c6dc03e307f4f2558498099493087cc4
      lastState: {}
      name: kube-proxy
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:19:56Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 145.40.93.185
    podIPs:
    - ip: 145.40.93.185
    qosClass: BestEffort
    startTime: "2021-05-03T13:19:51Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      kubernetes.io/config.hash: 90280dfce8bf44f46a3e41b6c4a9f551
      kubernetes.io/config.mirror: 90280dfce8bf44f46a3e41b6c4a9f551
      kubernetes.io/config.seen: "2021-05-03T13:14:57.122056876Z"
      kubernetes.io/config.source: file
    creationTimestamp: "2021-05-03T13:15:03Z"
    labels:
      component: kube-scheduler
      tier: control-plane
    name: kube-scheduler-kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    namespace: kube-system
    ownerReferences:
    - apiVersion: v1
      controller: true
      kind: Node
      name: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
      uid: 962e1cf3-2712-486b-a1d1-f3fa6170dbad
    resourceVersion: "2470"
    uid: 676a96db-3344-4741-8992-202e9996168e
  spec:
    containers:
    - command:
      - kube-scheduler
      - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
      - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
      - --bind-address=127.0.0.1
      - --kubeconfig=/etc/kubernetes/scheduler.conf
      - --leader-elect=true
      - --port=0
      image: k8s.gcr.io/kube-scheduler:v1.20.4
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 8
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      name: kube-scheduler
      resources:
        requests:
          cpu: 100m
      startupProbe:
        failureThreshold: 24
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10259
          scheme: HTTPS
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 15
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/kubernetes/scheduler.conf
        name: kubeconfig
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    preemptionPolicy: PreemptLowerPriority
    priority: 2000001000
    priorityClassName: system-node-critical
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      operator: Exists
    volumes:
    - hostPath:
        path: /etc/kubernetes/scheduler.conf
        type: FileOrCreate
      name: kubeconfig
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:03Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:16:22Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:16:22Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:15:03Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cf223ef303ae918ca14334801b420f12f665e26486df10b1230bf694ae475b6c
      image: k8s.gcr.io/kube-scheduler:v1.20.4
      imageID: k8s.gcr.io/kube-scheduler@sha256:e2e827b27282f0e36f40a9edc910630ba6124f78d572006f3d25bb000f7018ad
      lastState: {}
      name: kube-scheduler
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:14:26Z"
    hostIP: 10.25.32.129
    phase: Running
    podIP: 145.40.93.163
    podIPs:
    - ip: 145.40.93.163
    qosClass: Burstable
    startTime: "2021-05-03T13:15:03Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      scheduler.alpha.kubernetes.io/critical-pod: ""
    creationTimestamp: "2021-05-03T13:15:07Z"
    generateName: packet-cloud-controller-manager-77cd8c9c7c-
    labels:
      app: packet-cloud-controller-manager
      pod-template-hash: 77cd8c9c7c
    name: packet-cloud-controller-manager-77cd8c9c7c-nzwrr
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: packet-cloud-controller-manager-77cd8c9c7c
      uid: 2ab805a5-9a76-40ed-a772-358754280c5a
    resourceVersion: "2678"
    uid: 414995fb-b715-4f5f-9aaf-c71b2a500eb1
  spec:
    containers:
    - command:
      - ./packet-cloud-controller-manager
      - --cloud-provider=packet
      - --leader-elect=false
      - --allow-untagged-cloud=true
      - --authentication-skip-lookup=true
      - --provider-config=/etc/cloud-sa/cloud-sa.json
      image: packethost/packet-ccm:v1.1.0
      imagePullPolicy: IfNotPresent
      name: packet-cloud-controller-manager
      resources:
        requests:
          cpu: 100m
          memory: 50Mi
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/cloud-sa
        name: cloud-sa-volume
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: cloud-controller-manager-token-stxdh
        readOnly: true
    dnsPolicy: Default
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: cloud-controller-manager
    serviceAccountName: cloud-controller-manager
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoSchedule
      key: node.cloudprovider.kubernetes.io/uninitialized
      value: "true"
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: cloud-sa-volume
      secret:
        defaultMode: 420
        secretName: packet-cloud-config
    - name: cloud-controller-manager-token-stxdh
      secret:
        defaultMode: 420
        secretName: cloud-controller-manager-token-stxdh
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:44Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a44ab19e446ff34ac16a73bd102feda90ebf75a06400556a053c8420fa3835dc
      image: docker.io/packethost/packet-ccm:v1.1.0
      imageID: docker.io/packethost/packet-ccm@sha256:988243ed4299f1ee2148bfaabee0cb8feb65f53321ea9000c2305693c2fc7788
      lastState: {}
      name: packet-cloud-controller-manager
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:48Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 145.40.93.187
    podIPs:
    - ip: 145.40.93.187
    qosClass: Burstable
    startTime: "2021-05-03T13:28:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "7472"
      prometheus.io/scrape: "true"
    creationTimestamp: "2021-05-03T13:28:42Z"
    generateName: controller-675d6c9976-
    labels:
      app: metallb
      component: controller
      pod-template-hash: 675d6c9976
    name: controller-675d6c9976-2qzb5
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: controller-675d6c9976
      uid: 37608d6c-ad0a-49b1-8e1b-dd0da3e787f1
    resourceVersion: "2534"
    uid: cb633596-0872-417c-bddf-eb605a232b78
  spec:
    containers:
    - args:
      - --port=7472
      - --config=config
      image: metallb/controller:v0.8.2
      imagePullPolicy: IfNotPresent
      name: controller
      ports:
      - containerPort: 7472
        name: monitoring
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
          - all
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: controller-token-jx7j9
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext:
      runAsNonRoot: true
      runAsUser: 65534
    serviceAccount: controller
    serviceAccountName: controller
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: controller-token-jx7j9
      secret:
        defaultMode: 420
        secretName: controller-token-jx7j9
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:07Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:07Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4dda796ebdc196d509488176e4c9d4b39cc169a0ea787cb562f1945450cadde2
      image: docker.io/metallb/controller:v0.8.2
      imageID: docker.io/metallb/controller@sha256:5c050e59074e152711737d2bb9ede96dff67016c80cf25cdf5fc46109718a583
      lastState: {}
      name: controller
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:32Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 192.168.5.28
    podIPs:
    - ip: 192.168.5.28
    qosClass: Guaranteed
    startTime: "2021-05-03T13:29:07Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "7472"
      prometheus.io/scrape: "true"
    creationTimestamp: "2021-05-03T13:28:57Z"
    generateName: speaker-
    labels:
      app: metallb
      component: speaker
      controller-revision-hash: 587c7c54d5
      pod-template-generation: "1"
    name: speaker-8pw7x
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: speaker
      uid: 5e52970f-a55d-4cea-9bcd-08a78686bccc
    resourceVersion: "2318"
    uid: 74121192-051e-40b2-b2fd-06a7d34a45af
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    containers:
    - args:
      - --port=7472
      - --config=config
      env:
      - name: METALLB_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: METALLB_HOST
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      image: metallb/speaker:v0.8.2
      imagePullPolicy: IfNotPresent
      name: speaker
      ports:
      - containerPort: 7472
        hostPort: 7472
        name: monitoring
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
          - SYS_ADMIN
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: speaker-token-5bd6g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: speaker
    serviceAccountName: speaker
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: speaker-token-5bd6g
      secret:
        defaultMode: 420
        secretName: speaker-token-5bd6g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:57Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://e5f1e0d368f8f82253da6dea5cf2647b603a32c272833c44ae1239ffb146e621
      image: docker.io/metallb/speaker:v0.8.2
      imageID: docker.io/metallb/speaker@sha256:f1941498a28cdb332429e25d18233683da6949ecfc4f6dacf12b1416d7d38263
      lastState: {}
      name: speaker
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:01Z"
    hostIP: 10.25.32.135
    phase: Running
    podIP: 10.25.32.135
    podIPs:
    - ip: 10.25.32.135
    qosClass: Guaranteed
    startTime: "2021-05-03T13:28:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "7472"
      prometheus.io/scrape: "true"
    creationTimestamp: "2021-05-03T13:28:54Z"
    generateName: speaker-
    labels:
      app: metallb
      component: speaker
      controller-revision-hash: 587c7c54d5
      pod-template-generation: "1"
    name: speaker-cvvfq
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: speaker
      uid: 5e52970f-a55d-4cea-9bcd-08a78686bccc
    resourceVersion: "2296"
    uid: c960e067-7d0e-4c76-b226-0ce44a7c51c3
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    containers:
    - args:
      - --port=7472
      - --config=config
      env:
      - name: METALLB_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: METALLB_HOST
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      image: metallb/speaker:v0.8.2
      imagePullPolicy: IfNotPresent
      name: speaker
      ports:
      - containerPort: 7472
        hostPort: 7472
        name: monitoring
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
          - SYS_ADMIN
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: speaker-token-5bd6g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-control-plane-fbpl7
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: speaker
    serviceAccountName: speaker
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: speaker-token-5bd6g
      secret:
        defaultMode: 420
        secretName: speaker-token-5bd6g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:54Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:58Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:58Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:28:54Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://362a0041ab1b991ca599dff3741b09a4df3f6198b55b32c10149cceace84e27d
      image: docker.io/metallb/speaker:v0.8.2
      imageID: docker.io/metallb/speaker@sha256:f1941498a28cdb332429e25d18233683da6949ecfc4f6dacf12b1416d7d38263
      lastState: {}
      name: speaker
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:28:58Z"
    hostIP: 10.25.32.129
    phase: Running
    podIP: 10.25.32.129
    podIPs:
    - ip: 10.25.32.129
    qosClass: Guaranteed
    startTime: "2021-05-03T13:28:54Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "7472"
      prometheus.io/scrape: "true"
    creationTimestamp: "2021-05-03T13:29:01Z"
    generateName: speaker-
    labels:
      app: metallb
      component: speaker
      controller-revision-hash: 587c7c54d5
      pod-template-generation: "1"
    name: speaker-qtbtp
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: speaker
      uid: 5e52970f-a55d-4cea-9bcd-08a78686bccc
    resourceVersion: "2378"
    uid: f34b3b29-79e5-4469-a855-e1fcab9bffbc
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    containers:
    - args:
      - --port=7472
      - --config=config
      env:
      - name: METALLB_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: METALLB_HOST
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      image: metallb/speaker:v0.8.2
      imagePullPolicy: IfNotPresent
      name: speaker
      ports:
      - containerPort: 7472
        hostPort: 7472
        name: monitoring
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
          - SYS_ADMIN
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: speaker-token-5bd6g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: speaker
    serviceAccountName: speaker
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: speaker-token-5bd6g
      secret:
        defaultMode: 420
        secretName: speaker-token-5bd6g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:01Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:05Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:05Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:01Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a62cd70a173f3027193b3fcf5e0b097d5d85a4a48f64d9085c6ffae1b91fa194
      image: docker.io/metallb/speaker:v0.8.2
      imageID: docker.io/metallb/speaker@sha256:f1941498a28cdb332429e25d18233683da6949ecfc4f6dacf12b1416d7d38263
      lastState: {}
      name: speaker
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:04Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 10.25.32.133
    podIPs:
    - ip: 10.25.32.133
    qosClass: Guaranteed
    startTime: "2021-05-03T13:29:01Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "7472"
      prometheus.io/scrape: "true"
    creationTimestamp: "2021-05-03T13:29:04Z"
    generateName: speaker-
    labels:
      app: metallb
      component: speaker
      controller-revision-hash: 587c7c54d5
      pod-template-generation: "1"
    name: speaker-tp8sv
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: speaker
      uid: 5e52970f-a55d-4cea-9bcd-08a78686bccc
    resourceVersion: "2417"
    uid: 3b989734-3485-4e5c-a431-4c251da7f3f2
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    containers:
    - args:
      - --port=7472
      - --config=config
      env:
      - name: METALLB_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: METALLB_HOST
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.hostIP
      image: metallb/speaker:v0.8.2
      imagePullPolicy: IfNotPresent
      name: speaker
      ports:
      - containerPort: 7472
        hostPort: 7472
        name: monitoring
        protocol: TCP
      resources:
        limits:
          cpu: 100m
          memory: 100Mi
        requests:
          cpu: 100m
          memory: 100Mi
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          add:
          - NET_ADMIN
          - NET_RAW
          - SYS_ADMIN
          drop:
          - ALL
        readOnlyRootFilesystem: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: speaker-token-5bd6g
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    hostNetwork: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      beta.kubernetes.io/os: linux
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: speaker
    serviceAccountName: speaker
    terminationGracePeriodSeconds: 0
    tolerations:
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/network-unavailable
      operator: Exists
    volumes:
    - name: speaker-token-5bd6g
      secret:
        defaultMode: 420
        secretName: speaker-token-5bd6g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:08Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:08Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a6188b5541b6792f22659e45e0546319eb555879f31173bde706dd86ec02e99c
      image: docker.io/metallb/speaker:v0.8.2
      imageID: docker.io/metallb/speaker@sha256:f1941498a28cdb332429e25d18233683da6949ecfc4f6dacf12b1416d7d38263
      lastState: {}
      name: speaker
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:08Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 10.25.32.131
    podIPs:
    - ip: 10.25.32.131
    qosClass: Guaranteed
    startTime: "2021-05-03T13:29:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:30Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 59f45cd547
      pod-template-generation: "1"
    name: csi-cephfsplugin-2wc5z
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: a8401a9a-b7cc-4bc3-b0cd-4d7b8c80ba47
    resourceVersion: "2906"
    uid: f3a6c11d-5376-4601-a16b-6d56f748e370
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9091
      - --forcecephkernelclient=true
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9081
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: rook-csi-cephfs-plugin-sa-token-mg29g
      secret:
        defaultMode: 420
        secretName: rook-csi-cephfs-plugin-sa-token-mg29g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c783e5c2e934c0755072d6c4150e953e174528f67f3d11880f72df3ad9aa98f2
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    - containerID: containerd://a5ac34bfbbd70487ce3afe90f8ba99b85e814b086eb5917244de5ac56eb5d07c
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imageID: k8s.gcr.io/sig-storage/csi-node-driver-registrar@sha256:e07f914c32f0505e4c470a62a40ee43f84cbf8dc46ff861f31b14457ccbad108
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    - containerID: containerd://054263a2c9e97099271b6bad3fb89d68ad846e450fff0ab72feeeba1552b4cff
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 192.168.0.149
    podIPs:
    - ip: 192.168.0.149
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:30Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 59f45cd547
      pod-template-generation: "1"
    name: csi-cephfsplugin-flvxl
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: a8401a9a-b7cc-4bc3-b0cd-4d7b8c80ba47
    resourceVersion: "3157"
    uid: e44e74d3-7708-45d0-85a7-937ecec82961
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9091
      - --forcecephkernelclient=true
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9081
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: rook-csi-cephfs-plugin-sa-token-mg29g
      secret:
        defaultMode: 420
        secretName: rook-csi-cephfs-plugin-sa-token-mg29g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c26a34bbdbe1c4263e3162bf454c9397579bd28bf6cfcfc769d0d088e835e5db
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:57Z"
    - containerID: containerd://1ba9496d96663c2d1faeda03475ed8b62cc15ae65acc2783dcdf12951c633fc2
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imageID: k8s.gcr.io/sig-storage/csi-node-driver-registrar@sha256:e07f914c32f0505e4c470a62a40ee43f84cbf8dc46ff861f31b14457ccbad108
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    - containerID: containerd://a4bd2a91fe55ff39da906932cb4a83c93ca7b049a185ff8fb90dd0ba7deef6bf
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:57Z"
    hostIP: 10.25.32.135
    phase: Running
    podIP: 192.168.6.115
    podIPs:
    - ip: 192.168.6.115
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:30Z"
    generateName: csi-cephfsplugin-provisioner-5b989b9977-
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 5b989b9977
    name: csi-cephfsplugin-provisioner-5b989b9977-lcfnm
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-cephfsplugin-provisioner-5b989b9977
      uid: f8990e57-3edc-41b0-9410-87282d729c59
    resourceVersion: "3191"
    uid: 08fd3546-d8a3-4141-b57b-a29f43560d5a
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-cephfsplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --v=0
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --timeout=150s
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --leader-election
      - --leader-election-namespace=rook-ceph
      - --handle-volume-inuse-error=false
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --controllerserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9091
      - --forcecephkernelclient=true
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9081
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-provisioner-sa
    serviceAccountName: rook-csi-cephfs-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: socket-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: rook-csi-cephfs-provisioner-sa-token-ksx9t
      secret:
        defaultMode: 420
        secretName: rook-csi-cephfs-provisioner-sa-token-ksx9t
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:00Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:00Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d4cae6ff338cbafbe32f9f9e7c8e01b2e8204045d407a0f94dc14ba93c87a38f
      image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
      imageID: k8s.gcr.io/sig-storage/csi-attacher@sha256:6f80b12657a7e0a5c683b24e806c4bbbe33a43e39b041fe9b7514d665d478ea4
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:55Z"
    - containerID: containerd://1a86d558568dc02de6d5c0427c739044c8737c96f4d0cf8537cb0eb41e1b1129
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:00Z"
    - containerID: containerd://b48c239859df5e3a7d51194d75a36ae054339385ff3ede93d65affd3a4c3f748
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
      imageID: k8s.gcr.io/sig-storage/csi-provisioner@sha256:bec571992d40203edcd056ac0b0d97003887ee5e4be144c41932d18639673b03
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:00Z"
    - containerID: containerd://6c1b877524af245d820773543d6a1e20eb7a5503d3da5fb7c3b582e28c3bcb8e
      image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
      imageID: k8s.gcr.io/sig-storage/csi-resizer@sha256:ca160717df7f6b7964c936faf608b914f2277363e6d708ea2e2dbbf2c5f56a83
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:58Z"
    - containerID: containerd://91ccc145987a13c58ff5468b3d9ad24fd1b0bc632e9377794b9cf9bbb11558df
      image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
      imageID: k8s.gcr.io/sig-storage/csi-snapshotter@sha256:26e327b018c21a49523b759d7787e99553181ae9ef90b6bdc13abe362a43ced0
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:57Z"
    - containerID: containerd://a9ef94a73afad00d9b8b22117b76ead7f0970efc2e0cdefae952c96eec2c8f5b
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:00Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 192.168.4.203
    podIPs:
    - ip: 192.168.4.203
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:30Z"
    generateName: csi-cephfsplugin-provisioner-5b989b9977-
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 5b989b9977
    name: csi-cephfsplugin-provisioner-5b989b9977-vjlfx
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-cephfsplugin-provisioner-5b989b9977
      uid: f8990e57-3edc-41b0-9410-87282d729c59
    resourceVersion: "2976"
    uid: 9cede318-77c3-4c55-a892-85a0fd5bc809
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-cephfsplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --v=0
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --timeout=150s
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --leader-election
      - --leader-election-namespace=rook-ceph
      - --handle-volume-inuse-error=false
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --controllerserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9091
      - --forcecephkernelclient=true
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9081
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-provisioner-sa-token-ksx9t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-provisioner-sa
    serviceAccountName: rook-csi-cephfs-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir:
        medium: Memory
      name: socket-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: rook-csi-cephfs-provisioner-sa-token-ksx9t
      secret:
        defaultMode: 420
        secretName: rook-csi-cephfs-provisioner-sa-token-ksx9t
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://3fb763a673553c9685a980fef7aeaeb2858071d6b372a18491d7e74586881447
      image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
      imageID: k8s.gcr.io/sig-storage/csi-attacher@sha256:6f80b12657a7e0a5c683b24e806c4bbbe33a43e39b041fe9b7514d665d478ea4
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:34Z"
    - containerID: containerd://2adc27aeee51b211eacc5d962b5ca59e13867778fdfda2150118f7527844824c
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:38Z"
    - containerID: containerd://f08caf09c22f2e038666ac6fa14812b7e0bfadb2ac56410e2576f7b47904daab
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
      imageID: k8s.gcr.io/sig-storage/csi-provisioner@sha256:bec571992d40203edcd056ac0b0d97003887ee5e4be144c41932d18639673b03
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:38Z"
    - containerID: containerd://5dde5f9bb3815de769a1eb9a7254a7e4b8fc09efd68f7740a5ae49f8bba561a2
      image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
      imageID: k8s.gcr.io/sig-storage/csi-resizer@sha256:ca160717df7f6b7964c936faf608b914f2277363e6d708ea2e2dbbf2c5f56a83
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:38Z"
    - containerID: containerd://3c15742c2a73f1738e24e850bda3d9397949769b724cfaef442aa896ba37213d
      image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
      imageID: k8s.gcr.io/sig-storage/csi-snapshotter@sha256:26e327b018c21a49523b759d7787e99553181ae9ef90b6bdc13abe362a43ced0
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:37Z"
    - containerID: containerd://24034f9680f0f854cdfbc40f894f90730623d28fb10dab4bc2f3df346eed597f
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:38Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 192.168.0.47
    podIPs:
    - ip: 192.168.0.47
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:30Z"
    generateName: csi-cephfsplugin-
    labels:
      app: csi-cephfsplugin
      contains: csi-cephfsplugin-metrics
      controller-revision-hash: 59f45cd547
      pod-template-generation: "1"
    name: csi-cephfsplugin-txgnj
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-cephfsplugin
      uid: a8401a9a-b7cc-4bc3-b0cd-4d7b8c80ba47
    resourceVersion: "3120"
    uid: cbc06072-78c9-4f0f-9bcd-5cb39a883f40
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --type=cephfs
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --nodeserver=true
      - --drivername=rook-ceph.cephfs.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9091
      - --forcecephkernelclient=true
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-cephfsplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: csi-plugins-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /dev
        name: host-dev
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9081
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-cephfs-plugin-sa-token-mg29g
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-cephfs-plugin-sa
    serviceAccountName: rook-csi-cephfs-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: csi-plugins-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: rook-csi-cephfs-plugin-sa-token-mg29g
      secret:
        defaultMode: 420
        secretName: rook-csi-cephfs-plugin-sa-token-mg29g
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:30Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9bcf59cab77d95246a73c64b2df065f198a30b8a6a0100de5cdd7de75a61ed5d
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-cephfsplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:54Z"
    - containerID: containerd://4d573a6ebcda9d16bd0850c27d93d1276c15fbc41d187e980b7596f930dc2dbd
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imageID: k8s.gcr.io/sig-storage/csi-node-driver-registrar@sha256:e07f914c32f0505e4c470a62a40ee43f84cbf8dc46ff861f31b14457ccbad108
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    - containerID: containerd://ca63ec76cf4668a055f69f375868c9fd210fdd660c8e4c37d7a47a62a83761cc
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:54Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 192.168.5.75
    podIPs:
    - ip: 192.168.5.75
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:30Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:29Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 748b65f9bb
      pod-template-generation: "1"
    name: csi-rbdplugin-4j84x
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: 6a65e3ea-b563-4e1d-a00c-50d91a14f5c1
    resourceVersion: "2910"
    uid: 94805f08-27c8-4b3f-859a-5df925e7fe1f
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9090
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9080
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostPID: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: rook-csi-rbd-plugin-sa-token-48tks
      secret:
        defaultMode: 420
        secretName: rook-csi-rbd-plugin-sa-token-48tks
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://8503426360df509348b1e79f06a049b290df3c36e0c6e83af43dceac4676843c
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    - containerID: containerd://5691b71c3b11f4efd48edb31c40c33d2b77e9f553ed2441b91152644fb0f86fe
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imageID: k8s.gcr.io/sig-storage/csi-node-driver-registrar@sha256:e07f914c32f0505e4c470a62a40ee43f84cbf8dc46ff861f31b14457ccbad108
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    - containerID: containerd://96b72d7d32e9824302917cefd12c7498862b052e070c46c1bce850b315513a66
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 192.168.1.76
    podIPs:
    - ip: 192.168.1.76
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:29Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 748b65f9bb
      pod-template-generation: "1"
    name: csi-rbdplugin-bkq65
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: 6a65e3ea-b563-4e1d-a00c-50d91a14f5c1
    resourceVersion: "3124"
    uid: 6eed7270-caeb-49c9-b4ad-d4166cb2f791
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9090
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9080
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostPID: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: rook-csi-rbd-plugin-sa-token-48tks
      secret:
        defaultMode: 420
        secretName: rook-csi-rbd-plugin-sa-token-48tks
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:55Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:55Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://57b9849b85b73bb3de41aaf0eef58d4706770be9c071d93718fc843e0a8ae4a5
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:54Z"
    - containerID: containerd://37f91250e9d2776e078592be1ada93d172d97ead6bd3414635665508eae583e3
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imageID: k8s.gcr.io/sig-storage/csi-node-driver-registrar@sha256:e07f914c32f0505e4c470a62a40ee43f84cbf8dc46ff861f31b14457ccbad108
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    - containerID: containerd://f1a1473cb230b087f626e285e84470b91f26779cabbad1c1ce92212c9ae05e5d
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:54Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 192.168.5.215
    podIPs:
    - ip: 192.168.5.215
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:29Z"
    generateName: csi-rbdplugin-
    labels:
      app: csi-rbdplugin
      contains: csi-rbdplugin-metrics
      controller-revision-hash: 748b65f9bb
      pod-template-generation: "1"
    name: csi-rbdplugin-lqbth
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: DaemonSet
      name: csi-rbdplugin
      uid: 6a65e3ea-b563-4e1d-a00c-50d91a14f5c1
    resourceVersion: "3153"
    uid: 6cd04371-04cb-4815-b036-ec88473b1e7a
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchFields:
            - key: metadata.name
              operator: In
              values:
              - kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    containers:
    - args:
      - --v=0
      - --csi-address=/csi/csi.sock
      - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
      env:
      - name: KUBE_NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imagePullPolicy: IfNotPresent
      name: driver-registrar
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /registration
        name: registration-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --nodeserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9090
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      securityContext:
        allowPrivilegeEscalation: true
        capabilities:
          add:
          - SYS_ADMIN
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/lib/kubelet/pods
        mountPropagation: Bidirectional
        name: pods-mount-dir
      - mountPath: /var/lib/kubelet/plugins
        mountPropagation: Bidirectional
        name: plugin-mount-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /run/mount
        name: host-run-mount
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9080
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: plugin-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-plugin-sa-token-48tks
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    hostPID: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-plugin-sa
    serviceAccountName: rook-csi-rbd-plugin-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/disk-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/memory-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/pid-pressure
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/unschedulable
      operator: Exists
    volumes:
    - hostPath:
        path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
        type: DirectoryOrCreate
      name: plugin-dir
    - hostPath:
        path: /var/lib/kubelet/plugins
        type: Directory
      name: plugin-mount-dir
    - hostPath:
        path: /var/lib/kubelet/plugins_registry/
        type: Directory
      name: registration-dir
    - hostPath:
        path: /var/lib/kubelet/pods
        type: Directory
      name: pods-mount-dir
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - hostPath:
        path: /run/mount
        type: ""
      name: host-run-mount
    - name: rook-csi-rbd-plugin-sa-token-48tks
      secret:
        defaultMode: 420
        secretName: rook-csi-rbd-plugin-sa-token-48tks
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:57Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:57Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://933fea3fd7318e66481d5ecc2b5e5d79f8bede0a506bc77db0c6cff2b0b0358a
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:57Z"
    - containerID: containerd://ee4a5e82762fbd2a35893d8d82124942578116e59ff33094df6fee93f8181e6a
      image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
      imageID: k8s.gcr.io/sig-storage/csi-node-driver-registrar@sha256:e07f914c32f0505e4c470a62a40ee43f84cbf8dc46ff861f31b14457ccbad108
      lastState: {}
      name: driver-registrar
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:31Z"
    - containerID: containerd://85ecfc72c48369f2abe861c7231676ac8987f899eeabd2f94bd7738ca8a065f8
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:57Z"
    hostIP: 10.25.32.135
    phase: Running
    podIP: 192.168.7.23
    podIPs:
    - ip: 192.168.7.23
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:29Z"
    generateName: csi-rbdplugin-provisioner-55f998c984-
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 55f998c984
    name: csi-rbdplugin-provisioner-55f998c984-5bc7m
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-rbdplugin-provisioner-55f998c984
      uid: 4133ce73-37d7-49ec-a4a5-b64723cb5593
    resourceVersion: "2980"
    uid: 4cb9355d-d73e-413d-8402-b11b04a339b5
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-rbdplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --default-fstype=ext4
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --leader-election
      - --leader-election-namespace=rook-ceph
      - --handle-volume-inuse-error=false
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --v=0
      - --timeout=150s
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --controllerserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9090
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9080
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-provisioner-sa
    serviceAccountName: rook-csi-rbd-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - emptyDir:
        medium: Memory
      name: socket-dir
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: rook-csi-rbd-provisioner-sa-token-h76h2
      secret:
        defaultMode: 420
        secretName: rook-csi-rbd-provisioner-sa-token-h76h2
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:38Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:38Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5ce4dffb200f06d2aeda45bebac468932fde0c698cfa23b60380a154a5140c6e
      image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
      imageID: k8s.gcr.io/sig-storage/csi-attacher@sha256:6f80b12657a7e0a5c683b24e806c4bbbe33a43e39b041fe9b7514d665d478ea4
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:36Z"
    - containerID: containerd://7df3c121f6bb6610ecceeb95e640c262216cec99c140d02f5795bfb48f593d16
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
      imageID: k8s.gcr.io/sig-storage/csi-provisioner@sha256:bec571992d40203edcd056ac0b0d97003887ee5e4be144c41932d18639673b03
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:33Z"
    - containerID: containerd://69d03f6e21e6a2ce8925da9a4f1897978a1f81d3bc1566bde5bf34233480a42f
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:38Z"
    - containerID: containerd://e1bf4d038d9bfa4ec5d1248be4b63f816ebaa1e984c24ebbb6a2aa14c200304b
      image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
      imageID: k8s.gcr.io/sig-storage/csi-resizer@sha256:ca160717df7f6b7964c936faf608b914f2277363e6d708ea2e2dbbf2c5f56a83
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:36Z"
    - containerID: containerd://7a4d3076b958b10e11d0db6654ed76dbd67a36ba298d515ce9bb65b476175153
      image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
      imageID: k8s.gcr.io/sig-storage/csi-snapshotter@sha256:26e327b018c21a49523b759d7787e99553181ae9ef90b6bdc13abe362a43ced0
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:38Z"
    - containerID: containerd://5a9d86d6bb876012ccb79d05537f0c9805e096224069af7fcc688f96fcc2cb14
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:38Z"
    hostIP: 10.25.32.133
    phase: Running
    podIP: 192.168.1.167
    podIPs:
    - ip: 192.168.1.167
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:30:29Z"
    generateName: csi-rbdplugin-provisioner-55f998c984-
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 55f998c984
    name: csi-rbdplugin-provisioner-55f998c984-bhlvx
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: csi-rbdplugin-provisioner-55f998c984
      uid: 4133ce73-37d7-49ec-a4a5-b64723cb5593
    resourceVersion: "3302"
    uid: ba2667d6-887f-4507-ab59-a982ab0e50a1
  spec:
    affinity:
      nodeAffinity: {}
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - csi-rbdplugin-provisioner
          topologyKey: kubernetes.io/hostname
    containers:
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --retry-interval-start=500ms
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      - --default-fstype=ext4
      - --extra-create-metadata=true
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
      imagePullPolicy: IfNotPresent
      name: csi-provisioner
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --leader-election
      - --leader-election-namespace=rook-ceph
      - --handle-volume-inuse-error=false
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
      imagePullPolicy: IfNotPresent
      name: csi-resizer
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --v=0
      - --timeout=150s
      - --csi-address=$(ADDRESS)
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: /csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
      imagePullPolicy: IfNotPresent
      name: csi-attacher
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --csi-address=$(ADDRESS)
      - --v=0
      - --timeout=150s
      - --leader-election=true
      - --leader-election-namespace=rook-ceph
      env:
      - name: ADDRESS
        value: unix:///csi/csi-provisioner.sock
      image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
      imagePullPolicy: IfNotPresent
      name: csi-snapshotter
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --nodeid=$(NODE_ID)
      - --endpoint=$(CSI_ENDPOINT)
      - --v=0
      - --type=rbd
      - --controllerserver=true
      - --drivername=rook-ceph.rbd.csi.ceph.com
      - --pidlimit=-1
      - --metricsport=9090
      - --metricspath=/metrics
      - --enablegrpcmetrics=false
      env:
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: NODE_ID
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: csi-rbdplugin
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /dev
        name: host-dev
      - mountPath: /sys
        name: host-sys
      - mountPath: /lib/modules
        name: lib-modules
        readOnly: true
      - mountPath: /etc/ceph-csi-config/
        name: ceph-csi-config
      - mountPath: /tmp/csi/keys
        name: keys-tmp-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    - args:
      - --type=liveness
      - --endpoint=$(CSI_ENDPOINT)
      - --metricsport=9080
      - --metricspath=/metrics
      - --polltime=60s
      - --timeout=3s
      env:
      - name: CSI_ENDPOINT
        value: unix:///csi/csi-provisioner.sock
      - name: POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imagePullPolicy: IfNotPresent
      name: liveness-prometheus
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /csi
        name: socket-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-csi-rbd-provisioner-sa-token-h76h2
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-csi-rbd-provisioner-sa
    serviceAccountName: rook-csi-rbd-provisioner-sa
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /dev
        type: ""
      name: host-dev
    - hostPath:
        path: /sys
        type: ""
      name: host-sys
    - hostPath:
        path: /lib/modules
        type: ""
      name: lib-modules
    - emptyDir:
        medium: Memory
      name: socket-dir
    - configMap:
        defaultMode: 420
        items:
        - key: csi-cluster-config-json
          path: config.json
        name: rook-ceph-csi-config
      name: ceph-csi-config
    - emptyDir:
        medium: Memory
      name: keys-tmp-dir
    - name: rook-csi-rbd-provisioner-sa-token-h76h2
      secret:
        defaultMode: 420
        secretName: rook-csi-rbd-provisioner-sa-token-h76h2
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:30:29Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1a5e9f766ca4858d17ff92c60c1e464490b9f9d8db0e0f3534134e569b1a5006
      image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
      imageID: k8s.gcr.io/sig-storage/csi-attacher@sha256:6f80b12657a7e0a5c683b24e806c4bbbe33a43e39b041fe9b7514d665d478ea4
      lastState: {}
      name: csi-attacher
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:18Z"
    - containerID: containerd://afc465adc68cca42558cac3d3a1435b995910e6504dacf918889af309a2d3978
      image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
      imageID: k8s.gcr.io/sig-storage/csi-provisioner@sha256:bec571992d40203edcd056ac0b0d97003887ee5e4be144c41932d18639673b03
      lastState: {}
      name: csi-provisioner
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:33Z"
    - containerID: containerd://4010b440573936cd30917e35c1593a476003171345d912e6b1c5839e2d6df950
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: csi-rbdplugin
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:20Z"
    - containerID: containerd://04a23efba4511f149570394931c917a58893914972d248cfa59801a427cb5710
      image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
      imageID: k8s.gcr.io/sig-storage/csi-resizer@sha256:ca160717df7f6b7964c936faf608b914f2277363e6d708ea2e2dbbf2c5f56a83
      lastState: {}
      name: csi-resizer
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:30:57Z"
    - containerID: containerd://815c5f31486def99bb217c5a9958ee403df071d3a477be18bd14a57e6403fa18
      image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
      imageID: k8s.gcr.io/sig-storage/csi-snapshotter@sha256:26e327b018c21a49523b759d7787e99553181ae9ef90b6bdc13abe362a43ced0
      lastState: {}
      name: csi-snapshotter
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:20Z"
    - containerID: containerd://f5354d068de98a172919bb8a6e09646cae92670f799783dfd62226b9a603c34c
      image: quay.io/cephcsi/cephcsi:v3.3.1
      imageID: quay.io/cephcsi/cephcsi@sha256:21ee2b825b2b238330bc3a7877b6798a700dfcdb559245e992637e19dbd35b5b
      lastState: {}
      name: liveness-prometheus
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:20Z"
    hostIP: 10.25.32.135
    phase: Running
    podIP: 192.168.6.43
    podIPs:
    - ip: 192.168.6.43
    qosClass: BestEffort
    startTime: "2021-05-03T13:30:29Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:25Z"
    generateName: rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0-59fb6b7f87-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      pod-template-hash: 59fb6b7f87
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0-gfh8v
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0-59fb6b7f87
      uid: 6d5d8c91-8d0f-4edc-aa48-bd49b6ad1002
    resourceVersion: "4412"
    uid: f548b347-bff8-407f-9a93-290411f0543b
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: default-token-k9mpw
      secret:
        defaultMode: 420
        secretName: default-token-k9mpw
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9924bdec215689b3b084d30ddccbeeca524da05f1b4db247188363df2ae6d1c7
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:28Z"
    hostIP: 10.25.32.135
    initContainerStatuses:
    - containerID: containerd://118e230365329f0dcc4946ca04500f05f2a69380be39e6fff0619a7e9e451d6b
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://118e230365329f0dcc4946ca04500f05f2a69380be39e6fff0619a7e9e451d6b
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    - containerID: containerd://2c0122177a693347334099dbdd495dc34b2583659d3a00b08e34dbbb8f4aa80f
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://2c0122177a693347334099dbdd495dc34b2583659d3a00b08e34dbbb8f4aa80f
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:27Z"
    phase: Running
    podIP: 192.168.6.84
    podIPs:
    - ip: 192.168.6.84
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:31:57Z"
    generateName: rook-ceph-crashcollector-ceeab05259c8d89b0904c022bc5f8be0-7975c649-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      pod-template-hash: 7975c649
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-ceeab05259c8d89b0904c022bc5f8be0-wzm5z
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-ceeab05259c8d89b0904c022bc5f8be0-7975c649
      uid: 7354f394-7e11-4895-a93b-2692599d1fba
    resourceVersion: "4004"
    uid: f5f5c835-d85f-4883-ab5e-45318ea61f35
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: default-token-k9mpw
      secret:
        defaultMode: 420
        secretName: default-token-k9mpw
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:15Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:16Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:16Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://864a0c658a63f5cdc2729771d350ad0470f19024d1637c4b26e5bf240af91d3e
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:15Z"
    hostIP: 10.25.32.133
    initContainerStatuses:
    - containerID: containerd://06ceee295296691bfa323f8b0724b2fc6e9450866a609a79a4df9a28e1377479
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://06ceee295296691bfa323f8b0724b2fc6e9450866a609a79a4df9a28e1377479
          exitCode: 0
          finishedAt: "2021-05-03T13:32:14Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:14Z"
    - containerID: containerd://6a90d14bd4892d7b672ce9de4792a83f75c8dad78b58b5233f9786f48d360d25
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://6a90d14bd4892d7b672ce9de4792a83f75c8dad78b58b5233f9786f48d360d25
          exitCode: 0
          finishedAt: "2021-05-03T13:32:14Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:14Z"
    phase: Running
    podIP: 192.168.0.136
    podIPs:
    - ip: 192.168.0.136
    qosClass: BestEffort
    startTime: "2021-05-03T13:31:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:31:43Z"
    generateName: rook-ceph-crashcollector-fb5b167868730b399f51c41bfe43cdc0-66c6d67c89-
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      pod-template-hash: 66c6d67c89
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-fb5b167868730b399f51c41bfe43cdc0-g8tq8
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-crashcollector-fb5b167868730b399f51c41bfe43cdc0-66c6d67c89
      uid: c8e559b7-45dd-4dc5-8e02-a84ce271e682
    resourceVersion: "4025"
    uid: 03d76aa4-7cb7-4221-b0e5-792041876c6f
  spec:
    containers:
    - command:
      - ceph-crash
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: ceph-crash
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /etc/ceph/crash-collector-keyring-store/
        name: rook-ceph-crash-collector-keyring
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - /var/lib/ceph/crash/posted
      command:
      - mkdir
      - -p
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: make-container-crash-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - name: rook-ceph-crash-collector-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-crash-collector-keyring
    - name: default-token-k9mpw
      secret:
        defaultMode: 420
        secretName: default-token-k9mpw
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:17Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:18Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:18Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c6b9dfee81e0f7d1acc8bd1a3ae740a9fd2fec8d249716742e7112939a7b2f0a
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: ceph-crash
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:17Z"
    hostIP: 10.25.32.131
    initContainerStatuses:
    - containerID: containerd://95f16b1f466cb228d283e33030dfcf2d65ceb2e41ae73ba5acfc2fe303e08bd2
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: make-container-crash-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://95f16b1f466cb228d283e33030dfcf2d65ceb2e41ae73ba5acfc2fe303e08bd2
          exitCode: 0
          finishedAt: "2021-05-03T13:32:15Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:15Z"
    - containerID: containerd://7cbf2e8744a0a7698ce8161e94395d0aac36a68519f0903eab2589f4e69f4d75
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://7cbf2e8744a0a7698ce8161e94395d0aac36a68519f0903eab2589f4e69f4d75
          exitCode: 0
          finishedAt: "2021-05-03T13:32:16Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:16Z"
    phase: Running
    podIP: 192.168.4.42
    podIPs:
    - ip: 192.168.4.42
    qosClass: BestEffort
    startTime: "2021-05-03T13:31:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    annotations:
      prometheus.io/port: "9283"
      prometheus.io/scrape: "true"
    creationTimestamp: "2021-05-03T13:32:08Z"
    generateName: rook-ceph-mgr-a-65f9b6b9d6-
    labels:
      app: rook-ceph-mgr
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      pod-template-hash: 65f9b6b9d6
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-65f9b6b9d6-7tpm2
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mgr-a-65f9b6b9d6
      uid: a49e5e8a-e39e-42ab-ae04-cc51ea5f909f
    resourceVersion: "3874"
    uid: ebca68ef-3b9d-47c2-aec1-ea1853eab456
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=742018bb-f74c-4681-bfea-4ab962753821
      - --keyring=/etc/ceph/keyring-store/keyring
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      - --client-mount-uid=0
      - --client-mount-gid=0
      - --foreground
      - --public-addr=$(ROOK_POD_IP)
      command:
      - ceph-mgr
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_OPERATOR_NAMESPACE
        value: rook-ceph
      - name: ROOK_CEPH_CLUSTER_CRD_VERSION
        value: v1
      - name: ROOK_CEPH_CLUSTER_CRD_NAME
        value: rook-ceph
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        failureThreshold: 3
        httpGet:
          path: /
          port: 9283
          scheme: HTTP
        initialDelaySeconds: 60
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: mgr
      ports:
      - containerPort: 6800
        name: mgr
        protocol: TCP
      - containerPort: 9283
        name: http-metrics
        protocol: TCP
      - containerPort: 8443
        name: dashboard
        protocol: TCP
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-mgr-token-hvkj6
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /var/lib/ceph/mgr/ceph-a
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mgr-a-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mgr/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-mgr-token-hvkj6
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-mgr
    serviceAccountName: rook-ceph-mgr
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mgr-a-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mgr-a-keyring
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: ceph-daemon-data
    - name: rook-ceph-mgr-token-hvkj6
      secret:
        defaultMode: 420
        secretName: rook-ceph-mgr-token-hvkj6
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:09Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:10Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:10Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:08Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://7286f2c559ab6f98f228852593747f0ae0731f979df26d483a74be249d8e888c
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: mgr
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:09Z"
    hostIP: 10.25.32.135
    initContainerStatuses:
    - containerID: containerd://a40b741a2a520d895aed83c1ca31400d779543394e2b3d58e74bc2b4eabdb853
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://a40b741a2a520d895aed83c1ca31400d779543394e2b3d58e74bc2b4eabdb853
          exitCode: 0
          finishedAt: "2021-05-03T13:32:09Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:09Z"
    phase: Running
    podIP: 192.168.7.168
    podIPs:
    - ip: 192.168.7.168
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:08Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:31:28Z"
    generateName: rook-ceph-mon-a-9db7d95f6-
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 9db7d95f6
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-9db7d95f6-j9fx6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-a-9db7d95f6
      uid: fe2d878b-4127-491f-ac23-4296c735d23e
    resourceVersion: "3537"
    uid: b9f49f03-bb96-4058-970e-b7a9650594c5
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=742018bb-f74c-4681-bfea-4ab962753821
      - --keyring=/etc/ceph/keyring-store/keyring
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      - --foreground
      - --public-addr=172.26.200.254
      - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: mon
      ports:
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /var/lib/ceph/mon/ceph-a
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    - args:
      - --fsid=742018bb-f74c-4681-bfea-4ab962753821
      - --keyring=/etc/ceph/keyring-store/keyring
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=a
      - --setuser=ceph
      - --setgroup=ceph
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      - --public-addr=172.26.200.254
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-a
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-a/data
        type: ""
      name: ceph-daemon-data
    - name: default-token-k9mpw
      secret:
        defaultMode: 420
        secretName: default-token-k9mpw
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:35Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:36Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:36Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:32Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4f4e85bba9127d9451528bbfe6e8f03ffabccefceee9497018a1334050d454c8
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:35Z"
    hostIP: 10.25.32.135
    initContainerStatuses:
    - containerID: containerd://c7ab7ea5d7ed76ae4a30d9fefce0e768c981c6c22fd1359cf3f1558d2e900d6f
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://c7ab7ea5d7ed76ae4a30d9fefce0e768c981c6c22fd1359cf3f1558d2e900d6f
          exitCode: 0
          finishedAt: "2021-05-03T13:31:33Z"
          reason: Completed
          startedAt: "2021-05-03T13:31:33Z"
    - containerID: containerd://4e6418dfb3cc5bb5a99682757e536ea6cbfac4f811b86fde79be686ab560c019
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://4e6418dfb3cc5bb5a99682757e536ea6cbfac4f811b86fde79be686ab560c019
          exitCode: 0
          finishedAt: "2021-05-03T13:31:34Z"
          reason: Completed
          startedAt: "2021-05-03T13:31:34Z"
    phase: Running
    podIP: 192.168.6.112
    podIPs:
    - ip: 192.168.6.112
    qosClass: BestEffort
    startTime: "2021-05-03T13:31:32Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:31:40Z"
    generateName: rook-ceph-mon-b-6778b88456-
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: b
      ceph_daemon_type: mon
      mon: b
      mon_cluster: rook-ceph
      pod-template-hash: 6778b88456
      rook_cluster: rook-ceph
    name: rook-ceph-mon-b-6778b88456-rxlct
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-b-6778b88456
      uid: 83203b8b-9ae8-4847-a0a4-77ae33974d27
    resourceVersion: "3669"
    uid: c50b3b81-8dbd-4175-b3c7-caceb6348c8f
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=742018bb-f74c-4681-bfea-4ab962753821
      - --keyring=/etc/ceph/keyring-store/keyring
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=b
      - --setuser=ceph
      - --setgroup=ceph
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      - --foreground
      - --public-addr=172.26.167.208
      - --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-mon.b.asok mon_status
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: mon
      ports:
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /var/lib/ceph/mon/ceph-b
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    - args:
      - --fsid=742018bb-f74c-4681-bfea-4ab962753821
      - --keyring=/etc/ceph/keyring-store/keyring
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=b
      - --setuser=ceph
      - --setgroup=ceph
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      - --public-addr=172.26.167.208
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-b
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-b/data
        type: ""
      name: ceph-daemon-data
    - name: default-token-k9mpw
      secret:
        defaultMode: 420
        secretName: default-token-k9mpw
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:48Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:49Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:49Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:43Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d9122380699229c510d94330ab6afd77d5acc6e38b52416ca77c504f4526f36d
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:48Z"
    hostIP: 10.25.32.131
    initContainerStatuses:
    - containerID: containerd://441bc7be43dd4db3a59c033b33c6572f24d13bb8be6fac5e1ea9a9d766a681e5
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://441bc7be43dd4db3a59c033b33c6572f24d13bb8be6fac5e1ea9a9d766a681e5
          exitCode: 0
          finishedAt: "2021-05-03T13:31:46Z"
          reason: Completed
          startedAt: "2021-05-03T13:31:46Z"
    - containerID: containerd://203be6837c4eddaf08fb0947531bbd7eb79c71758c65bf8b81e53146e682c342
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://203be6837c4eddaf08fb0947531bbd7eb79c71758c65bf8b81e53146e682c342
          exitCode: 0
          finishedAt: "2021-05-03T13:31:47Z"
          reason: Completed
          startedAt: "2021-05-03T13:31:47Z"
    phase: Running
    podIP: 192.168.5.117
    podIPs:
    - ip: 192.168.5.117
    qosClass: BestEffort
    startTime: "2021-05-03T13:31:43Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:31:57Z"
    generateName: rook-ceph-mon-c-55ccc87ccb-
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: c
      ceph_daemon_type: mon
      mon: c
      mon_cluster: rook-ceph
      pod-template-hash: 55ccc87ccb
      rook_cluster: rook-ceph
    name: rook-ceph-mon-c-55ccc87ccb-dl8hm
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-mon-c-55ccc87ccb
      uid: fde81815-6cbc-48c0-bf0b-604790ecd453
    resourceVersion: "3780"
    uid: ee807757-87bc-4d29-b77e-b1827435c627
  spec:
    affinity: {}
    containers:
    - args:
      - --fsid=742018bb-f74c-4681-bfea-4ab962753821
      - --keyring=/etc/ceph/keyring-store/keyring
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=c
      - --setuser=ceph
      - --setgroup=ceph
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      - --foreground
      - --public-addr=172.26.84.185
      - --setuser-match-path=/var/lib/ceph/mon/ceph-c/store.db
      - --public-bind-addr=$(ROOK_POD_IP)
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      - name: ROOK_POD_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status
        failureThreshold: 3
        initialDelaySeconds: 10
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: mon
      ports:
      - containerPort: 6789
        name: tcp-msgr1
        protocol: TCP
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-c
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      - /var/lib/ceph/mon/ceph-c
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-c
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    - args:
      - --fsid=742018bb-f74c-4681-bfea-4ab962753821
      - --keyring=/etc/ceph/keyring-store/keyring
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --mon-host=$(ROOK_CEPH_MON_HOST)
      - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
      - --id=c
      - --setuser=ceph
      - --setgroup=ceph
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      - --public-addr=172.26.84.185
      - --mkfs
      command:
      - ceph-mon
      env:
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: ROOK_CEPH_MON_INITIAL_MEMBERS
        valueFrom:
          secretKeyRef:
            key: mon_initial_members
            name: rook-ceph-config
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: init-mon-fs
      resources: {}
      securityContext:
        privileged: false
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /etc/ceph/keyring-store/
        name: rook-ceph-mons-keyring
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /var/lib/ceph/mon/ceph-c
        name: ceph-daemon-data
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - name: rook-ceph-mons-keyring
      secret:
        defaultMode: 420
        secretName: rook-ceph-mons-keyring
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /var/lib/rook/mon-c/data
        type: ""
      name: ceph-daemon-data
    - name: default-token-k9mpw
      secret:
        defaultMode: 420
        secretName: default-token-k9mpw
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:00Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:01Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:01Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:57Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://a17f743f29dd249102cba840203f4e264a86873d6b0b4dff01f7e9a465fa9c09
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: mon
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:00Z"
    hostIP: 10.25.32.133
    initContainerStatuses:
    - containerID: containerd://2300ecbb4faf7e2c1e8de319b61fb7f226fcfb1053b1d60fa33e95bcad8b91f1
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://2300ecbb4faf7e2c1e8de319b61fb7f226fcfb1053b1d60fa33e95bcad8b91f1
          exitCode: 0
          finishedAt: "2021-05-03T13:31:58Z"
          reason: Completed
          startedAt: "2021-05-03T13:31:58Z"
    - containerID: containerd://fc624509658d16bff011c25c5846d9e791ba4a9b031d891179af8e444cde9638
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: init-mon-fs
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://fc624509658d16bff011c25c5846d9e791ba4a9b031d891179af8e444cde9638
          exitCode: 0
          finishedAt: "2021-05-03T13:31:59Z"
          reason: Completed
          startedAt: "2021-05-03T13:31:59Z"
    phase: Running
    podIP: 192.168.0.221
    podIPs:
    - ip: 192.168.0.221
    qosClass: BestEffort
    startTime: "2021-05-03T13:31:57Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:28:35Z"
    generateName: rook-ceph-operator-84c85574d9-
    labels:
      app: rook-ceph-operator
      pod-template-hash: 84c85574d9
    name: rook-ceph-operator-84c85574d9-56t5r
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-operator-84c85574d9
      uid: 68870733-2418-477e-b59b-06f5d3bae8b5
    resourceVersion: "2532"
    uid: 805daec9-8a1c-495a-a715-93405890e34e
  spec:
    containers:
    - args:
      - ceph
      - operator
      env:
      - name: ROOK_CURRENT_NAMESPACE_ONLY
        value: "false"
      - name: ROOK_LOG_LEVEL
        value: INFO
      - name: ROOK_DISCOVER_DEVICES_INTERVAL
        value: 60m
      - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
        value: "false"
      - name: ROOK_ENABLE_SELINUX_RELABELING
        value: "true"
      - name: ROOK_ENABLE_FSGROUP
        value: "true"
      - name: ROOK_DISABLE_DEVICE_HOTPLUG
        value: "false"
      - name: DISCOVER_DAEMON_UDEV_BLACKLIST
        value: (?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+
      - name: ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS
        value: "5"
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.namespace
      image: rook/ceph:master
      imagePullPolicy: IfNotPresent
      name: rook-ceph-operator
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-config
      - mountPath: /etc/ceph
        name: default-config-dir
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-system-token-wb55t
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-system
    serviceAccountName: rook-ceph-system
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - emptyDir: {}
      name: rook-config
    - emptyDir: {}
      name: default-config-dir
    - name: rook-ceph-system-token-wb55t
      secret:
        defaultMode: 420
        secretName: rook-ceph-system-token-wb55t
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:32Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:32Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1749a5add99017d6b06b7b539c95a09407dfadb15858556dc0e4be56aa3ce1ce
      image: docker.io/rook/ceph:master
      imageID: docker.io/rook/ceph@sha256:3863685467eaf11992fd477ceddcc396806d4036cad2d3882b6bbd189e197128
      lastState: {}
      name: rook-ceph-operator
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:29:31Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 192.168.5.3
    podIPs:
    - ip: 192.168.5.3
    qosClass: BestEffort
    startTime: "2021-05-03T13:29:04Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:24Z"
    generateName: rook-ceph-osd-0-56c4fc758d-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "0"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "0"
      pod-template-hash: 56c4fc758d
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-0-56c4fc758d-dnx42
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-0-56c4fc758d
      uid: 4886b261-e1aa-41cf-a1df-39b6a6a3e92d
    resourceVersion: "4375"
    uid: 2798c8f5-e9a6-4c10-b33f-de09cd93de9b
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "0"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: f8e31292-e88c-48ef-9806-f45a7280254f
      - name: ROOK_OSD_ID
        value: "0"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.0.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=0\nOSD_UUID=f8e31292-e88c-48ef-9806-f45a7280254f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-0
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sda
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://4a9e09c0577a12869bf3ce46085e4c2fa2fed5266f549e2f4a279c5610024044
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:27Z"
    hostIP: 10.25.32.131
    initContainerStatuses:
    - containerID: containerd://1af5b773a8cf2d6c54b4a19cde4a6e4ccbbf3c322c59afcc70cef3fd98844124
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://1af5b773a8cf2d6c54b4a19cde4a6e4ccbbf3c322c59afcc70cef3fd98844124
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    - containerID: containerd://af7e8a9fda7958fa258c2ae0219dcbc6e6aabc117c74f0d125e39181be151b04
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://af7e8a9fda7958fa258c2ae0219dcbc6e6aabc117c74f0d125e39181be151b04
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    phase: Running
    podIP: 192.168.4.132
    podIPs:
    - ip: 192.168.4.132
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:24Z"
    generateName: rook-ceph-osd-1-6c7bbc6996-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "1"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "1"
      pod-template-hash: 6c7bbc6996
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-1-6c7bbc6996-wk8tq
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-1-6c7bbc6996
      uid: 42738293-12f9-4480-80a7-598e5c25003d
    resourceVersion: "4427"
    uid: b035de43-3068-473a-b772-8c29ee404b47
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "1"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: cbf296c6-1064-405f-b8a2-fd4ede68c69f
      - name: ROOK_OSD_ID
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.1.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=1\nOSD_UUID=cbf296c6-1064-405f-b8a2-fd4ede68c69f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-1
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sda
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://6b62d0b3a9eace0c5fc14c43988408128a6b8647a9d48a95c85ec92b34535848
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:28Z"
    hostIP: 10.25.32.135
    initContainerStatuses:
    - containerID: containerd://67cfa9d201e17981e5bbcb652c39281819846b8632085dd5952e034a4430f03a
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://67cfa9d201e17981e5bbcb652c39281819846b8632085dd5952e034a4430f03a
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    - containerID: containerd://1a8c35b319a5b15436d528c5e12fccd7fb815128eceb0c6f132176a56479e517
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://1a8c35b319a5b15436d528c5e12fccd7fb815128eceb0c6f132176a56479e517
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:27Z"
    phase: Running
    podIP: 192.168.7.47
    podIPs:
    - ip: 192.168.7.47
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:25Z"
    generateName: rook-ceph-osd-2-64c7556b68-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "2"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "2"
      pod-template-hash: 64c7556b68
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-2-64c7556b68-wcmm7
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-2-64c7556b68
      uid: bc550f2b-aea5-47f0-a90b-e7d050bdc7c9
    resourceVersion: "4405"
    uid: ba114d0b-418b-4745-8a18-c6dfd04d5491
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "2"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: 0794c043-8b9a-4600-9e56-e27358665ea5
      - name: ROOK_OSD_ID
        value: "2"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.2.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=2\nOSD_UUID=0794c043-8b9a-4600-9e56-e27358665ea5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sda
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-2
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sda
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://c47ed2e4c559eddd24979be926aa97bd93ff2411c4241da5817752530d5917ff
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:28Z"
    hostIP: 10.25.32.133
    initContainerStatuses:
    - containerID: containerd://ae0a6e6e1eaad52de80e4b74038ec63a736d8aff55bae4cbcd2fbc7c7aee0853
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://ae0a6e6e1eaad52de80e4b74038ec63a736d8aff55bae4cbcd2fbc7c7aee0853
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    - containerID: containerd://acdc698fe4eed252a5cbae67ceb605da9e7efa9448411cf26ae7bbcbf49a25e9
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://acdc698fe4eed252a5cbae67ceb605da9e7efa9448411cf26ae7bbcbf49a25e9
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:27Z"
    phase: Running
    podIP: 192.168.1.42
    podIPs:
    - ip: 192.168.1.42
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:24Z"
    generateName: rook-ceph-osd-3-6c56cdc4cc-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "3"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "3"
      pod-template-hash: 6c56cdc4cc
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-3-6c56cdc4cc-8lmv6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-3-6c56cdc4cc
      uid: c7fd58e7-cf00-4956-a75d-c006990bc7d3
    resourceVersion: "4367"
    uid: 76c9683e-67ed-4c12-9ff5-b8afdc5be342
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "3"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: 0439f447-d0de-4e7d-8b78-8e112a4181ce
      - name: ROOK_OSD_ID
        value: "3"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.3.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=3\nOSD_UUID=0439f447-d0de-4e7d-8b78-8e112a4181ce\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-3
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sdb
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://cd1b3ea6ca0bad8ef5e4b77c7625f889bd47156d360bde8167eb514fef6b80e3
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:27Z"
    hostIP: 10.25.32.131
    initContainerStatuses:
    - containerID: containerd://f193916331ff86c3a743cf7511c147b686e7b861181588eb1d2c15d0f7001451
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://f193916331ff86c3a743cf7511c147b686e7b861181588eb1d2c15d0f7001451
          exitCode: 0
          finishedAt: "2021-05-03T13:32:25Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:25Z"
    - containerID: containerd://e5fa987e47f765b411c43981488044d1480055079373a47a6c11b776f307e422
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://e5fa987e47f765b411c43981488044d1480055079373a47a6c11b776f307e422
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    phase: Running
    podIP: 192.168.5.130
    podIPs:
    - ip: 192.168.5.130
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:24Z"
    generateName: rook-ceph-osd-4-8fb498649-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "4"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "4"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "4"
      pod-template-hash: 8fb498649
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-4-8fb498649-85ldk
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-4-8fb498649
      uid: 8bebd2e0-7d90-4711-a3db-6ebf4f85d6b0
    resourceVersion: "4452"
    uid: da3adf8d-e5ad-4b32-b83c-9cf3cd5b73a3
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "4"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: a82f8fcd-16ea-47aa-9cf1-634534f103cc
      - name: ROOK_OSD_ID
        value: "4"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.4.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-4
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=4\nOSD_UUID=a82f8fcd-16ea-47aa-9cf1-634534f103cc\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdb
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-4
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-4
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sdb
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://ed558ad7b09beabb5acb9010d814241b14258836fba832808b17eb217e55bf18
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:28Z"
    hostIP: 10.25.32.135
    initContainerStatuses:
    - containerID: containerd://e7ce147819a287bbfa8f9c35dc1995f1bd53c291753f72878fac2c1da0cdcbaa
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://e7ce147819a287bbfa8f9c35dc1995f1bd53c291753f72878fac2c1da0cdcbaa
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    - containerID: containerd://cd4282f62fa5635eb9d0a856809cc8bc7a23a5b21579b88a4a722a8679a7e71c
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://cd4282f62fa5635eb9d0a856809cc8bc7a23a5b21579b88a4a722a8679a7e71c
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:27Z"
    phase: Running
    podIP: 192.168.6.191
    podIPs:
    - ip: 192.168.6.191
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:25Z"
    generateName: rook-ceph-osd-5-6df5f5cc94-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "5"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "5"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "5"
      pod-template-hash: 6df5f5cc94
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-5-6df5f5cc94-bl5l9
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-5-6df5f5cc94
      uid: 04b6e554-879d-4f43-8797-b5ea83165933
    resourceVersion: "4400"
    uid: f38375ce-104a-4bb0-859f-4057456c4882
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "5"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: ba3f17ca-a4d6-4a31-adbb-68c70a0a6196
      - name: ROOK_OSD_ID
        value: "5"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.5.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-5
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=5\nOSD_UUID=ba3f17ca-a4d6-4a31-adbb-68c70a0a6196\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-5
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-5
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sdc
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:25Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://05cb3c2f20015b699303c7a7370d08e336ad9bbbf0054af9b796574413122ae9
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:28Z"
    hostIP: 10.25.32.133
    initContainerStatuses:
    - containerID: containerd://a6d61b0c45c39741f00e91f911d850b5951f3ed3420cf03072e7b12b457e3d0d
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://a6d61b0c45c39741f00e91f911d850b5951f3ed3420cf03072e7b12b457e3d0d
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    - containerID: containerd://a9ac079c34f6aa998fce15f0540c77c7a515a12c92ee4522feb6db1fc0570df2
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://a9ac079c34f6aa998fce15f0540c77c7a515a12c92ee4522feb6db1fc0570df2
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:27Z"
    phase: Running
    podIP: 192.168.0.243
    podIPs:
    - ip: 192.168.0.243
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:25Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:24Z"
    generateName: rook-ceph-osd-6-c97896bb6-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "6"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "6"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "6"
      pod-template-hash: c97896bb6
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-6-c97896bb6-6rllk
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-6-c97896bb6
      uid: f8a2a47a-5c61-4431-b633-d127eb6afd2e
    resourceVersion: "4371"
    uid: d07d844b-3b69-45a1-95bc-fab485cf77e0
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "6"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: 1e2abf51-b10e-4d8b-a409-989eed9ff755
      - name: ROOK_OSD_ID
        value: "6"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.6.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-6
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=6\nOSD_UUID=1e2abf51-b10e-4d8b-a409-989eed9ff755\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-6
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-6
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sdc
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:27Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://5d08b0900ea126e8ddc085c89881db0978b83597dfb06003c64f7e3bc675d93c
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:27Z"
    hostIP: 10.25.32.131
    initContainerStatuses:
    - containerID: containerd://1ec244a6eb51a7626f57674324321e9f88e2eab463942c5d501e3b447bd8ea3b
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://1ec244a6eb51a7626f57674324321e9f88e2eab463942c5d501e3b447bd8ea3b
          exitCode: 0
          finishedAt: "2021-05-03T13:32:25Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:25Z"
    - containerID: containerd://d4f470ac5227914ff36bf1a549a9ec9e060b88e2c9334942c66ac92db24e9cb8
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://d4f470ac5227914ff36bf1a549a9ec9e060b88e2c9334942c66ac92db24e9cb8
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    phase: Running
    podIP: 192.168.5.146
    podIPs:
    - ip: 192.168.5.146
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:24Z"
    generateName: rook-ceph-osd-7-75b5fcc6b-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "7"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "7"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "7"
      pod-template-hash: 75b5fcc6b
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-7-75b5fcc6b-rm5js
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-7-75b5fcc6b
      uid: 84f359c4-c1c3-4800-a612-d5ec2be24f57
    resourceVersion: "4432"
    uid: f204f9cc-c75d-4cbb-b2d3-8bbbbecedb26
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "7"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: a71d8128-94c0-494b-9e3a-e47b2341d1d7
      - name: ROOK_OSD_ID
        value: "7"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.7.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-7
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=7\nOSD_UUID=a71d8128-94c0-494b-9e3a-e47b2341d1d7\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdc
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-7
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-7
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sdc
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:24Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d3389fb9afab89ef10f3ca259fa40f6c030825a71dd95030946eeed59dbeb3c2
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:28Z"
    hostIP: 10.25.32.135
    initContainerStatuses:
    - containerID: containerd://9e03c7c4facfd4cfabd77c5de5518cf64fbb1ece6d3e4179dc25418a4bdbd84b
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://9e03c7c4facfd4cfabd77c5de5518cf64fbb1ece6d3e4179dc25418a4bdbd84b
          exitCode: 0
          finishedAt: "2021-05-03T13:32:26Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:26Z"
    - containerID: containerd://c2b967045e1a5503e22f226f96be74a7b89e073f0ad715b36e0a14d5cd1287f4
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://c2b967045e1a5503e22f226f96be74a7b89e073f0ad715b36e0a14d5cd1287f4
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:27Z"
    phase: Running
    podIP: 192.168.6.129
    podIPs:
    - ip: 192.168.6.129
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:24Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:26Z"
    generateName: rook-ceph-osd-8-69865cc7ff-
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "8"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "8"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "8"
      pod-template-hash: 69865cc7ff
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-8-69865cc7ff-kqvw7
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-osd-8-69865cc7ff
      uid: 7a9a6d7d-56f6-48e2-a6fa-a5b060efd353
    resourceVersion: "4402"
    uid: a7af5038-ba15-4da0-aee9-ed24c0beff0b
  spec:
    affinity: {}
    containers:
    - args:
      - --foreground
      - --id
      - "8"
      - --fsid
      - 742018bb-f74c-4681-bfea-4ab962753821
      - --setuser
      - ceph
      - --setgroup
      - ceph
      - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        region=fr2
      - --log-to-stderr=true
      - --err-to-stderr=true
      - --mon-cluster-log-to-stderr=true
      - '--log-stderr-prefix=debug '
      - --default-log-to-file=false
      - --default-mon-cluster-log-to-file=false
      - --ms-learn-addr-from-peer=false
      - --ms-bind-ipv4=true
      - --ms-bind-ipv6=false
      command:
      - ceph-osd
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: TINI_SUBREAPER
      - name: CONTAINER_IMAGE
        value: ceph/ceph:v15.2.11
      - name: POD_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: metadata.name
      - name: POD_MEMORY_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: limits.memory
      - name: POD_MEMORY_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.memory
      - name: POD_CPU_LIMIT
        valueFrom:
          resourceFieldRef:
            divisor: "1"
            resource: limits.cpu
      - name: POD_CPU_REQUEST
        valueFrom:
          resourceFieldRef:
            divisor: "0"
            resource: requests.cpu
      - name: ROOK_OSD_UUID
        value: af0ac0df-b3d4-425b-ac37-b28f7e40a590
      - name: ROOK_OSD_ID
        value: "8"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdd
      - name: ROOK_CV_MODE
        value: raw
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      livenessProbe:
        exec:
          command:
          - env
          - -i
          - sh
          - -c
          - ceph --admin-daemon /run/ceph/ceph-osd.8.asok status
        failureThreshold: 3
        initialDelaySeconds: 45
        periodSeconds: 10
        successThreshold: 1
        timeoutSeconds: 1
      name: osd
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-8
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
      workingDir: /var/log/ceph
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - command:
      - /bin/bash
      - -c
      - "\nset -ex\n\nOSD_ID=8\nOSD_UUID=af0ac0df-b3d4-425b-ac37-b28f7e40a590\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"
      env:
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_CEPH_MON_HOST
        valueFrom:
          secretKeyRef:
            key: mon_host
            name: rook-ceph-config
      - name: CEPH_ARGS
        value: -m $(ROOK_CEPH_MON_HOST)
      - name: ROOK_BLOCK_PATH
        value: /dev/sdd
      - name: ROOK_METADATA_DEVICE
      - name: ROOK_WAL_DEVICE
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: activate
      resources: {}
      securityContext:
        privileged: true
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/ceph/osd/ceph-8
        name: activate-osd
      - mountPath: /dev
        name: devices
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    - args:
      - --verbose
      - --recursive
      - ceph:ceph
      - /var/log/ceph
      - /var/lib/ceph/crash
      command:
      - chown
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: chown-container-data-dir
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: rook-config-override
        readOnly: true
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: run-udev
      - mountPath: /var/lib/ceph/osd/ceph-8
        name: activate-osd
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - name: rook-config-override
      projected:
        defaultMode: 420
        sources:
        - configMap:
            items:
            - key: config
              mode: 292
              path: ceph.conf
            name: rook-config-override
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: run-udev
    - hostPath:
        path: /var/lib/rook/rook-ceph/_dev_sdd
        type: DirectoryOrCreate
      name: activate-osd
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:28Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:29Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:26Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://9fa523c9e00f85de2be23ff9c02d3523ea25901695f7f80e300989621317fee6
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: osd
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:32:28Z"
    hostIP: 10.25.32.133
    initContainerStatuses:
    - containerID: containerd://b70373f24bb7f03e59cf6a553882bb8094578b1075797ca8bea054bd3c7c22f2
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: activate
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://b70373f24bb7f03e59cf6a553882bb8094578b1075797ca8bea054bd3c7c22f2
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:27Z"
    - containerID: containerd://b47f51d3e9d5a0007aa0b57cb4a5104ed977bda0f1ee18a13c9cbb8bd6613798
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: chown-container-data-dir
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://b47f51d3e9d5a0007aa0b57cb4a5104ed977bda0f1ee18a13c9cbb8bd6613798
          exitCode: 0
          finishedAt: "2021-05-03T13:32:27Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:27Z"
    phase: Running
    podIP: 192.168.1.124
    podIPs:
    - ip: 192.168.1.124
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:26Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:44Z"
    generateName: rook-ceph-osd-prepare-6f643bc7106c83d1bb0a9274a2947bc0-
    labels:
      app: rook-ceph-osd-prepare
      ceph.rook.io/pvc: ""
      controller-uid: 85d20e1d-7757-4238-a7a3-401aeeaa7366
      job-name: rook-ceph-osd-prepare-6f643bc7106c83d1bb0a9274a2947bc0
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-6f643bc7106c83d1bb0a9274a2947bc0-tsfsl
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-6f643bc7106c83d1bb0a9274a2947bc0
      uid: 85d20e1d-7757-4238-a7a3-401aeeaa7366
    resourceVersion: "4600"
    uid: 921d86ac-da15-47db-a765-7059ff9cce06
  spec:
    affinity: {}
    containers:
    - args:
      - --
      - /rook/rook
      - ceph
      - osd
      - provision
      command:
      - /rook/tini
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: all
      - name: ROOK_CEPH_VERSION
        value: ceph version 15.2.11-0 octopus
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: provision
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - copy-binaries
      - --copy-to-dir
      - /rook
      image: rook/ceph:master
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:46Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:48Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:48Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:44Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://0924d9a7df977d89dbde3d62bbf753b738d7f6532367541edca930d19a40659d
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://0924d9a7df977d89dbde3d62bbf753b738d7f6532367541edca930d19a40659d
          exitCode: 0
          finishedAt: "2021-05-03T13:32:47Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:46Z"
    hostIP: 10.25.32.135
    initContainerStatuses:
    - containerID: containerd://10513d0bd457060ff3b02963f4d4373efcd1b21fc657f9be6bdb1b884c9ea480
      image: docker.io/rook/ceph:master
      imageID: docker.io/rook/ceph@sha256:3863685467eaf11992fd477ceddcc396806d4036cad2d3882b6bbd189e197128
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://10513d0bd457060ff3b02963f4d4373efcd1b21fc657f9be6bdb1b884c9ea480
          exitCode: 0
          finishedAt: "2021-05-03T13:32:44Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:44Z"
    phase: Succeeded
    podIP: 192.168.6.36
    podIPs:
    - ip: 192.168.6.36
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:44Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:48Z"
    generateName: rook-ceph-osd-prepare-ceeab05259c8d89b0904c022bc5f8be0-
    labels:
      app: rook-ceph-osd-prepare
      ceph.rook.io/pvc: ""
      controller-uid: 4b4bff43-200a-4b1d-b614-d503218be780
      job-name: rook-ceph-osd-prepare-ceeab05259c8d89b0904c022bc5f8be0
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-ceeab05259c8d89b0904c022bc5f8be0-lcr8k
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-ceeab05259c8d89b0904c022bc5f8be0
      uid: 4b4bff43-200a-4b1d-b614-d503218be780
    resourceVersion: "4650"
    uid: 9b7c72f9-96f9-451e-9cd3-a5f7a33f8e83
  spec:
    affinity: {}
    containers:
    - args:
      - --
      - /rook/rook
      - ceph
      - osd
      - provision
      command:
      - /rook/tini
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: all
      - name: ROOK_CEPH_VERSION
        value: ceph version 15.2.11-0 octopus
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: provision
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - copy-binaries
      - --copy-to-dir
      - /rook
      image: rook/ceph:master
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:49Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:51Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:51Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:48Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://930890bd02000b5b9668fa14f3175001dccd9b8bc72f6c9d435fd5b0b3998c0a
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://930890bd02000b5b9668fa14f3175001dccd9b8bc72f6c9d435fd5b0b3998c0a
          exitCode: 0
          finishedAt: "2021-05-03T13:32:50Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:49Z"
    hostIP: 10.25.32.133
    initContainerStatuses:
    - containerID: containerd://a3a8912d584a657cab0c95f3714a9388d36c59f90b8a7f38f9b9972efb50def9
      image: docker.io/rook/ceph:master
      imageID: docker.io/rook/ceph@sha256:3863685467eaf11992fd477ceddcc396806d4036cad2d3882b6bbd189e197128
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://a3a8912d584a657cab0c95f3714a9388d36c59f90b8a7f38f9b9972efb50def9
          exitCode: 0
          finishedAt: "2021-05-03T13:32:49Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:48Z"
    phase: Succeeded
    podIP: 192.168.0.37
    podIPs:
    - ip: 192.168.0.37
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:48Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:32:46Z"
    generateName: rook-ceph-osd-prepare-fb5b167868730b399f51c41bfe43cdc0-
    labels:
      app: rook-ceph-osd-prepare
      ceph.rook.io/pvc: ""
      controller-uid: 287f95e9-0081-4b46-965e-33e4f7634769
      job-name: rook-ceph-osd-prepare-fb5b167868730b399f51c41bfe43cdc0
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-fb5b167868730b399f51c41bfe43cdc0-cbjn6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: batch/v1
      blockOwnerDeletion: true
      controller: true
      kind: Job
      name: rook-ceph-osd-prepare-fb5b167868730b399f51c41bfe43cdc0
      uid: 287f95e9-0081-4b46-965e-33e4f7634769
    resourceVersion: "4638"
    uid: dbde6347-9792-4fe7-9a76-d4e90a97c033
  spec:
    affinity: {}
    containers:
    - args:
      - --
      - /rook/rook
      - ceph
      - osd
      - provision
      command:
      - /rook/tini
      env:
      - name: ROOK_NODE_NAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      - name: ROOK_CLUSTER_ID
        value: 3d3233b3-387a-4444-85cf-8ae39d38e989
      - name: ROOK_CLUSTER_NAME
        value: rook-ceph
      - name: ROOK_PRIVATE_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: ROOK_PUBLIC_IP
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: status.podIP
      - name: POD_NAMESPACE
        value: rook-ceph
      - name: ROOK_MON_ENDPOINTS
        valueFrom:
          configMapKeyRef:
            key: data
            name: rook-ceph-mon-endpoints
      - name: ROOK_MON_SECRET
        valueFrom:
          secretKeyRef:
            key: mon-secret
            name: rook-ceph-mon
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      - name: ROOK_CONFIG_DIR
        value: /var/lib/rook
      - name: ROOK_CEPH_CONFIG_OVERRIDE
        value: /etc/rook/config/override.conf
      - name: ROOK_FSID
        valueFrom:
          secretKeyRef:
            key: fsid
            name: rook-ceph-mon
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            apiVersion: v1
            fieldPath: spec.nodeName
      - name: ROOK_CRUSHMAP_ROOT
        value: default
      - name: ROOK_CRUSHMAP_HOSTNAME
        value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      - name: CEPH_VOLUME_DEBUG
        value: "1"
      - name: CEPH_VOLUME_SKIP_RESTORECON
        value: "1"
      - name: DM_DISABLE_UDEV
        value: "1"
      - name: ROOK_OSDS_PER_DEVICE
        value: "1"
      - name: ROOK_LOG_LEVEL
        value: DEBUG
      - name: ROOK_DATA_DEVICE_FILTER
        value: all
      - name: ROOK_CEPH_VERSION
        value: ceph version 15.2.11-0 octopus
      - name: ROOK_OSD_CRUSH_DEVICE_CLASS
      - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
      image: ceph/ceph:v15.2.11
      imagePullPolicy: IfNotPresent
      name: provision
      resources: {}
      securityContext:
        privileged: true
        readOnlyRootFilesystem: false
        runAsNonRoot: false
        runAsUser: 0
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /var/lib/rook
        name: rook-data
      - mountPath: /etc/ceph
        name: ceph-conf-emptydir
      - mountPath: /var/log/ceph
        name: rook-ceph-log
      - mountPath: /var/lib/ceph/crash
        name: rook-ceph-crash
      - mountPath: /dev
        name: devices
      - mountPath: /run/udev
        name: udev
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /rootfs
        name: rootfs
        readOnly: true
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    dnsPolicy: ClusterFirst
    enableServiceLinks: true
    initContainers:
    - args:
      - copy-binaries
      - --copy-to-dir
      - /rook
      image: rook/ceph:master
      imagePullPolicy: IfNotPresent
      name: copy-bins
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /rook
        name: rook-binaries
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: rook-ceph-osd-token-sdsmk
        readOnly: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    nodeSelector:
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: OnFailure
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: rook-ceph-osd
    serviceAccountName: rook-ceph-osd
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - hostPath:
        path: /var/lib/rook
        type: ""
      name: rook-data
    - emptyDir: {}
      name: ceph-conf-emptydir
    - hostPath:
        path: /var/lib/rook/rook-ceph/log
        type: ""
      name: rook-ceph-log
    - hostPath:
        path: /var/lib/rook/rook-ceph/crash
        type: ""
      name: rook-ceph-crash
    - emptyDir: {}
      name: rook-binaries
    - hostPath:
        path: /dev
        type: ""
      name: devices
    - hostPath:
        path: /run/udev
        type: ""
      name: udev
    - hostPath:
        path: /
        type: ""
      name: rootfs
    - name: rook-ceph-osd-token-sdsmk
      secret:
        defaultMode: 420
        secretName: rook-ceph-osd-token-sdsmk
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:47Z"
      reason: PodCompleted
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:49Z"
      reason: PodCompleted
      status: "False"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:49Z"
      reason: PodCompleted
      status: "False"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:32:46Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://d74f1e86ddcbfebe3acdc56eef30ffe8906b04023504911a892912ad231d9fe1
      image: docker.io/ceph/ceph:v15.2.11
      imageID: docker.io/ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
      lastState: {}
      name: provision
      ready: false
      restartCount: 0
      started: false
      state:
        terminated:
          containerID: containerd://d74f1e86ddcbfebe3acdc56eef30ffe8906b04023504911a892912ad231d9fe1
          exitCode: 0
          finishedAt: "2021-05-03T13:32:49Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:47Z"
    hostIP: 10.25.32.131
    initContainerStatuses:
    - containerID: containerd://ccbff7df451957a86949427441da4f1f83682b7e12d9d14e1999e915e17c3703
      image: docker.io/rook/ceph:master
      imageID: docker.io/rook/ceph@sha256:3863685467eaf11992fd477ceddcc396806d4036cad2d3882b6bbd189e197128
      lastState: {}
      name: copy-bins
      ready: true
      restartCount: 0
      state:
        terminated:
          containerID: containerd://ccbff7df451957a86949427441da4f1f83682b7e12d9d14e1999e915e17c3703
          exitCode: 0
          finishedAt: "2021-05-03T13:32:47Z"
          reason: Completed
          startedAt: "2021-05-03T13:32:46Z"
    phase: Succeeded
    podIP: 192.168.5.91
    podIPs:
    - ip: 192.168.5.91
    qosClass: BestEffort
    startTime: "2021-05-03T13:32:46Z"
- apiVersion: v1
  kind: Pod
  metadata:
    creationTimestamp: "2021-05-03T13:28:39Z"
    generateName: rook-ceph-tools-78cdfd976c-
    labels:
      app: rook-ceph-tools
      pod-template-hash: 78cdfd976c
    name: rook-ceph-tools-78cdfd976c-jx79j
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: ReplicaSet
      name: rook-ceph-tools-78cdfd976c
      uid: 2bd47236-e82b-413a-b01c-3d4278380da0
    resourceVersion: "3308"
    uid: 7b84be38-cab4-4d50-99e0-d0f4bb1c77d5
  spec:
    containers:
    - args:
      - -g
      - --
      - /usr/local/bin/toolbox.sh
      command:
      - /tini
      env:
      - name: ROOK_CEPH_USERNAME
        valueFrom:
          secretKeyRef:
            key: ceph-username
            name: rook-ceph-mon
      - name: ROOK_CEPH_SECRET
        valueFrom:
          secretKeyRef:
            key: ceph-secret
            name: rook-ceph-mon
      image: rook/ceph:master
      imagePullPolicy: IfNotPresent
      name: rook-ceph-tools
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
      - mountPath: /etc/ceph
        name: ceph-config
      - mountPath: /etc/rook
        name: mon-endpoint-volume
      - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        name: default-token-k9mpw
        readOnly: true
    dnsPolicy: ClusterFirstWithHostNet
    enableServiceLinks: true
    nodeName: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    preemptionPolicy: PreemptLowerPriority
    priority: 0
    restartPolicy: Always
    schedulerName: default-scheduler
    securityContext: {}
    serviceAccount: default
    serviceAccountName: default
    terminationGracePeriodSeconds: 30
    tolerations:
    - effect: NoExecute
      key: node.kubernetes.io/unreachable
      operator: Exists
      tolerationSeconds: 5
    - effect: NoExecute
      key: node.kubernetes.io/not-ready
      operator: Exists
      tolerationSeconds: 300
    volumes:
    - configMap:
        defaultMode: 420
        items:
        - key: data
          path: mon-endpoints
        name: rook-ceph-mon-endpoints
      name: mon-endpoint-volume
    - emptyDir: {}
      name: ceph-config
    - name: default-token-k9mpw
      secret:
        defaultMode: 420
        secretName: default-token-k9mpw
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:04Z"
      status: "True"
      type: Initialized
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:21Z"
      status: "True"
      type: Ready
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:31:21Z"
      status: "True"
      type: ContainersReady
    - lastProbeTime: null
      lastTransitionTime: "2021-05-03T13:29:04Z"
      status: "True"
      type: PodScheduled
    containerStatuses:
    - containerID: containerd://1529364a1228118b5af4e05aca66e748c2a31c9b61ae36a3cd8e73eeb5488a5f
      image: docker.io/rook/ceph:master
      imageID: docker.io/rook/ceph@sha256:3863685467eaf11992fd477ceddcc396806d4036cad2d3882b6bbd189e197128
      lastState: {}
      name: rook-ceph-tools
      ready: true
      restartCount: 0
      started: true
      state:
        running:
          startedAt: "2021-05-03T13:31:20Z"
    hostIP: 10.25.32.131
    phase: Running
    podIP: 192.168.5.49
    podIPs:
    - ip: 192.168.5.49
    qosClass: BestEffort
    startTime: "2021-05-03T13:29:04Z"
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"k8s-app":"hubble-relay"},"name":"hubble-relay","namespace":"cilium"},"spec":{"ports":[{"port":80,"protocol":"TCP","targetPort":4245}],"selector":{"k8s-app":"hubble-relay"},"type":"ClusterIP"}}
    creationTimestamp: "2021-05-03T13:28:23Z"
    labels:
      k8s-app: hubble-relay
    name: hubble-relay
    namespace: cilium
    resourceVersion: "1731"
    uid: e39a5d8a-3436-4dc2-b0be-7a6b78f98273
  spec:
    clusterIP: 172.26.143.218
    clusterIPs:
    - 172.26.143.218
    ports:
    - port: 80
      protocol: TCP
      targetPort: 4245
    selector:
      k8s-app: hubble-relay
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"hubble-ui","namespace":"cilium"},"spec":{"ports":[{"name":"http","port":80,"targetPort":12000}],"selector":{"k8s-app":"hubble-ui"},"type":"ClusterIP"}}
    creationTimestamp: "2021-05-03T13:28:23Z"
    name: hubble-ui
    namespace: cilium
    resourceVersion: "1738"
    uid: 01bca63a-ef70-44f2-95d3-2cef9e39cd03
  spec:
    clusterIP: 172.26.97.243
    clusterIPs:
    - 172.26.97.243
    ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 12000
    selector:
      k8s-app: hubble-ui
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"klustered","namespace":"default"},"spec":{"ports":[{"port":8080}],"selector":{"app":"klustered"}}}
    creationTimestamp: "2021-05-03T13:28:46Z"
    name: klustered
    namespace: default
    resourceVersion: "2201"
    uid: 8cf12bbd-901e-4292-9d03-a08397b13a4c
  spec:
    clusterIP: 172.26.135.249
    clusterIPs:
    - 172.26.135.249
    ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
    selector:
      app: klustered
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2021-05-03T13:14:50Z"
    labels:
      component: apiserver
      provider: kubernetes
    name: kubernetes
    namespace: default
    resourceVersion: "196"
    uid: 39b696c2-ef7b-4b1f-a6e6-4f9a768d69ec
  spec:
    clusterIP: 172.26.0.1
    clusterIPs:
    - 172.26.0.1
    ports:
    - name: https
      port: 443
      protocol: TCP
      targetPort: 6443
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"postgres","namespace":"default"},"spec":{"ports":[{"port":5432}],"selector":{"app":"postgresql"}}}
    creationTimestamp: "2021-05-03T13:28:45Z"
    name: postgres
    namespace: default
    resourceVersion: "2188"
    uid: 05c38dce-e599-4392-99f9-8b34799c1ecc
  spec:
    clusterIP: 172.26.105.226
    clusterIPs:
    - 172.26.105.226
    ports:
    - port: 5432
      protocol: TCP
      targetPort: 5432
    selector:
      app: postgresql
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    annotations:
      prometheus.io/port: "9153"
      prometheus.io/scrape: "true"
    creationTimestamp: "2021-05-03T13:14:52Z"
    labels:
      k8s-app: kube-dns
      kubernetes.io/cluster-service: "true"
      kubernetes.io/name: KubeDNS
    name: kube-dns
    namespace: kube-system
    resourceVersion: "232"
    uid: e38b9712-573c-4c8f-bef7-e485cd999e76
  spec:
    clusterIP: 172.26.0.10
    clusterIPs:
    - 172.26.0.10
    ports:
    - name: dns
      port: 53
      protocol: UDP
      targetPort: 53
    - name: dns-tcp
      port: 53
      protocol: TCP
      targetPort: 53
    - name: metrics
      port: 9153
      protocol: TCP
      targetPort: 9153
    selector:
      k8s-app: kube-dns
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2021-05-03T13:30:31Z"
    labels:
      app: csi-metrics
    name: csi-cephfsplugin-metrics
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 02df6b1e-2271-4cf7-be8a-89315e8c2fbb
    resourceVersion: "2866"
    uid: d9504859-d760-4a47-891b-6fa70c762b5e
  spec:
    clusterIP: 172.26.204.21
    clusterIPs:
    - 172.26.204.21
    ports:
    - name: csi-http-metrics
      port: 8080
      protocol: TCP
      targetPort: 9081
    - name: csi-grpc-metrics
      port: 8081
      protocol: TCP
      targetPort: 9091
    selector:
      contains: csi-cephfsplugin-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2021-05-03T13:30:30Z"
    labels:
      app: csi-metrics
    name: csi-rbdplugin-metrics
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 02df6b1e-2271-4cf7-be8a-89315e8c2fbb
    resourceVersion: "2795"
    uid: 35c30412-df92-4b31-93b2-5ffbd27546d2
  spec:
    clusterIP: 172.26.54.41
    clusterIPs:
    - 172.26.54.41
    ports:
    - name: csi-http-metrics
      port: 8080
      protocol: TCP
      targetPort: 9080
    - name: csi-grpc-metrics
      port: 8081
      protocol: TCP
      targetPort: 9090
    selector:
      contains: csi-rbdplugin-metrics
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2021-05-03T13:32:11Z"
    labels:
      app: rook-ceph-mgr
      ceph_daemon_id: a
      rook_cluster: rook-ceph
    name: rook-ceph-mgr
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3887"
    uid: 3e7780d9-3526-4b15-afeb-462702fc8fa3
  spec:
    clusterIP: 172.26.56.19
    clusterIPs:
    - 172.26.56.19
    ports:
    - name: http-metrics
      port: 9283
      protocol: TCP
      targetPort: 9283
    selector:
      app: rook-ceph-mgr
      ceph_daemon_id: a
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2021-05-03T13:32:11Z"
    labels:
      app: rook-ceph-mgr
      ceph_daemon_id: a
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-dashboard
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3885"
    uid: f546ef8a-9c8b-4dc6-90ae-36c8bddf4ec5
  spec:
    clusterIP: 172.26.192.40
    clusterIPs:
    - 172.26.192.40
    ports:
    - name: https-dashboard
      port: 8443
      protocol: TCP
      targetPort: 8443
    selector:
      app: rook-ceph-mgr
      ceph_daemon_id: a
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2021-05-03T13:31:26Z"
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3429"
    uid: e39f8147-d7cd-462b-8050-bed2122655a8
  spec:
    clusterIP: 172.26.200.254
    clusterIPs:
    - 172.26.200.254
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: a
      mon: a
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2021-05-03T13:31:39Z"
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: b
      ceph_daemon_type: mon
      mon: b
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3560"
    uid: 9852add0-bd39-40b8-a4c1-bd236f382391
  spec:
    clusterIP: 172.26.167.208
    clusterIPs:
    - 172.26.167.208
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: b
      mon: b
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: v1
  kind: Service
  metadata:
    creationTimestamp: "2021-05-03T13:31:55Z"
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: c
      ceph_daemon_type: mon
      mon: c
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    name: rook-ceph-mon-c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3704"
    uid: 6709c52f-f643-40f0-b8b3-6bb4e31dd481
  spec:
    clusterIP: 172.26.84.185
    clusterIPs:
    - 172.26.84.185
    ports:
    - name: tcp-msgr1
      port: 6789
      protocol: TCP
      targetPort: 6789
    - name: tcp-msgr2
      port: 3300
      protocol: TCP
      targetPort: 3300
    selector:
      app: rook-ceph-mon
      ceph_daemon_id: c
      mon: c
      mon_cluster: rook-ceph
      rook_cluster: rook-ceph
    sessionAffinity: None
    type: ClusterIP
  status:
    loadBalancer: {}
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"k8s-app":"cilium"},"name":"cilium","namespace":"cilium"},"spec":{"selector":{"matchLabels":{"k8s-app":"cilium"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"k8s-app":"cilium"}},"spec":{"affinity":{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"k8s-app","operator":"In","values":["cilium"]}]},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["--config-dir=/tmp/cilium/config-map"],"command":["cilium-agent"],"env":[{"name":"K8S_NODE_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"CILIUM_K8S_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}},{"name":"CILIUM_FLANNEL_MASTER_DEVICE","valueFrom":{"configMapKeyRef":{"key":"flannel-master-device","name":"cilium-config","optional":true}}},{"name":"CILIUM_FLANNEL_UNINSTALL_ON_EXIT","valueFrom":{"configMapKeyRef":{"key":"flannel-uninstall-on-exit","name":"cilium-config","optional":true}}},{"name":"CILIUM_CLUSTERMESH_CONFIG","value":"/var/lib/cilium/clustermesh/"},{"name":"CILIUM_CNI_CHAINING_MODE","valueFrom":{"configMapKeyRef":{"key":"cni-chaining-mode","name":"cilium-config","optional":true}}},{"name":"CILIUM_CUSTOM_CNI_CONF","valueFrom":{"configMapKeyRef":{"key":"custom-cni-conf","name":"cilium-config","optional":true}}}],"image":"docker.io/cilium/cilium:v1.8.5","imagePullPolicy":"IfNotPresent","lifecycle":{"postStart":{"exec":{"command":["/cni-install.sh","--enable-debug=false"]}},"preStop":{"exec":{"command":["/cni-uninstall.sh"]}}},"livenessProbe":{"failureThreshold":10,"httpGet":{"host":"127.0.0.1","httpHeaders":[{"name":"brief","value":"true"}],"path":"/healthz","port":9876,"scheme":"HTTP"},"initialDelaySeconds":120,"periodSeconds":30,"successThreshold":1,"timeoutSeconds":5},"name":"cilium-agent","readinessProbe":{"failureThreshold":3,"httpGet":{"host":"127.0.0.1","httpHeaders":[{"name":"brief","value":"true"}],"path":"/healthz","port":9876,"scheme":"HTTP"},"initialDelaySeconds":5,"periodSeconds":30,"successThreshold":1,"timeoutSeconds":5},"securityContext":{"capabilities":{"add":["NET_ADMIN","SYS_MODULE"]},"privileged":true},"volumeMounts":[{"mountPath":"/sys/fs/bpf","name":"bpf-maps"},{"mountPath":"/var/run/cilium","name":"cilium-run"},{"mountPath":"/host/opt/cni/bin","name":"cni-path"},{"mountPath":"/host/etc/cni/net.d","name":"etc-cni-netd"},{"mountPath":"/var/lib/cilium/clustermesh","name":"clustermesh-secrets","readOnly":true},{"mountPath":"/tmp/cilium/config-map","name":"cilium-config-path","readOnly":true},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/run/xtables.lock","name":"xtables-lock"}]}],"hostNetwork":true,"initContainers":[{"command":["/init-container.sh"],"env":[{"name":"CILIUM_ALL_STATE","valueFrom":{"configMapKeyRef":{"key":"clean-cilium-state","name":"cilium-config","optional":true}}},{"name":"CILIUM_BPF_STATE","valueFrom":{"configMapKeyRef":{"key":"clean-cilium-bpf-state","name":"cilium-config","optional":true}}},{"name":"CILIUM_WAIT_BPF_MOUNT","valueFrom":{"configMapKeyRef":{"key":"wait-bpf-mount","name":"cilium-config","optional":true}}}],"image":"docker.io/cilium/cilium:v1.8.5","imagePullPolicy":"IfNotPresent","name":"clean-cilium-state","resources":{"requests":{"cpu":"100m","memory":"100Mi"}},"securityContext":{"capabilities":{"add":["NET_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/sys/fs/bpf","mountPropagation":"HostToContainer","name":"bpf-maps"},{"mountPath":"/var/run/cilium","name":"cilium-run"}]}],"priorityClassName":"system-node-critical","restartPolicy":"Always","serviceAccount":"cilium","serviceAccountName":"cilium","terminationGracePeriodSeconds":1,"tolerations":[{"operator":"Exists"}],"volumes":[{"hostPath":{"path":"/var/run/cilium","type":"DirectoryOrCreate"},"name":"cilium-run"},{"hostPath":{"path":"/sys/fs/bpf","type":"DirectoryOrCreate"},"name":"bpf-maps"},{"hostPath":{"path":"/opt/cni/bin","type":"DirectoryOrCreate"},"name":"cni-path"},{"hostPath":{"path":"/etc/cni/net.d","type":"DirectoryOrCreate"},"name":"etc-cni-netd"},{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/run/xtables.lock","type":"FileOrCreate"},"name":"xtables-lock"},{"name":"clustermesh-secrets","secret":{"defaultMode":420,"optional":true,"secretName":"cilium-clustermesh"}},{"configMap":{"name":"cilium-config"},"name":"cilium-config-path"}]}},"updateStrategy":{"rollingUpdate":{"maxUnavailable":2},"type":"RollingUpdate"}}}
    creationTimestamp: "2021-05-03T13:28:23Z"
    generation: 1
    labels:
      k8s-app: cilium
    name: cilium
    namespace: cilium
    resourceVersion: "2428"
    uid: 389969c7-854b-4159-ac1a-42689975152a
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: cilium
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          k8s-app: cilium
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: k8s-app
                  operator: In
                  values:
                  - cilium
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --config-dir=/tmp/cilium/config-map
          command:
          - cilium-agent
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CILIUM_FLANNEL_MASTER_DEVICE
            valueFrom:
              configMapKeyRef:
                key: flannel-master-device
                name: cilium-config
                optional: true
          - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT
            valueFrom:
              configMapKeyRef:
                key: flannel-uninstall-on-exit
                name: cilium-config
                optional: true
          - name: CILIUM_CLUSTERMESH_CONFIG
            value: /var/lib/cilium/clustermesh/
          - name: CILIUM_CNI_CHAINING_MODE
            valueFrom:
              configMapKeyRef:
                key: cni-chaining-mode
                name: cilium-config
                optional: true
          - name: CILIUM_CUSTOM_CNI_CONF
            valueFrom:
              configMapKeyRef:
                key: custom-cni-conf
                name: cilium-config
                optional: true
          image: docker.io/cilium/cilium:v1.8.5
          imagePullPolicy: IfNotPresent
          lifecycle:
            postStart:
              exec:
                command:
                - /cni-install.sh
                - --enable-debug=false
            preStop:
              exec:
                command:
                - /cni-uninstall.sh
          livenessProbe:
            failureThreshold: 10
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9876
              scheme: HTTP
            initialDelaySeconds: 120
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          name: cilium-agent
          readinessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              httpHeaders:
              - name: brief
                value: "true"
              path: /healthz
              port: 9876
              scheme: HTTP
            initialDelaySeconds: 5
            periodSeconds: 30
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
              - SYS_MODULE
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /sys/fs/bpf
            name: bpf-maps
          - mountPath: /var/run/cilium
            name: cilium-run
          - mountPath: /host/opt/cni/bin
            name: cni-path
          - mountPath: /host/etc/cni/net.d
            name: etc-cni-netd
          - mountPath: /var/lib/cilium/clustermesh
            name: clustermesh-secrets
            readOnly: true
          - mountPath: /tmp/cilium/config-map
            name: cilium-config-path
            readOnly: true
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /run/xtables.lock
            name: xtables-lock
        dnsPolicy: ClusterFirst
        hostNetwork: true
        initContainers:
        - command:
          - /init-container.sh
          env:
          - name: CILIUM_ALL_STATE
            valueFrom:
              configMapKeyRef:
                key: clean-cilium-state
                name: cilium-config
                optional: true
          - name: CILIUM_BPF_STATE
            valueFrom:
              configMapKeyRef:
                key: clean-cilium-bpf-state
                name: cilium-config
                optional: true
          - name: CILIUM_WAIT_BPF_MOUNT
            valueFrom:
              configMapKeyRef:
                key: wait-bpf-mount
                name: cilium-config
                optional: true
          image: docker.io/cilium/cilium:v1.8.5
          imagePullPolicy: IfNotPresent
          name: clean-cilium-state
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          securityContext:
            capabilities:
              add:
              - NET_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /sys/fs/bpf
            mountPropagation: HostToContainer
            name: bpf-maps
          - mountPath: /var/run/cilium
            name: cilium-run
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cilium
        serviceAccountName: cilium
        terminationGracePeriodSeconds: 1
        tolerations:
        - operator: Exists
        volumes:
        - hostPath:
            path: /var/run/cilium
            type: DirectoryOrCreate
          name: cilium-run
        - hostPath:
            path: /sys/fs/bpf
            type: DirectoryOrCreate
          name: bpf-maps
        - hostPath:
            path: /opt/cni/bin
            type: DirectoryOrCreate
          name: cni-path
        - hostPath:
            path: /etc/cni/net.d
            type: DirectoryOrCreate
          name: etc-cni-netd
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - name: clustermesh-secrets
          secret:
            defaultMode: 420
            optional: true
            secretName: cilium-clustermesh
        - configMap:
            defaultMode: 420
            name: cilium-config
          name: cilium-config-path
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 2
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2021-05-03T13:14:52Z"
    generation: 1
    labels:
      k8s-app: kube-proxy
    name: kube-proxy
    namespace: kube-system
    resourceVersion: "917"
    uid: e8921f46-1f52-4fc1-a44c-ff042c7d442f
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-proxy
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-proxy
      spec:
        containers:
        - command:
          - /usr/local/bin/kube-proxy
          - --config=/var/lib/kube-proxy/config.conf
          - --hostname-override=$(NODE_NAME)
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: k8s.gcr.io/kube-proxy:v1.20.4
          imagePullPolicy: IfNotPresent
          name: kube-proxy
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/kube-proxy
            name: kube-proxy
          - mountPath: /run/xtables.lock
            name: xtables-lock
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-node-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: kube-proxy
        serviceAccountName: kube-proxy
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: kube-proxy
          name: kube-proxy
        - hostPath:
            path: /run/xtables.lock
            type: FileOrCreate
          name: xtables-lock
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"DaemonSet","metadata":{"annotations":{},"labels":{"app":"metallb","component":"speaker"},"name":"speaker","namespace":"metallb-system"},"spec":{"selector":{"matchLabels":{"app":"metallb","component":"speaker"}},"template":{"metadata":{"annotations":{"prometheus.io/port":"7472","prometheus.io/scrape":"true"},"labels":{"app":"metallb","component":"speaker"}},"spec":{"containers":[{"args":["--port=7472","--config=config"],"env":[{"name":"METALLB_NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"METALLB_HOST","valueFrom":{"fieldRef":{"fieldPath":"status.hostIP"}}}],"image":"metallb/speaker:v0.8.2","imagePullPolicy":"IfNotPresent","name":"speaker","ports":[{"containerPort":7472,"name":"monitoring"}],"resources":{"limits":{"cpu":"100m","memory":"100Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"add":["NET_ADMIN","NET_RAW","SYS_ADMIN"],"drop":["ALL"]},"readOnlyRootFilesystem":true}}],"hostNetwork":true,"nodeSelector":{"beta.kubernetes.io/os":"linux"},"serviceAccountName":"speaker","terminationGracePeriodSeconds":0,"tolerations":[{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}]}}}}
    creationTimestamp: "2021-05-03T13:28:41Z"
    generation: 1
    labels:
      app: metallb
      component: speaker
    name: speaker
    namespace: metallb-system
    resourceVersion: "2418"
    uid: 5e52970f-a55d-4cea-9bcd-08a78686bccc
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: metallb
        component: speaker
    template:
      metadata:
        annotations:
          prometheus.io/port: "7472"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: metallb
          component: speaker
      spec:
        containers:
        - args:
          - --port=7472
          - --config=config
          env:
          - name: METALLB_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: METALLB_HOST
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.hostIP
          image: metallb/speaker:v0.8.2
          imagePullPolicy: IfNotPresent
          name: speaker
          ports:
          - containerPort: 7472
            hostPort: 7472
            name: monitoring
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_ADMIN
              - NET_RAW
              - SYS_ADMIN
              drop:
              - ALL
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        hostNetwork: true
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: speaker
        serviceAccountName: speaker
        terminationGracePeriodSeconds: 0
        tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 4
    desiredNumberScheduled: 4
    numberAvailable: 4
    numberMisscheduled: 0
    numberReady: 4
    observedGeneration: 1
    updatedNumberScheduled: 4
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2021-05-03T13:30:30Z"
    generation: 1
    name: csi-cephfsplugin
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 02df6b1e-2271-4cf7-be8a-89315e8c2fbb
    resourceVersion: "3160"
    uid: a8401a9a-b7cc-4bc3-b0cd-4d7b8c80ba47
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-cephfsplugin
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin
          contains: csi-cephfsplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
        containers:
        - args:
          - --v=0
          - --csi-address=/csi/csi.sock
          - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/csi.sock
          env:
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
          imagePullPolicy: IfNotPresent
          name: driver-registrar
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /registration
            name: registration-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --nodeserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --metricsport=9091
          - --forcecephkernelclient=true
          - --metricspath=/metrics
          - --enablegrpcmetrics=false
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /var/lib/kubelet/plugins
            mountPropagation: Bidirectional
            name: csi-plugins-dir
          - mountPath: /var/lib/kubelet/pods
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/mount
            name: host-run-mount
        - args:
          - --type=liveness
          - --endpoint=$(CSI_ENDPOINT)
          - --metricsport=9081
          - --metricspath=/metrics
          - --polltime=60s
          - --timeout=3s
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: liveness-prometheus
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-plugin-sa
        serviceAccountName: rook-csi-cephfs-plugin-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/rook-ceph.cephfs.csi.ceph.com/
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: Directory
          name: csi-plugins-dir
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - hostPath:
            path: /run/mount
            type: ""
          name: host-run-mount
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: DaemonSet
  metadata:
    annotations:
      deprecated.daemonset.template.generation: "1"
    creationTimestamp: "2021-05-03T13:30:29Z"
    generation: 1
    name: csi-rbdplugin
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 02df6b1e-2271-4cf7-be8a-89315e8c2fbb
    resourceVersion: "3156"
    uid: 6a65e3ea-b563-4e1d-a00c-50d91a14f5c1
  spec:
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-rbdplugin
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin
          contains: csi-rbdplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
        containers:
        - args:
          - --v=0
          - --csi-address=/csi/csi.sock
          - --kubelet-registration-path=/var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com/csi.sock
          env:
          - name: KUBE_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.0.1
          imagePullPolicy: IfNotPresent
          name: driver-registrar
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /registration
            name: registration-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --nodeserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          - --metricsport=9090
          - --metricspath=/metrics
          - --enablegrpcmetrics=false
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
          - mountPath: /var/lib/kubelet/pods
            mountPropagation: Bidirectional
            name: pods-mount-dir
          - mountPath: /var/lib/kubelet/plugins
            mountPropagation: Bidirectional
            name: plugin-mount-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
          - mountPath: /run/mount
            name: host-run-mount
        - args:
          - --type=liveness
          - --endpoint=$(CSI_ENDPOINT)
          - --metricsport=9080
          - --metricspath=/metrics
          - --polltime=60s
          - --timeout=3s
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi.sock
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: liveness-prometheus
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: plugin-dir
        dnsPolicy: ClusterFirstWithHostNet
        hostPID: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-plugin-sa
        serviceAccountName: rook-csi-rbd-plugin-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/kubelet/plugins/rook-ceph.rbd.csi.ceph.com
            type: DirectoryOrCreate
          name: plugin-dir
        - hostPath:
            path: /var/lib/kubelet/plugins
            type: Directory
          name: plugin-mount-dir
        - hostPath:
            path: /var/lib/kubelet/plugins_registry/
            type: Directory
          name: registration-dir
        - hostPath:
            path: /var/lib/kubelet/pods
            type: Directory
          name: pods-mount-dir
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
        - hostPath:
            path: /run/mount
            type: ""
          name: host-run-mount
    updateStrategy:
      rollingUpdate:
        maxUnavailable: 1
      type: RollingUpdate
  status:
    currentNumberScheduled: 3
    desiredNumberScheduled: 3
    numberAvailable: 3
    numberMisscheduled: 0
    numberReady: 3
    observedGeneration: 1
    updatedNumberScheduled: 3
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"io.cilium/app":"operator","name":"cilium-operator"},"name":"cilium-operator","namespace":"cilium"},"spec":{"replicas":2,"selector":{"matchLabels":{"io.cilium/app":"operator","name":"cilium-operator"}},"strategy":{"rollingUpdate":{"maxSurge":1,"maxUnavailable":1},"type":"RollingUpdate"},"template":{"metadata":{"annotations":null,"labels":{"io.cilium/app":"operator","name":"cilium-operator"}},"spec":{"affinity":{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"io.cilium/app","operator":"In","values":["operator"]}]},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["--config-dir=/tmp/cilium/config-map","--debug=$(CILIUM_DEBUG)"],"command":["cilium-operator-generic"],"env":[{"name":"K8S_NODE_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"CILIUM_K8S_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}},{"name":"CILIUM_DEBUG","valueFrom":{"configMapKeyRef":{"key":"debug","name":"cilium-config","optional":true}}},{"name":"AWS_ACCESS_KEY_ID","valueFrom":{"secretKeyRef":{"key":"AWS_ACCESS_KEY_ID","name":"cilium-aws","optional":true}}},{"name":"AWS_SECRET_ACCESS_KEY","valueFrom":{"secretKeyRef":{"key":"AWS_SECRET_ACCESS_KEY","name":"cilium-aws","optional":true}}},{"name":"AWS_DEFAULT_REGION","valueFrom":{"secretKeyRef":{"key":"AWS_DEFAULT_REGION","name":"cilium-aws","optional":true}}}],"image":"docker.io/cilium/operator-generic:v1.8.5","imagePullPolicy":"IfNotPresent","livenessProbe":{"httpGet":{"host":"127.0.0.1","path":"/healthz","port":9234,"scheme":"HTTP"},"initialDelaySeconds":60,"periodSeconds":10,"timeoutSeconds":3},"name":"cilium-operator","volumeMounts":[{"mountPath":"/tmp/cilium/config-map","name":"cilium-config-path","readOnly":true}]}],"hostNetwork":true,"priorityClassName":"system-cluster-critical","restartPolicy":"Always","serviceAccount":"cilium-operator","serviceAccountName":"cilium-operator","tolerations":[{"operator":"Exists"}],"volumes":[{"configMap":{"name":"cilium-config"},"name":"cilium-config-path"}]}}}}
    creationTimestamp: "2021-05-03T13:28:24Z"
    generation: 1
    labels:
      io.cilium/app: operator
      name: cilium-operator
    name: cilium-operator
    namespace: cilium
    resourceVersion: "2053"
    uid: bb3ebe48-20c9-48cd-8a2b-47022b561310
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        io.cilium/app: operator
        name: cilium-operator
    strategy:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          io.cilium/app: operator
          name: cilium-operator
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: io.cilium/app
                  operator: In
                  values:
                  - operator
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --config-dir=/tmp/cilium/config-map
          - --debug=$(CILIUM_DEBUG)
          command:
          - cilium-operator-generic
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CILIUM_DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: cilium-config
                optional: true
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: cilium-aws
                optional: true
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: cilium-aws
                optional: true
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                key: AWS_DEFAULT_REGION
                name: cilium-aws
                optional: true
          image: docker.io/cilium/operator-generic:v1.8.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: cilium-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/cilium/config-map
            name: cilium-config-path
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cilium-operator
        serviceAccountName: cilium-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: cilium-config
          name: cilium-config-path
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2021-05-03T13:28:36Z"
      lastUpdateTime: "2021-05-03T13:28:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:28:24Z"
      lastUpdateTime: "2021-05-03T13:28:36Z"
      message: ReplicaSet "cilium-operator-648569fbb8" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"k8s-app":"hubble-relay"},"name":"hubble-relay","namespace":"cilium"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"hubble-relay"}},"template":{"metadata":{"labels":{"k8s-app":"hubble-relay"}},"spec":{"affinity":{"podAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"k8s-app","operator":"In","values":["cilium"]}]},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["serve","--peer-service=unix:///var/run/cilium/hubble.sock","--listen-address=:4245"],"command":["hubble-relay"],"image":"docker.io/cilium/hubble-relay:v1.8.5","imagePullPolicy":"IfNotPresent","livenessProbe":{"tcpSocket":{"port":"grpc"}},"name":"hubble-relay","ports":[{"containerPort":4245,"name":"grpc"}],"readinessProbe":{"tcpSocket":{"port":"grpc"}},"volumeMounts":[{"mountPath":"/var/run/cilium","name":"hubble-sock-dir","readOnly":true}]}],"restartPolicy":"Always","serviceAccount":"hubble-relay","serviceAccountName":"hubble-relay","terminationGracePeriodSeconds":0,"volumes":[{"hostPath":{"path":"/var/run/cilium","type":"Directory"},"name":"hubble-sock-dir"}]}}}}
    creationTimestamp: "2021-05-03T13:28:24Z"
    generation: 1
    labels:
      k8s-app: hubble-relay
    name: hubble-relay
    namespace: cilium
    resourceVersion: "2508"
    uid: d2d34311-d00d-4dfe-ad4f-05aed653c5d9
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: hubble-relay
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: hubble-relay
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: k8s-app
                  operator: In
                  values:
                  - cilium
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - serve
          - --peer-service=unix:///var/run/cilium/hubble.sock
          - --listen-address=:4245
          command:
          - hubble-relay
          image: docker.io/cilium/hubble-relay:v1.8.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          name: hubble-relay
          ports:
          - containerPort: 4245
            name: grpc
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/run/cilium
            name: hubble-sock-dir
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hubble-relay
        serviceAccountName: hubble-relay
        terminationGracePeriodSeconds: 0
        volumes:
        - hostPath:
            path: /var/run/cilium
            type: Directory
          name: hubble-sock-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:29:27Z"
      lastUpdateTime: "2021-05-03T13:29:27Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:28:24Z"
      lastUpdateTime: "2021-05-03T13:29:27Z"
      message: ReplicaSet "hubble-relay-5f65cc9b96" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"hubble-ui","namespace":"cilium"},"spec":{"replicas":1,"selector":{"matchLabels":{"k8s-app":"hubble-ui"}},"template":{"metadata":{"labels":{"k8s-app":"hubble-ui"}},"spec":{"containers":[{"env":[{"name":"NODE_ENV","value":"production"},{"name":"LOG_LEVEL","value":"info"},{"name":"HUBBLE","value":"true"},{"name":"HUBBLE_SERVICE","value":"hubble-relay"},{"name":"HUBBLE_PORT","value":"80"}],"image":"quay.io/cilium/hubble-ui:v0.6.1","imagePullPolicy":"IfNotPresent","name":"hubble-ui","ports":[{"containerPort":12000,"name":"http"}],"resources":{}}],"securityContext":{"runAsUser":1001},"serviceAccount":"hubble-ui","serviceAccountName":"hubble-ui"}}}}
    creationTimestamp: "2021-05-03T13:28:24Z"
    generation: 1
    name: hubble-ui
    namespace: cilium
    resourceVersion: "2477"
    uid: bbfcb8ac-d339-4fad-bc16-b6a237e2e517
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: hubble-ui
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: hubble-ui
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: LOG_LEVEL
            value: info
          - name: HUBBLE
            value: "true"
          - name: HUBBLE_SERVICE
            value: hubble-relay
          - name: HUBBLE_PORT
            value: "80"
          image: quay.io/cilium/hubble-ui:v0.6.1
          imagePullPolicy: IfNotPresent
          name: hubble-ui
          ports:
          - containerPort: 12000
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 1001
        serviceAccount: hubble-ui
        serviceAccountName: hubble-ui
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:29:22Z"
      lastUpdateTime: "2021-05-03T13:29:22Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:28:24Z"
      lastUpdateTime: "2021-05-03T13:29:22Z"
      message: ReplicaSet "hubble-ui-7854cf65dc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"klustered","namespace":"default"},"spec":{"selector":{"matchLabels":{"app":"klustered"}},"template":{"metadata":{"labels":{"app":"klustered"}},"spec":{"containers":[{"image":"ghcr.io/rawkode/klustered:v1","imagePullPolicy":"Always","livenessProbe":{"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":2,"periodSeconds":1},"name":"klustered","ports":[{"containerPort":8080}],"readinessProbe":{"httpGet":{"path":"/health","port":8080},"initialDelaySeconds":2,"periodSeconds":1},"resources":{"limits":{"cpu":"500m","memory":"128Mi"}}}]}}}}
    creationTimestamp: "2021-05-03T13:28:45Z"
    generation: 1
    name: klustered
    namespace: default
    resourceVersion: "2516"
    uid: 4947aadc-59a9-450e-af0b-1e9997e172f0
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: klustered
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: klustered
      spec:
        containers:
        - image: ghcr.io/rawkode/klustered:v1
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 1
            successThreshold: 1
            timeoutSeconds: 1
          name: klustered
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 1
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:29:28Z"
      lastUpdateTime: "2021-05-03T13:29:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:28:45Z"
      lastUpdateTime: "2021-05-03T13:29:28Z"
      message: ReplicaSet "klustered-69b868776d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:14:52Z"
    generation: 1
    labels:
      k8s-app: kube-dns
    name: coredns
    namespace: kube-system
    resourceVersion: "2403"
    uid: 0941b9e5-0d85-431e-b48f-28396ef66fdc
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        k8s-app: kube-dns
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 1
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: k8s.gcr.io/coredns:1.7.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2021-05-03T13:29:06Z"
      lastUpdateTime: "2021-05-03T13:29:06Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:29:06Z"
      lastUpdateTime: "2021-05-03T13:29:07Z"
      message: ReplicaSet "coredns-74ff55c5b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"packet-cloud-controller-manager"},"name":"packet-cloud-controller-manager","namespace":"kube-system"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"packet-cloud-controller-manager"}},"template":{"metadata":{"annotations":{"scheduler.alpha.kubernetes.io/critical-pod":""},"labels":{"app":"packet-cloud-controller-manager"}},"spec":{"containers":[{"command":["./packet-cloud-controller-manager","--cloud-provider=packet","--leader-elect=false","--allow-untagged-cloud=true","--authentication-skip-lookup=true","--provider-config=/etc/cloud-sa/cloud-sa.json"],"image":"packethost/packet-ccm:v1.1.0","name":"packet-cloud-controller-manager","resources":{"requests":{"cpu":"100m","memory":"50Mi"}},"volumeMounts":[{"mountPath":"/etc/cloud-sa","name":"cloud-sa-volume","readOnly":true}]}],"dnsPolicy":"Default","hostNetwork":true,"serviceAccountName":"cloud-controller-manager","tolerations":[{"effect":"NoSchedule","key":"node.cloudprovider.kubernetes.io/uninitialized","value":"true"},{"key":"CriticalAddonsOnly","operator":"Exists"},{"effect":"NoSchedule","key":"node-role.kubernetes.io/master"}],"volumes":[{"name":"cloud-sa-volume","secret":{"secretName":"packet-cloud-config"}}]}}}}
    creationTimestamp: "2021-05-03T13:15:02Z"
    generation: 1
    labels:
      app: packet-cloud-controller-manager
    name: packet-cloud-controller-manager
    namespace: kube-system
    resourceVersion: "2220"
    uid: 3e69bac1-28a6-45e2-8b7c-fb24ad216c04
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: packet-cloud-controller-manager
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          app: packet-cloud-controller-manager
      spec:
        containers:
        - command:
          - ./packet-cloud-controller-manager
          - --cloud-provider=packet
          - --leader-elect=false
          - --allow-untagged-cloud=true
          - --authentication-skip-lookup=true
          - --provider-config=/etc/cloud-sa/cloud-sa.json
          image: packethost/packet-ccm:v1.1.0
          imagePullPolicy: IfNotPresent
          name: packet-cloud-controller-manager
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloud-sa
            name: cloud-sa-volume
            readOnly: true
        dnsPolicy: Default
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cloud-controller-manager
        serviceAccountName: cloud-controller-manager
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          value: "true"
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - name: cloud-sa-volume
          secret:
            defaultMode: 420
            secretName: packet-cloud-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:28:49Z"
      lastUpdateTime: "2021-05-03T13:28:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:28:49Z"
      lastUpdateTime: "2021-05-03T13:28:49Z"
      message: ReplicaSet "packet-cloud-controller-manager-77cd8c9c7c" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"metallb","component":"controller"},"name":"controller","namespace":"metallb-system"},"spec":{"revisionHistoryLimit":3,"selector":{"matchLabels":{"app":"metallb","component":"controller"}},"template":{"metadata":{"annotations":{"prometheus.io/port":"7472","prometheus.io/scrape":"true"},"labels":{"app":"metallb","component":"controller"}},"spec":{"containers":[{"args":["--port=7472","--config=config"],"image":"metallb/controller:v0.8.2","imagePullPolicy":"IfNotPresent","name":"controller","ports":[{"containerPort":7472,"name":"monitoring"}],"resources":{"limits":{"cpu":"100m","memory":"100Mi"}},"securityContext":{"allowPrivilegeEscalation":false,"capabilities":{"drop":["all"]},"readOnlyRootFilesystem":true}}],"nodeSelector":{"beta.kubernetes.io/os":"linux"},"securityContext":{"runAsNonRoot":true,"runAsUser":65534},"serviceAccountName":"controller","terminationGracePeriodSeconds":0}}}}
    creationTimestamp: "2021-05-03T13:28:42Z"
    generation: 1
    labels:
      app: metallb
      component: controller
    name: controller
    namespace: metallb-system
    resourceVersion: "2537"
    uid: e88f421f-4dd3-4c39-b94f-1de7441ddf7f
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 3
    selector:
      matchLabels:
        app: metallb
        component: controller
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        annotations:
          prometheus.io/port: "7472"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: metallb
          component: controller
      spec:
        containers:
        - args:
          - --port=7472
          - --config=config
          image: metallb/controller:v0.8.2
          imagePullPolicy: IfNotPresent
          name: controller
          ports:
          - containerPort: 7472
            name: monitoring
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: controller
        serviceAccountName: controller
        terminationGracePeriodSeconds: 0
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:29:32Z"
      lastUpdateTime: "2021-05-03T13:29:32Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:28:42Z"
      lastUpdateTime: "2021-05-03T13:29:32Z"
      message: ReplicaSet "controller-675d6c9976" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"apiVersion":"apps/v1","kind":"Deployment","metadata":{"name":"csi-cephfsplugin-provisioner","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"apps/v1","controller":true,"kind":"Deployment","name":"rook-ceph-operator","uid":"02df6b1e-2271-4cf7-be8a-89315e8c2fbb"}]},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"csi-cephfsplugin-provisioner"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"csi-cephfsplugin-provisioner","contains":"csi-cephfsplugin-metrics"}},"spec":{"affinity":{"nodeAffinity":{},"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["csi-cephfsplugin-provisioner"]}]},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["--v=0","--csi-address=$(ADDRESS)","--leader-election=true","--timeout=150s","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"/csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-attacher:v3.0.2","imagePullPolicy":"IfNotPresent","name":"csi-attacher","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--leader-election=true","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2","imagePullPolicy":"IfNotPresent","name":"csi-snapshotter","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--leader-election","--leader-election-namespace=rook-ceph","--handle-volume-inuse-error=false"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-resizer:v1.0.1","imagePullPolicy":"IfNotPresent","name":"csi-resizer","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--retry-interval-start=500ms","--leader-election=true","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4","imagePullPolicy":"IfNotPresent","name":"csi-provisioner","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--nodeid=$(NODE_ID)","--type=cephfs","--endpoint=$(CSI_ENDPOINT)","--v=0","--controllerserver=true","--drivername=rook-ceph.cephfs.csi.ceph.com","--pidlimit=-1","--metricsport=9091","--forcecephkernelclient=true","--metricspath=/metrics","--enablegrpcmetrics=false"],"env":[{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"NODE_ID","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"CSI_ENDPOINT","value":"unix:///csi/csi-provisioner.sock"}],"image":"quay.io/cephcsi/cephcsi:v3.3.1","imagePullPolicy":"IfNotPresent","name":"csi-cephfsplugin","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"},{"mountPath":"/sys","name":"host-sys"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/dev","name":"host-dev"},{"mountPath":"/etc/ceph-csi-config/","name":"ceph-csi-config"},{"mountPath":"/tmp/csi/keys","name":"keys-tmp-dir"}]},{"args":["--type=liveness","--endpoint=$(CSI_ENDPOINT)","--metricsport=9081","--metricspath=/metrics","--polltime=60s","--timeout=3s"],"env":[{"name":"CSI_ENDPOINT","value":"unix:///csi/csi-provisioner.sock"},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"quay.io/cephcsi/cephcsi:v3.3.1","imagePullPolicy":"IfNotPresent","name":"liveness-prometheus","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]}],"serviceAccount":"rook-csi-cephfs-provisioner-sa","volumes":[{"emptyDir":{"medium":"Memory"},"name":"socket-dir"},{"hostPath":{"path":"/sys"},"name":"host-sys"},{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/dev"},"name":"host-dev"},{"configMap":{"items":[{"key":"csi-cluster-config-json","path":"config.json"}],"name":"rook-ceph-csi-config"},"name":"ceph-csi-config"},{"emptyDir":{"medium":"Memory"},"name":"keys-tmp-dir"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:30:30Z"
    generation: 1
    name: csi-cephfsplugin-provisioner
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 02df6b1e-2271-4cf7-be8a-89315e8c2fbb
    resourceVersion: "3195"
    uid: a286dac5-f283-49f8-8a4c-40c1f731dde7
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-cephfsplugin-provisioner
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin-provisioner
          contains: csi-cephfsplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-cephfsplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --v=0
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --timeout=150s
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --leader-election
          - --leader-election-namespace=rook-ceph
          - --handle-volume-inuse-error=false
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --controllerserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --metricsport=9091
          - --forcecephkernelclient=true
          - --metricspath=/metrics
          - --enablegrpcmetrics=false
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        - args:
          - --type=liveness
          - --endpoint=$(CSI_ENDPOINT)
          - --metricsport=9081
          - --metricspath=/metrics
          - --polltime=60s
          - --timeout=3s
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: liveness-prometheus
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-provisioner-sa
        serviceAccountName: rook-csi-cephfs-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: socket-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2021-05-03T13:31:00Z"
      lastUpdateTime: "2021-05-03T13:31:00Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:30:30Z"
      lastUpdateTime: "2021-05-03T13:31:00Z"
      message: ReplicaSet "csi-cephfsplugin-provisioner-5b989b9977" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"apiVersion":"apps/v1","kind":"Deployment","metadata":{"name":"csi-rbdplugin-provisioner","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"apps/v1","controller":true,"kind":"Deployment","name":"rook-ceph-operator","uid":"02df6b1e-2271-4cf7-be8a-89315e8c2fbb"}]},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"csi-rbdplugin-provisioner"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"csi-rbdplugin-provisioner","contains":"csi-rbdplugin-metrics"}},"spec":{"affinity":{"nodeAffinity":{},"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["csi-rbdplugin-provisioner"]}]},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--retry-interval-start=500ms","--leader-election=true","--leader-election-namespace=rook-ceph","--default-fstype=ext4","--extra-create-metadata=true"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4","imagePullPolicy":"IfNotPresent","name":"csi-provisioner","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--leader-election","--leader-election-namespace=rook-ceph","--handle-volume-inuse-error=false"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-resizer:v1.0.1","imagePullPolicy":"IfNotPresent","name":"csi-resizer","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--v=0","--timeout=150s","--csi-address=$(ADDRESS)","--leader-election=true","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"/csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-attacher:v3.0.2","imagePullPolicy":"IfNotPresent","name":"csi-attacher","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--leader-election=true","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2","imagePullPolicy":"IfNotPresent","name":"csi-snapshotter","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--nodeid=$(NODE_ID)","--endpoint=$(CSI_ENDPOINT)","--v=0","--type=rbd","--controllerserver=true","--drivername=rook-ceph.rbd.csi.ceph.com","--pidlimit=-1","--metricsport=9090","--metricspath=/metrics","--enablegrpcmetrics=false"],"env":[{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"NODE_ID","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"CSI_ENDPOINT","value":"unix:///csi/csi-provisioner.sock"}],"image":"quay.io/cephcsi/cephcsi:v3.3.1","imagePullPolicy":"IfNotPresent","name":"csi-rbdplugin","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"},{"mountPath":"/dev","name":"host-dev"},{"mountPath":"/sys","name":"host-sys"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/etc/ceph-csi-config/","name":"ceph-csi-config"},{"mountPath":"/tmp/csi/keys","name":"keys-tmp-dir"}]},{"args":["--type=liveness","--endpoint=$(CSI_ENDPOINT)","--metricsport=9080","--metricspath=/metrics","--polltime=60s","--timeout=3s"],"env":[{"name":"CSI_ENDPOINT","value":"unix:///csi/csi-provisioner.sock"},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"quay.io/cephcsi/cephcsi:v3.3.1","imagePullPolicy":"IfNotPresent","name":"liveness-prometheus","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]}],"serviceAccount":"rook-csi-rbd-provisioner-sa","volumes":[{"hostPath":{"path":"/dev"},"name":"host-dev"},{"hostPath":{"path":"/sys"},"name":"host-sys"},{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"emptyDir":{"medium":"Memory"},"name":"socket-dir"},{"configMap":{"items":[{"key":"csi-cluster-config-json","path":"config.json"}],"name":"rook-ceph-csi-config"},"name":"ceph-csi-config"},{"emptyDir":{"medium":"Memory"},"name":"keys-tmp-dir"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:30:29Z"
    generation: 1
    name: csi-rbdplugin-provisioner
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: false
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 02df6b1e-2271-4cf7-be8a-89315e8c2fbb
    resourceVersion: "3306"
    uid: 8ef373d1-db7c-4126-b62d-6bd0bef80af7
  spec:
    progressDeadlineSeconds: 600
    replicas: 2
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: csi-rbdplugin-provisioner
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin-provisioner
          contains: csi-rbdplugin-metrics
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-rbdplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --default-fstype=ext4
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --leader-election
          - --leader-election-namespace=rook-ceph
          - --handle-volume-inuse-error=false
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --v=0
          - --timeout=150s
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --controllerserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          - --metricsport=9090
          - --metricspath=/metrics
          - --enablegrpcmetrics=false
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        - args:
          - --type=liveness
          - --endpoint=$(CSI_ENDPOINT)
          - --metricsport=9080
          - --metricspath=/metrics
          - --polltime=60s
          - --timeout=3s
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: liveness-prometheus
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-provisioner-sa
        serviceAccountName: rook-csi-rbd-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - emptyDir:
            medium: Memory
          name: socket-dir
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    availableReplicas: 2
    conditions:
    - lastTransitionTime: "2021-05-03T13:31:21Z"
      lastUpdateTime: "2021-05-03T13:31:21Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:30:29Z"
      lastUpdateTime: "2021-05-03T13:31:21Z"
      message: ReplicaSet "csi-rbdplugin-provisioner-55f998c984" has successfully
        progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
    updatedReplicas: 2
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "3"
    creationTimestamp: "2021-05-03T13:31:32Z"
    generation: 3
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4421"
    uid: fd4f168f-30f6-4287-8463-6bea3f3850cc
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 15.2.11-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:11Z"
      lastUpdateTime: "2021-05-03T13:32:11Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:31:32Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: ReplicaSet "rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0-59fb6b7f87"
        has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:57Z"
    generation: 1
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-ceeab05259c8d89b0904c022bc5f8be0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4006"
    uid: a8aa0dfb-1d88-4370-869e-1b5f0d2c2c2a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 15.2.11-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:16Z"
      lastUpdateTime: "2021-05-03T13:32:16Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:31:57Z"
      lastUpdateTime: "2021-05-03T13:32:16Z"
      message: ReplicaSet "rook-ceph-crashcollector-ceeab05259c8d89b0904c022bc5f8be0-7975c649"
        has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:43Z"
    generation: 1
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-fb5b167868730b399f51c41bfe43cdc0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4027"
    uid: a18ffeb3-6d8d-4d9d-9009-b15d548fdfd8
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 15.2.11-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:18Z"
      lastUpdateTime: "2021-05-03T13:32:18Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:31:43Z"
      lastUpdateTime: "2021-05-03T13:32:18Z"
      message: ReplicaSet "rook-ceph-crashcollector-fb5b167868730b399f51c41bfe43cdc0-66c6d67c89"
        has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-mgr","ceph-version":"15.2.11-0","ceph_daemon_id":"a","ceph_daemon_type":"mgr","instance":"a","mgr":"a","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph"},"name":"rook-ceph-mgr-a","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-mgr","ceph_daemon_id":"a","instance":"a","mgr":"a","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"annotations":{"prometheus.io/port":"9283","prometheus.io/scrape":"true"},"labels":{"app":"rook-ceph-mgr","ceph_daemon_id":"a","ceph_daemon_type":"mgr","instance":"a","mgr":"a","rook_cluster":"rook-ceph"},"name":"rook-ceph-mgr-a"},"spec":{"affinity":{},"containers":[{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=a","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--client-mount-uid=0","--client-mount-gid=0","--foreground","--public-addr=$(ROOK_POD_IP)"],"command":["ceph-mgr"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}},{"name":"ROOK_OPERATOR_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_CEPH_CLUSTER_CRD_VERSION","value":"v1"},{"name":"ROOK_CEPH_CLUSTER_CRD_NAME","value":"rook-ceph"},{"name":"ROOK_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"httpGet":{"path":"/","port":9283},"initialDelaySeconds":60},"name":"mgr","ports":[{"containerPort":6800,"name":"mgr","protocol":"TCP"},{"containerPort":9283,"name":"http-metrics","protocol":"TCP"},{"containerPort":8443,"name":"dashboard","protocol":"TCP"}],"resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mgr-a-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mgr/ceph-a","name":"ceph-daemon-data"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash","/var/lib/ceph/mgr/ceph-a"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mgr-a-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mgr/ceph-a","name":"ceph-daemon-data"}]}],"restartPolicy":"Always","serviceAccountName":"rook-ceph-mgr","tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":5}],"volumes":[{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"name":"rook-ceph-mgr-a-keyring","secret":{"secretName":"rook-ceph-mgr-a-keyring"}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"emptyDir":{},"name":"ceph-daemon-data"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:08Z"
    generation: 1
    labels:
      app: rook-ceph-mgr
      ceph-version: 15.2.11-0
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3878"
    uid: 8887887b-b04f-42b0-b14b-2f83fad5ecb0
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9283
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:10Z"
      lastUpdateTime: "2021-05-03T13:32:10Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:08Z"
      lastUpdateTime: "2021-05-03T13:32:10Z"
      message: ReplicaSet "rook-ceph-mgr-a-65f9b6b9d6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-mon","ceph-version":"15.2.11-0","ceph_daemon_id":"a","ceph_daemon_type":"mon","mon":"a","mon_cluster":"rook-ceph","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-a","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-mon","ceph_daemon_id":"a","mon":"a","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-mon","ceph_daemon_id":"a","ceph_daemon_type":"mon","mon":"a","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-a","namespace":"rook-ceph"},"spec":{"containers":[{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=a","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--foreground","--public-addr=172.26.200.254","--setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db","--public-bind-addr=$(ROOK_POD_IP)"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}},{"name":"ROOK_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-mon.a.asok mon_status"]},"initialDelaySeconds":10},"name":"mon","ports":[{"containerPort":6789,"name":"tcp-msgr1","protocol":"TCP"}],"resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-a","name":"ceph-daemon-data"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash","/var/lib/ceph/mon/ceph-a"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-a","name":"ceph-daemon-data"}]},{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=a","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--public-addr=172.26.200.254","--mkfs"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}}],"image":"ceph/ceph:v15.2.11","name":"init-mon-fs","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-a","name":"ceph-daemon-data"}]}],"restartPolicy":"Always","volumes":[{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"name":"rook-ceph-mons-keyring","secret":{"secretName":"rook-ceph-mons-keyring"}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:28Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      ceph-version: 15.2.11-0
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3541"
    uid: 4119c5a6-7312-4852-ae2d-1c7530dd656a
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --foreground
          - --public-addr=172.26.200.254
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mon
          ports:
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --public-addr=172.26.200.254
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:31:36Z"
      lastUpdateTime: "2021-05-03T13:31:36Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:31:28Z"
      lastUpdateTime: "2021-05-03T13:31:36Z"
      message: ReplicaSet "rook-ceph-mon-a-9db7d95f6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-mon","ceph-version":"15.2.11-0","ceph_daemon_id":"b","ceph_daemon_type":"mon","mon":"b","mon_cluster":"rook-ceph","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-b","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-mon","ceph_daemon_id":"b","mon":"b","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-mon","ceph_daemon_id":"b","ceph_daemon_type":"mon","mon":"b","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-b","namespace":"rook-ceph"},"spec":{"containers":[{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=b","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--foreground","--public-addr=172.26.167.208","--setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db","--public-bind-addr=$(ROOK_POD_IP)"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}},{"name":"ROOK_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-mon.b.asok mon_status"]},"initialDelaySeconds":10},"name":"mon","ports":[{"containerPort":6789,"name":"tcp-msgr1","protocol":"TCP"}],"resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-b","name":"ceph-daemon-data"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash","/var/lib/ceph/mon/ceph-b"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-b","name":"ceph-daemon-data"}]},{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=b","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--public-addr=172.26.167.208","--mkfs"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}}],"image":"ceph/ceph:v15.2.11","name":"init-mon-fs","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-b","name":"ceph-daemon-data"}]}],"restartPolicy":"Always","volumes":[{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"name":"rook-ceph-mons-keyring","secret":{"secretName":"rook-ceph-mons-keyring"}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:40Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      ceph-version: 15.2.11-0
      ceph_daemon_id: b
      ceph_daemon_type: mon
      mon: b
      mon_cluster: rook-ceph
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-mon-b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3673"
    uid: 1da32b15-dd43-453b-8589-60ed98934e0c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: b
        mon: b
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          ceph_daemon_id: b
          ceph_daemon_type: mon
          mon: b
          mon_cluster: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-b
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=b
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --foreground
          - --public-addr=172.26.167.208
          - --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-mon.b.asok mon_status
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mon
          ports:
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-b
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /var/lib/ceph/mon/ceph-b
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-b
            name: ceph-daemon-data
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=b
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --public-addr=172.26.167.208
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-b
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-b/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:31:49Z"
      lastUpdateTime: "2021-05-03T13:31:49Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:31:40Z"
      lastUpdateTime: "2021-05-03T13:31:49Z"
      message: ReplicaSet "rook-ceph-mon-b-6778b88456" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-mon","ceph-version":"15.2.11-0","ceph_daemon_id":"c","ceph_daemon_type":"mon","mon":"c","mon_cluster":"rook-ceph","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-c","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-mon","ceph_daemon_id":"c","mon":"c","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-mon","ceph_daemon_id":"c","ceph_daemon_type":"mon","mon":"c","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-c","namespace":"rook-ceph"},"spec":{"containers":[{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=c","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--foreground","--public-addr=172.26.84.185","--setuser-match-path=/var/lib/ceph/mon/ceph-c/store.db","--public-bind-addr=$(ROOK_POD_IP)"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}},{"name":"ROOK_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-mon.c.asok mon_status"]},"initialDelaySeconds":10},"name":"mon","ports":[{"containerPort":6789,"name":"tcp-msgr1","protocol":"TCP"}],"resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-c","name":"ceph-daemon-data"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash","/var/lib/ceph/mon/ceph-c"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-c","name":"ceph-daemon-data"}]},{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=c","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--public-addr=172.26.84.185","--mkfs"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}}],"image":"ceph/ceph:v15.2.11","name":"init-mon-fs","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-c","name":"ceph-daemon-data"}]}],"restartPolicy":"Always","volumes":[{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"name":"rook-ceph-mons-keyring","secret":{"secretName":"rook-ceph-mons-keyring"}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:57Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      ceph-version: 15.2.11-0
      ceph_daemon_id: c
      ceph_daemon_type: mon
      mon: c
      mon_cluster: rook-ceph
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-mon-c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "3784"
    uid: 1bccac83-2b73-41dd-9aa5-172fee0eaad1
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: c
        mon: c
        mon_cluster: rook-ceph
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          ceph_daemon_id: c
          ceph_daemon_type: mon
          mon: c
          mon_cluster: rook-ceph
          rook_cluster: rook-ceph
        name: rook-ceph-mon-c
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=c
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --foreground
          - --public-addr=172.26.84.185
          - --setuser-match-path=/var/lib/ceph/mon/ceph-c/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mon
          ports:
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /var/lib/ceph/mon/ceph-c
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=c
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --public-addr=172.26.84.185
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-c/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:01Z"
      lastUpdateTime: "2021-05-03T13:32:01Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:31:57Z"
      lastUpdateTime: "2021-05-03T13:32:01Z"
      message: ReplicaSet "rook-ceph-mon-c-55ccc87ccb" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"operator":"rook","storage-backend":"ceph"},"name":"rook-ceph-operator","namespace":"rook-ceph"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-operator"}},"template":{"metadata":{"labels":{"app":"rook-ceph-operator"}},"spec":{"containers":[{"args":["ceph","operator"],"env":[{"name":"ROOK_CURRENT_NAMESPACE_ONLY","value":"false"},{"name":"ROOK_LOG_LEVEL","value":"INFO"},{"name":"ROOK_DISCOVER_DEVICES_INTERVAL","value":"60m"},{"name":"ROOK_HOSTPATH_REQUIRES_PRIVILEGED","value":"false"},{"name":"ROOK_ENABLE_SELINUX_RELABELING","value":"true"},{"name":"ROOK_ENABLE_FSGROUP","value":"true"},{"name":"ROOK_DISABLE_DEVICE_HOTPLUG","value":"false"},{"name":"DISCOVER_DAEMON_UDEV_BLACKLIST","value":"(?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+"},{"name":"ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS","value":"5"},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}}],"image":"rook/ceph:master","name":"rook-ceph-operator","volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-config"},{"mountPath":"/etc/ceph","name":"default-config-dir"}]}],"serviceAccountName":"rook-ceph-system","volumes":[{"emptyDir":{},"name":"rook-config"},{"emptyDir":{},"name":"default-config-dir"}]}}}}
    creationTimestamp: "2021-05-03T13:28:35Z"
    generation: 1
    labels:
      operator: rook
      storage-backend: ceph
    name: rook-ceph-operator
    namespace: rook-ceph
    resourceVersion: "2536"
    uid: 02df6b1e-2271-4cf7-be8a-89315e8c2fbb
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-operator
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-operator
      spec:
        containers:
        - args:
          - ceph
          - operator
          env:
          - name: ROOK_CURRENT_NAMESPACE_ONLY
            value: "false"
          - name: ROOK_LOG_LEVEL
            value: INFO
          - name: ROOK_DISCOVER_DEVICES_INTERVAL
            value: 60m
          - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
            value: "false"
          - name: ROOK_ENABLE_SELINUX_RELABELING
            value: "true"
          - name: ROOK_ENABLE_FSGROUP
            value: "true"
          - name: ROOK_DISABLE_DEVICE_HOTPLUG
            value: "false"
          - name: DISCOVER_DAEMON_UDEV_BLACKLIST
            value: (?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+
          - name: ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS
            value: "5"
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rook/ceph:master
          imagePullPolicy: IfNotPresent
          name: rook-ceph-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-config
          - mountPath: /etc/ceph
            name: default-config-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-system
        serviceAccountName: rook-ceph-system
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: rook-config
        - emptyDir: {}
          name: default-config-dir
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:29:32Z"
      lastUpdateTime: "2021-05-03T13:29:32Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:28:35Z"
      lastUpdateTime: "2021-05-03T13:29:32Z"
      message: ReplicaSet "rook-ceph-operator-84c85574d9" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"0","ceph-version":"15.2.11-0","ceph_daemon_id":"0","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"0","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-0","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"0","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"0","ceph-version":"15.2.11-0","ceph_daemon_id":"0","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"0","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","0","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"f8e31292-e88c-48ef-9806-f45a7280254f"},{"name":"ROOK_OSD_ID","value":"0"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.0.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-0","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=0\nOSD_UUID=f8e31292-e88c-48ef-9806-f45a7280254f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-0","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-0","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sda","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "0"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "0"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4378"
    uid: bc19eb13-184d-489f-84df-dbef72814dca
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "0"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          osd: "0"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: f8e31292-e88c-48ef-9806-f45a7280254f
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.0.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=0\nOSD_UUID=f8e31292-e88c-48ef-9806-f45a7280254f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sda
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:28Z"
      lastUpdateTime: "2021-05-03T13:32:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:24Z"
      lastUpdateTime: "2021-05-03T13:32:28Z"
      message: ReplicaSet "rook-ceph-osd-0-56c4fc758d" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"1","ceph-version":"15.2.11-0","ceph_daemon_id":"1","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"1","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-1","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"1","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"1","ceph-version":"15.2.11-0","ceph_daemon_id":"1","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"1","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","1","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"cbf296c6-1064-405f-b8a2-fd4ede68c69f"},{"name":"ROOK_OSD_ID","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.1.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-1","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=1\nOSD_UUID=cbf296c6-1064-405f-b8a2-fd4ede68c69f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-1","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-1","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sda","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "1"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "1"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-1
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4429"
    uid: df854435-76b7-4e2f-877f-3b496c80a12c
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "1"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          osd: "1"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: cbf296c6-1064-405f-b8a2-fd4ede68c69f
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.1.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=1\nOSD_UUID=cbf296c6-1064-405f-b8a2-fd4ede68c69f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sda
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:29Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:24Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: ReplicaSet "rook-ceph-osd-1-6c7bbc6996" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"2","ceph-version":"15.2.11-0","ceph_daemon_id":"2","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"2","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-2","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"2","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"2","ceph-version":"15.2.11-0","ceph_daemon_id":"2","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"2","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","2","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"0794c043-8b9a-4600-9e56-e27358665ea5"},{"name":"ROOK_OSD_ID","value":"2"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.2.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-2","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=2\nOSD_UUID=0794c043-8b9a-4600-9e56-e27358665ea5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-2","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-2","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sda","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:25Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "2"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "2"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-2
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4408"
    uid: d07ee528-35c2-4c72-a258-ac2da6cb70ff
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "2"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          osd: "2"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: 0794c043-8b9a-4600-9e56-e27358665ea5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.2.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=2\nOSD_UUID=0794c043-8b9a-4600-9e56-e27358665ea5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sda
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:29Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:25Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: ReplicaSet "rook-ceph-osd-2-64c7556b68" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"3","ceph-version":"15.2.11-0","ceph_daemon_id":"3","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"3","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-3","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"3","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"3","ceph-version":"15.2.11-0","ceph_daemon_id":"3","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"3","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","3","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"0439f447-d0de-4e7d-8b78-8e112a4181ce"},{"name":"ROOK_OSD_ID","value":"3"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdb"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.3.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-3","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=3\nOSD_UUID=0439f447-d0de-4e7d-8b78-8e112a4181ce\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdb"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-3","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-3","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdb","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "3"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "3"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-3
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4373"
    uid: eae35e70-bb08-4d55-b654-f02f2fdf93a4
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "3"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "3"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "3"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          osd: "3"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "3"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: 0439f447-d0de-4e7d-8b78-8e112a4181ce
          - name: ROOK_OSD_ID
            value: "3"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.3.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=3\nOSD_UUID=0439f447-d0de-4e7d-8b78-8e112a4181ce\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdb
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:28Z"
      lastUpdateTime: "2021-05-03T13:32:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:24Z"
      lastUpdateTime: "2021-05-03T13:32:28Z"
      message: ReplicaSet "rook-ceph-osd-3-6c56cdc4cc" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"4","ceph-version":"15.2.11-0","ceph_daemon_id":"4","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"4","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-4","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"4","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"4","ceph-version":"15.2.11-0","ceph_daemon_id":"4","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"4","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","4","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"a82f8fcd-16ea-47aa-9cf1-634534f103cc"},{"name":"ROOK_OSD_ID","value":"4"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdb"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.4.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-4","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=4\nOSD_UUID=a82f8fcd-16ea-47aa-9cf1-634534f103cc\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdb"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-4","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-4","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdb","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "4"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "4"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "4"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-4
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4454"
    uid: 5da42462-c070-4a38-a58d-6a37c80964db
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "4"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "4"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "4"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          osd: "4"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "4"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: a82f8fcd-16ea-47aa-9cf1-634534f103cc
          - name: ROOK_OSD_ID
            value: "4"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.4.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-4
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=4\nOSD_UUID=a82f8fcd-16ea-47aa-9cf1-634534f103cc\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-4
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-4
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdb
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:31Z"
      lastUpdateTime: "2021-05-03T13:32:31Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:24Z"
      lastUpdateTime: "2021-05-03T13:32:31Z"
      message: ReplicaSet "rook-ceph-osd-4-8fb498649" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"5","ceph-version":"15.2.11-0","ceph_daemon_id":"5","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"5","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-5","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"5","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"5","ceph-version":"15.2.11-0","ceph_daemon_id":"5","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"5","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","5","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"ba3f17ca-a4d6-4a31-adbb-68c70a0a6196"},{"name":"ROOK_OSD_ID","value":"5"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.5.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-5","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=5\nOSD_UUID=ba3f17ca-a4d6-4a31-adbb-68c70a0a6196\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-5","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-5","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdc","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:25Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "5"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "5"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "5"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-5
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4403"
    uid: 17f19c59-7048-4926-9450-6921c06fb06e
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "5"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "5"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "5"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          osd: "5"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "5"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: ba3f17ca-a4d6-4a31-adbb-68c70a0a6196
          - name: ROOK_OSD_ID
            value: "5"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.5.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-5
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=5\nOSD_UUID=ba3f17ca-a4d6-4a31-adbb-68c70a0a6196\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-5
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-5
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdc
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:29Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:25Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: ReplicaSet "rook-ceph-osd-5-6df5f5cc94" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"6","ceph-version":"15.2.11-0","ceph_daemon_id":"6","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"6","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-6","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"6","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"6","ceph-version":"15.2.11-0","ceph_daemon_id":"6","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"6","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","6","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"1e2abf51-b10e-4d8b-a409-989eed9ff755"},{"name":"ROOK_OSD_ID","value":"6"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.6.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-6","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=6\nOSD_UUID=1e2abf51-b10e-4d8b-a409-989eed9ff755\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-6","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-6","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdc","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "6"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "6"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "6"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4376"
    uid: 65b23d19-3484-4eb5-af61-140981a03694
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "6"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "6"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "6"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          osd: "6"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "6"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: 1e2abf51-b10e-4d8b-a409-989eed9ff755
          - name: ROOK_OSD_ID
            value: "6"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.6.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-6
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=6\nOSD_UUID=1e2abf51-b10e-4d8b-a409-989eed9ff755\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-6
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-6
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdc
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:28Z"
      lastUpdateTime: "2021-05-03T13:32:28Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:24Z"
      lastUpdateTime: "2021-05-03T13:32:28Z"
      message: ReplicaSet "rook-ceph-osd-6-c97896bb6" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"7","ceph-version":"15.2.11-0","ceph_daemon_id":"7","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"7","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-7","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"7","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"7","ceph-version":"15.2.11-0","ceph_daemon_id":"7","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"7","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","7","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"a71d8128-94c0-494b-9e3a-e47b2341d1d7"},{"name":"ROOK_OSD_ID","value":"7"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.7.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-7","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=7\nOSD_UUID=a71d8128-94c0-494b-9e3a-e47b2341d1d7\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-7","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-7","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdc","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "7"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "7"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "7"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-7
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4434"
    uid: 859bf6a0-6b19-4e05-91d5-574a92a090e3
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "7"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "7"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "7"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          osd: "7"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "7"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: a71d8128-94c0-494b-9e3a-e47b2341d1d7
          - name: ROOK_OSD_ID
            value: "7"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.7.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-7
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=7\nOSD_UUID=a71d8128-94c0-494b-9e3a-e47b2341d1d7\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-7
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-7
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdc
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:30Z"
      lastUpdateTime: "2021-05-03T13:32:30Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:24Z"
      lastUpdateTime: "2021-05-03T13:32:30Z"
      message: ReplicaSet "rook-ceph-osd-7-75b5fcc6b" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"8","ceph-version":"15.2.11-0","ceph_daemon_id":"8","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"8","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-8","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"8","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"8","ceph-version":"15.2.11-0","ceph_daemon_id":"8","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"8","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","8","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"af0ac0df-b3d4-425b-ac37-b28f7e40a590"},{"name":"ROOK_OSD_ID","value":"8"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdd"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.8.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-8","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=8\nOSD_UUID=af0ac0df-b3d4-425b-ac37-b28f7e40a590\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdd"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-8","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-8","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdd","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:25Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "8"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "8"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "8"
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-8
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4406"
    uid: 43daa347-b4da-4501-9239-17a2ce38c1b6
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "8"
        rook_cluster: rook-ceph
    strategy:
      type: Recreate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "8"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "8"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          osd: "8"
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "8"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: af0ac0df-b3d4-425b-ac37-b28f7e40a590
          - name: ROOK_OSD_ID
            value: "8"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdd
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.8.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-8
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=8\nOSD_UUID=af0ac0df-b3d4-425b-ac37-b28f7e40a590\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdd
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-8
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-8
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdd
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:32:29Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:32:25Z"
      lastUpdateTime: "2021-05-03T13:32:29Z"
      message: ReplicaSet "rook-ceph-osd-8-69865cc7ff" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    annotations:
      deployment.kubernetes.io/revision: "1"
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"labels":{"app":"rook-ceph-tools"},"name":"rook-ceph-tools","namespace":"rook-ceph"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-tools"}},"template":{"metadata":{"labels":{"app":"rook-ceph-tools"}},"spec":{"containers":[{"args":["-g","--","/usr/local/bin/toolbox.sh"],"command":["/tini"],"env":[{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}}],"image":"rook/ceph:master","imagePullPolicy":"IfNotPresent","name":"rook-ceph-tools","volumeMounts":[{"mountPath":"/etc/ceph","name":"ceph-config"},{"mountPath":"/etc/rook","name":"mon-endpoint-volume"}]}],"dnsPolicy":"ClusterFirstWithHostNet","tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":5}],"volumes":[{"configMap":{"items":[{"key":"data","path":"mon-endpoints"}],"name":"rook-ceph-mon-endpoints"},"name":"mon-endpoint-volume"},{"emptyDir":{},"name":"ceph-config"}]}}}}
    creationTimestamp: "2021-05-03T13:28:39Z"
    generation: 1
    labels:
      app: rook-ceph-tools
    name: rook-ceph-tools
    namespace: rook-ceph
    resourceVersion: "3310"
    uid: 7fa66cc3-7a79-41cf-831a-ad2c52a5c827
  spec:
    progressDeadlineSeconds: 600
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: rook-ceph-tools
    strategy:
      rollingUpdate:
        maxSurge: 25%
        maxUnavailable: 25%
      type: RollingUpdate
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
      spec:
        containers:
        - args:
          - -g
          - --
          - /usr/local/bin/toolbox.sh
          command:
          - /tini
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          image: rook/ceph:master
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
  status:
    availableReplicas: 1
    conditions:
    - lastTransitionTime: "2021-05-03T13:31:21Z"
      lastUpdateTime: "2021-05-03T13:31:21Z"
      message: Deployment has minimum availability.
      reason: MinimumReplicasAvailable
      status: "True"
      type: Available
    - lastTransitionTime: "2021-05-03T13:28:39Z"
      lastUpdateTime: "2021-05-03T13:31:21Z"
      message: ReplicaSet "rook-ceph-tools-78cdfd976c" has successfully progressed.
      reason: NewReplicaSetAvailable
      status: "True"
      type: Progressing
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updatedReplicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:28:24Z"
    generation: 1
    labels:
      io.cilium/app: operator
      name: cilium-operator
      pod-template-hash: 648569fbb8
    name: cilium-operator-648569fbb8
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: cilium-operator
      uid: bb3ebe48-20c9-48cd-8a2b-47022b561310
    resourceVersion: "2052"
    uid: bfd77510-3944-4f42-a384-238db1943b68
  spec:
    replicas: 2
    selector:
      matchLabels:
        io.cilium/app: operator
        name: cilium-operator
        pod-template-hash: 648569fbb8
    template:
      metadata:
        creationTimestamp: null
        labels:
          io.cilium/app: operator
          name: cilium-operator
          pod-template-hash: 648569fbb8
      spec:
        affinity:
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: io.cilium/app
                  operator: In
                  values:
                  - operator
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --config-dir=/tmp/cilium/config-map
          - --debug=$(CILIUM_DEBUG)
          command:
          - cilium-operator-generic
          env:
          - name: K8S_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: CILIUM_K8S_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CILIUM_DEBUG
            valueFrom:
              configMapKeyRef:
                key: debug
                name: cilium-config
                optional: true
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                key: AWS_ACCESS_KEY_ID
                name: cilium-aws
                optional: true
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                key: AWS_SECRET_ACCESS_KEY
                name: cilium-aws
                optional: true
          - name: AWS_DEFAULT_REGION
            valueFrom:
              secretKeyRef:
                key: AWS_DEFAULT_REGION
                name: cilium-aws
                optional: true
          image: docker.io/cilium/operator-generic:v1.8.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 9234
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 3
          name: cilium-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /tmp/cilium/config-map
            name: cilium-config-path
            readOnly: true
        dnsPolicy: ClusterFirst
        hostNetwork: true
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cilium-operator
        serviceAccountName: cilium-operator
        terminationGracePeriodSeconds: 30
        tolerations:
        - operator: Exists
        volumes:
        - configMap:
            defaultMode: 420
            name: cilium-config
          name: cilium-config-path
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:28:24Z"
    generation: 1
    labels:
      k8s-app: hubble-relay
      pod-template-hash: 5f65cc9b96
    name: hubble-relay-5f65cc9b96
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hubble-relay
      uid: d2d34311-d00d-4dfe-ad4f-05aed653c5d9
    resourceVersion: "2507"
    uid: 7dfb07cd-a1a0-4ecb-9fec-703bcbd14ae8
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: hubble-relay
        pod-template-hash: 5f65cc9b96
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: hubble-relay
          pod-template-hash: 5f65cc9b96
      spec:
        affinity:
          podAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: k8s-app
                  operator: In
                  values:
                  - cilium
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - serve
          - --peer-service=unix:///var/run/cilium/hubble.sock
          - --listen-address=:4245
          command:
          - hubble-relay
          image: docker.io/cilium/hubble-relay:v1.8.5
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          name: hubble-relay
          ports:
          - containerPort: 4245
            name: grpc
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: grpc
            timeoutSeconds: 1
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/run/cilium
            name: hubble-sock-dir
            readOnly: true
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: hubble-relay
        serviceAccountName: hubble-relay
        terminationGracePeriodSeconds: 0
        volumes:
        - hostPath:
            path: /var/run/cilium
            type: Directory
          name: hubble-sock-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:28:24Z"
    generation: 1
    labels:
      k8s-app: hubble-ui
      pod-template-hash: 7854cf65dc
    name: hubble-ui-7854cf65dc
    namespace: cilium
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: hubble-ui
      uid: bbfcb8ac-d339-4fad-bc16-b6a237e2e517
    resourceVersion: "2476"
    uid: aa40d7e3-aeca-465d-98cd-e1fd04a181da
  spec:
    replicas: 1
    selector:
      matchLabels:
        k8s-app: hubble-ui
        pod-template-hash: 7854cf65dc
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: hubble-ui
          pod-template-hash: 7854cf65dc
      spec:
        containers:
        - env:
          - name: NODE_ENV
            value: production
          - name: LOG_LEVEL
            value: info
          - name: HUBBLE
            value: "true"
          - name: HUBBLE_SERVICE
            value: hubble-relay
          - name: HUBBLE_PORT
            value: "80"
          image: quay.io/cilium/hubble-ui:v0.6.1
          imagePullPolicy: IfNotPresent
          name: hubble-ui
          ports:
          - containerPort: 12000
            name: http
            protocol: TCP
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsUser: 1001
        serviceAccount: hubble-ui
        serviceAccountName: hubble-ui
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:28:45Z"
    generation: 1
    labels:
      app: klustered
      pod-template-hash: 69b868776d
    name: klustered-69b868776d
    namespace: default
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: klustered
      uid: 4947aadc-59a9-450e-af0b-1e9997e172f0
    resourceVersion: "2515"
    uid: 44c5a3e5-c83e-47f0-8da8-75f4811b983b
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: klustered
        pod-template-hash: 69b868776d
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: klustered
          pod-template-hash: 69b868776d
      spec:
        containers:
        - image: ghcr.io/rawkode/klustered:v1
          imagePullPolicy: Always
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 1
            successThreshold: 1
            timeoutSeconds: 1
          name: klustered
          ports:
          - containerPort: 8080
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 2
            periodSeconds: 1
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              cpu: 500m
              memory: 128Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "3"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:15:06Z"
    generation: 1
    labels:
      k8s-app: kube-dns
      pod-template-hash: 74ff55c5b
    name: coredns-74ff55c5b
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: coredns
      uid: 0941b9e5-0d85-431e-b48f-28396ef66fdc
    resourceVersion: "2402"
    uid: 8b04fffe-675d-4f90-8a47-289ed5e44bec
  spec:
    replicas: 2
    selector:
      matchLabels:
        k8s-app: kube-dns
        pod-template-hash: 74ff55c5b
    template:
      metadata:
        creationTimestamp: null
        labels:
          k8s-app: kube-dns
          pod-template-hash: 74ff55c5b
      spec:
        containers:
        - args:
          - -conf
          - /etc/coredns/Corefile
          image: k8s.gcr.io/coredns:1.7.0
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
          name: coredns
          ports:
          - containerPort: 53
            name: dns
            protocol: UDP
          - containerPort: 53
            name: dns-tcp
            protocol: TCP
          - containerPort: 9153
            name: metrics
            protocol: TCP
          readinessProbe:
            failureThreshold: 3
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
              - NET_BIND_SERVICE
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/coredns
            name: config-volume
            readOnly: true
        dnsPolicy: Default
        nodeSelector:
          kubernetes.io/os: linux
        priorityClassName: system-cluster-critical
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: coredns
        serviceAccountName: coredns
        terminationGracePeriodSeconds: 30
        tolerations:
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
            name: coredns
          name: config-volume
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:15:06Z"
    generation: 1
    labels:
      app: packet-cloud-controller-manager
      pod-template-hash: 77cd8c9c7c
    name: packet-cloud-controller-manager-77cd8c9c7c
    namespace: kube-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: packet-cloud-controller-manager
      uid: 3e69bac1-28a6-45e2-8b7c-fb24ad216c04
    resourceVersion: "2219"
    uid: 2ab805a5-9a76-40ed-a772-358754280c5a
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: packet-cloud-controller-manager
        pod-template-hash: 77cd8c9c7c
    template:
      metadata:
        annotations:
          scheduler.alpha.kubernetes.io/critical-pod: ""
        creationTimestamp: null
        labels:
          app: packet-cloud-controller-manager
          pod-template-hash: 77cd8c9c7c
      spec:
        containers:
        - command:
          - ./packet-cloud-controller-manager
          - --cloud-provider=packet
          - --leader-elect=false
          - --allow-untagged-cloud=true
          - --authentication-skip-lookup=true
          - --provider-config=/etc/cloud-sa/cloud-sa.json
          image: packethost/packet-ccm:v1.1.0
          imagePullPolicy: IfNotPresent
          name: packet-cloud-controller-manager
          resources:
            requests:
              cpu: 100m
              memory: 50Mi
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/cloud-sa
            name: cloud-sa-volume
            readOnly: true
        dnsPolicy: Default
        hostNetwork: true
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: cloud-controller-manager
        serviceAccountName: cloud-controller-manager
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoSchedule
          key: node.cloudprovider.kubernetes.io/uninitialized
          value: "true"
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          key: node-role.kubernetes.io/master
        volumes:
        - name: cloud-sa-volume
          secret:
            defaultMode: 420
            secretName: packet-cloud-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:28:42Z"
    generation: 1
    labels:
      app: metallb
      component: controller
      pod-template-hash: 675d6c9976
    name: controller-675d6c9976
    namespace: metallb-system
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: controller
      uid: e88f421f-4dd3-4c39-b94f-1de7441ddf7f
    resourceVersion: "2535"
    uid: 37608d6c-ad0a-49b1-8e1b-dd0da3e787f1
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: metallb
        component: controller
        pod-template-hash: 675d6c9976
    template:
      metadata:
        annotations:
          prometheus.io/port: "7472"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: metallb
          component: controller
          pod-template-hash: 675d6c9976
      spec:
        containers:
        - args:
          - --port=7472
          - --config=config
          image: metallb/controller:v0.8.2
          imagePullPolicy: IfNotPresent
          name: controller
          ports:
          - containerPort: 7472
            name: monitoring
            protocol: TCP
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - all
            readOnlyRootFilesystem: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
        dnsPolicy: ClusterFirst
        nodeSelector:
          beta.kubernetes.io/os: linux
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
        serviceAccount: controller
        serviceAccountName: controller
        terminationGracePeriodSeconds: 0
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"apiVersion":"apps/v1","kind":"Deployment","metadata":{"name":"csi-cephfsplugin-provisioner","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"apps/v1","controller":true,"kind":"Deployment","name":"rook-ceph-operator","uid":"02df6b1e-2271-4cf7-be8a-89315e8c2fbb"}]},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"csi-cephfsplugin-provisioner"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"csi-cephfsplugin-provisioner","contains":"csi-cephfsplugin-metrics"}},"spec":{"affinity":{"nodeAffinity":{},"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["csi-cephfsplugin-provisioner"]}]},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["--v=0","--csi-address=$(ADDRESS)","--leader-election=true","--timeout=150s","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"/csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-attacher:v3.0.2","imagePullPolicy":"IfNotPresent","name":"csi-attacher","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--leader-election=true","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2","imagePullPolicy":"IfNotPresent","name":"csi-snapshotter","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--leader-election","--leader-election-namespace=rook-ceph","--handle-volume-inuse-error=false"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-resizer:v1.0.1","imagePullPolicy":"IfNotPresent","name":"csi-resizer","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--retry-interval-start=500ms","--leader-election=true","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4","imagePullPolicy":"IfNotPresent","name":"csi-provisioner","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--nodeid=$(NODE_ID)","--type=cephfs","--endpoint=$(CSI_ENDPOINT)","--v=0","--controllerserver=true","--drivername=rook-ceph.cephfs.csi.ceph.com","--pidlimit=-1","--metricsport=9091","--forcecephkernelclient=true","--metricspath=/metrics","--enablegrpcmetrics=false"],"env":[{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"NODE_ID","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"CSI_ENDPOINT","value":"unix:///csi/csi-provisioner.sock"}],"image":"quay.io/cephcsi/cephcsi:v3.3.1","imagePullPolicy":"IfNotPresent","name":"csi-cephfsplugin","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"},{"mountPath":"/sys","name":"host-sys"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/dev","name":"host-dev"},{"mountPath":"/etc/ceph-csi-config/","name":"ceph-csi-config"},{"mountPath":"/tmp/csi/keys","name":"keys-tmp-dir"}]},{"args":["--type=liveness","--endpoint=$(CSI_ENDPOINT)","--metricsport=9081","--metricspath=/metrics","--polltime=60s","--timeout=3s"],"env":[{"name":"CSI_ENDPOINT","value":"unix:///csi/csi-provisioner.sock"},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"quay.io/cephcsi/cephcsi:v3.3.1","imagePullPolicy":"IfNotPresent","name":"liveness-prometheus","resources":{},"securityContext":{"allowPrivilegeEscalation":true,"capabilities":{"add":["SYS_ADMIN"]},"privileged":true},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]}],"serviceAccount":"rook-csi-cephfs-provisioner-sa","volumes":[{"emptyDir":{"medium":"Memory"},"name":"socket-dir"},{"hostPath":{"path":"/sys"},"name":"host-sys"},{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"hostPath":{"path":"/dev"},"name":"host-dev"},{"configMap":{"items":[{"key":"csi-cluster-config-json","path":"config.json"}],"name":"rook-ceph-csi-config"},"name":"ceph-csi-config"},{"emptyDir":{"medium":"Memory"},"name":"keys-tmp-dir"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:30:30Z"
    generation: 1
    labels:
      app: csi-cephfsplugin-provisioner
      contains: csi-cephfsplugin-metrics
      pod-template-hash: 5b989b9977
    name: csi-cephfsplugin-provisioner-5b989b9977
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-cephfsplugin-provisioner
      uid: a286dac5-f283-49f8-8a4c-40c1f731dde7
    resourceVersion: "3194"
    uid: f8990e57-3edc-41b0-9410-87282d729c59
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: csi-cephfsplugin-provisioner
        pod-template-hash: 5b989b9977
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-cephfsplugin-provisioner
          contains: csi-cephfsplugin-metrics
          pod-template-hash: 5b989b9977
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-cephfsplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --v=0
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --timeout=150s
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --leader-election
          - --leader-election-namespace=rook-ceph
          - --handle-volume-inuse-error=false
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --type=cephfs
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --controllerserver=true
          - --drivername=rook-ceph.cephfs.csi.ceph.com
          - --pidlimit=-1
          - --metricsport=9091
          - --forcecephkernelclient=true
          - --metricspath=/metrics
          - --enablegrpcmetrics=false
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: csi-cephfsplugin
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /dev
            name: host-dev
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        - args:
          - --type=liveness
          - --endpoint=$(CSI_ENDPOINT)
          - --metricsport=9081
          - --metricspath=/metrics
          - --polltime=60s
          - --timeout=3s
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: liveness-prometheus
          resources: {}
          securityContext:
            allowPrivilegeEscalation: true
            capabilities:
              add:
              - SYS_ADMIN
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-cephfs-provisioner-sa
        serviceAccountName: rook-csi-cephfs-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir:
            medium: Memory
          name: socket-dir
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"apiVersion":"apps/v1","kind":"Deployment","metadata":{"name":"csi-rbdplugin-provisioner","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"apps/v1","controller":true,"kind":"Deployment","name":"rook-ceph-operator","uid":"02df6b1e-2271-4cf7-be8a-89315e8c2fbb"}]},"spec":{"replicas":2,"selector":{"matchLabels":{"app":"csi-rbdplugin-provisioner"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"csi-rbdplugin-provisioner","contains":"csi-rbdplugin-metrics"}},"spec":{"affinity":{"nodeAffinity":{},"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["csi-rbdplugin-provisioner"]}]},"topologyKey":"kubernetes.io/hostname"}]}},"containers":[{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--retry-interval-start=500ms","--leader-election=true","--leader-election-namespace=rook-ceph","--default-fstype=ext4","--extra-create-metadata=true"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4","imagePullPolicy":"IfNotPresent","name":"csi-provisioner","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--leader-election","--leader-election-namespace=rook-ceph","--handle-volume-inuse-error=false"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-resizer:v1.0.1","imagePullPolicy":"IfNotPresent","name":"csi-resizer","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--v=0","--timeout=150s","--csi-address=$(ADDRESS)","--leader-election=true","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"/csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-attacher:v3.0.2","imagePullPolicy":"IfNotPresent","name":"csi-attacher","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--csi-address=$(ADDRESS)","--v=0","--timeout=150s","--leader-election=true","--leader-election-namespace=rook-ceph"],"env":[{"name":"ADDRESS","value":"unix:///csi/csi-provisioner.sock"}],"image":"k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2","imagePullPolicy":"IfNotPresent","name":"csi-snapshotter","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]},{"args":["--nodeid=$(NODE_ID)","--endpoint=$(CSI_ENDPOINT)","--v=0","--type=rbd","--controllerserver=true","--drivername=rook-ceph.rbd.csi.ceph.com","--pidlimit=-1","--metricsport=9090","--metricspath=/metrics","--enablegrpcmetrics=false"],"env":[{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"NODE_ID","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"CSI_ENDPOINT","value":"unix:///csi/csi-provisioner.sock"}],"image":"quay.io/cephcsi/cephcsi:v3.3.1","imagePullPolicy":"IfNotPresent","name":"csi-rbdplugin","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"},{"mountPath":"/dev","name":"host-dev"},{"mountPath":"/sys","name":"host-sys"},{"mountPath":"/lib/modules","name":"lib-modules","readOnly":true},{"mountPath":"/etc/ceph-csi-config/","name":"ceph-csi-config"},{"mountPath":"/tmp/csi/keys","name":"keys-tmp-dir"}]},{"args":["--type=liveness","--endpoint=$(CSI_ENDPOINT)","--metricsport=9080","--metricspath=/metrics","--polltime=60s","--timeout=3s"],"env":[{"name":"CSI_ENDPOINT","value":"unix:///csi/csi-provisioner.sock"},{"name":"POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"quay.io/cephcsi/cephcsi:v3.3.1","imagePullPolicy":"IfNotPresent","name":"liveness-prometheus","resources":{},"volumeMounts":[{"mountPath":"/csi","name":"socket-dir"}]}],"serviceAccount":"rook-csi-rbd-provisioner-sa","volumes":[{"hostPath":{"path":"/dev"},"name":"host-dev"},{"hostPath":{"path":"/sys"},"name":"host-sys"},{"hostPath":{"path":"/lib/modules"},"name":"lib-modules"},{"emptyDir":{"medium":"Memory"},"name":"socket-dir"},{"configMap":{"items":[{"key":"csi-cluster-config-json","path":"config.json"}],"name":"rook-ceph-csi-config"},"name":"ceph-csi-config"},{"emptyDir":{"medium":"Memory"},"name":"keys-tmp-dir"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "2"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:30:29Z"
    generation: 1
    labels:
      app: csi-rbdplugin-provisioner
      contains: csi-rbdplugin-metrics
      pod-template-hash: 55f998c984
    name: csi-rbdplugin-provisioner-55f998c984
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: csi-rbdplugin-provisioner
      uid: 8ef373d1-db7c-4126-b62d-6bd0bef80af7
    resourceVersion: "3305"
    uid: 4133ce73-37d7-49ec-a4a5-b64723cb5593
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: csi-rbdplugin-provisioner
        pod-template-hash: 55f998c984
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: csi-rbdplugin-provisioner
          contains: csi-rbdplugin-metrics
          pod-template-hash: 55f998c984
      spec:
        affinity:
          nodeAffinity: {}
          podAntiAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - csi-rbdplugin-provisioner
              topologyKey: kubernetes.io/hostname
        containers:
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --retry-interval-start=500ms
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          - --default-fstype=ext4
          - --extra-create-metadata=true
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-provisioner:v2.0.4
          imagePullPolicy: IfNotPresent
          name: csi-provisioner
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --leader-election
          - --leader-election-namespace=rook-ceph
          - --handle-volume-inuse-error=false
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-resizer:v1.0.1
          imagePullPolicy: IfNotPresent
          name: csi-resizer
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --v=0
          - --timeout=150s
          - --csi-address=$(ADDRESS)
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: /csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-attacher:v3.0.2
          imagePullPolicy: IfNotPresent
          name: csi-attacher
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --csi-address=$(ADDRESS)
          - --v=0
          - --timeout=150s
          - --leader-election=true
          - --leader-election-namespace=rook-ceph
          env:
          - name: ADDRESS
            value: unix:///csi/csi-provisioner.sock
          image: k8s.gcr.io/sig-storage/csi-snapshotter:v3.0.2
          imagePullPolicy: IfNotPresent
          name: csi-snapshotter
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        - args:
          - --nodeid=$(NODE_ID)
          - --endpoint=$(CSI_ENDPOINT)
          - --v=0
          - --type=rbd
          - --controllerserver=true
          - --drivername=rook-ceph.rbd.csi.ceph.com
          - --pidlimit=-1
          - --metricsport=9090
          - --metricspath=/metrics
          - --enablegrpcmetrics=false
          env:
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: csi-rbdplugin
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
          - mountPath: /dev
            name: host-dev
          - mountPath: /sys
            name: host-sys
          - mountPath: /lib/modules
            name: lib-modules
            readOnly: true
          - mountPath: /etc/ceph-csi-config/
            name: ceph-csi-config
          - mountPath: /tmp/csi/keys
            name: keys-tmp-dir
        - args:
          - --type=liveness
          - --endpoint=$(CSI_ENDPOINT)
          - --metricsport=9080
          - --metricspath=/metrics
          - --polltime=60s
          - --timeout=3s
          env:
          - name: CSI_ENDPOINT
            value: unix:///csi/csi-provisioner.sock
          - name: POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: quay.io/cephcsi/cephcsi:v3.3.1
          imagePullPolicy: IfNotPresent
          name: liveness-prometheus
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /csi
            name: socket-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-csi-rbd-provisioner-sa
        serviceAccountName: rook-csi-rbd-provisioner-sa
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /dev
            type: ""
          name: host-dev
        - hostPath:
            path: /sys
            type: ""
          name: host-sys
        - hostPath:
            path: /lib/modules
            type: ""
          name: lib-modules
        - emptyDir:
            medium: Memory
          name: socket-dir
        - configMap:
            defaultMode: 420
            items:
            - key: csi-cluster-config-json
              path: config.json
            name: rook-ceph-csi-config
          name: ceph-csi-config
        - emptyDir:
            medium: Memory
          name: keys-tmp-dir
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 1
    readyReplicas: 2
    replicas: 2
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "3"
      deployment.kubernetes.io/revision-history: "1"
    creationTimestamp: "2021-05-03T13:31:32Z"
    generation: 3
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      pod-template-hash: 59fb6b7f87
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0-59fb6b7f87
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0
      uid: fd4f168f-30f6-4287-8463-6bea3f3850cc
    resourceVersion: "4413"
    uid: 6d5d8c91-8d0f-4edc-aa48-bd49b6ad1002
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        pod-template-hash: 59fb6b7f87
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 15.2.11-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          pod-template-hash: 59fb6b7f87
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 3
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "2"
    creationTimestamp: "2021-05-03T13:32:08Z"
    generation: 2
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      pod-template-hash: 659c85fc85
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0-659c85fc85
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-6f643bc7106c83d1bb0a9274a2947bc0
      uid: fd4f168f-30f6-4287-8463-6bea3f3850cc
    resourceVersion: "4420"
    uid: d8bac366-6b00-4a62-82b3-dc324a562c54
  spec:
    replicas: 0
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        pod-template-hash: 659c85fc85
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 15.2.11-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          pod-template-hash: 659c85fc85
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    observedGeneration: 2
    replicas: 0
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:57Z"
    generation: 1
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      pod-template-hash: 7975c649
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-ceeab05259c8d89b0904c022bc5f8be0-7975c649
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-ceeab05259c8d89b0904c022bc5f8be0
      uid: a8aa0dfb-1d88-4370-869e-1b5f0d2c2c2a
    resourceVersion: "4005"
    uid: 7354f394-7e11-4895-a93b-2692599d1fba
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        pod-template-hash: 7975c649
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 15.2.11-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          pod-template-hash: 7975c649
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:43Z"
    generation: 1
    labels:
      app: rook-ceph-crashcollector
      ceph-version: 15.2.11-0
      ceph_daemon_id: crash
      crashcollector: crash
      kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      pod-template-hash: 66c6d67c89
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-crashcollector-fb5b167868730b399f51c41bfe43cdc0-66c6d67c89
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-crashcollector-fb5b167868730b399f51c41bfe43cdc0
      uid: a18ffeb3-6d8d-4d9d-9009-b15d548fdfd8
    resourceVersion: "4026"
    uid: c8e559b7-45dd-4dc5-8e02-a84ce271e682
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-crashcollector
        kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        pod-template-hash: 66c6d67c89
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-crashcollector
          ceph-version: 15.2.11-0
          ceph_daemon_id: crash
          crashcollector: crash
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          node_name: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          pod-template-hash: 66c6d67c89
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
      spec:
        containers:
        - command:
          - ceph-crash
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST) -k /etc/ceph/crash-collector-keyring-store/keyring
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: ceph-crash
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /etc/ceph/crash-collector-keyring-store/
            name: rook-ceph-crash-collector-keyring
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - /var/lib/ceph/crash/posted
          command:
          - mkdir
          - -p
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: make-container-crash-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/not-ready
          operator: Exists
          tolerationSeconds: 300
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 300
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - name: rook-ceph-crash-collector-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-crash-collector-keyring
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-mgr","ceph-version":"15.2.11-0","ceph_daemon_id":"a","ceph_daemon_type":"mgr","instance":"a","mgr":"a","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph"},"name":"rook-ceph-mgr-a","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-mgr","ceph_daemon_id":"a","instance":"a","mgr":"a","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"annotations":{"prometheus.io/port":"9283","prometheus.io/scrape":"true"},"labels":{"app":"rook-ceph-mgr","ceph_daemon_id":"a","ceph_daemon_type":"mgr","instance":"a","mgr":"a","rook_cluster":"rook-ceph"},"name":"rook-ceph-mgr-a"},"spec":{"affinity":{},"containers":[{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=a","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--client-mount-uid=0","--client-mount-gid=0","--foreground","--public-addr=$(ROOK_POD_IP)"],"command":["ceph-mgr"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}},{"name":"ROOK_OPERATOR_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_CEPH_CLUSTER_CRD_VERSION","value":"v1"},{"name":"ROOK_CEPH_CLUSTER_CRD_NAME","value":"rook-ceph"},{"name":"ROOK_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"httpGet":{"path":"/","port":9283},"initialDelaySeconds":60},"name":"mgr","ports":[{"containerPort":6800,"name":"mgr","protocol":"TCP"},{"containerPort":9283,"name":"http-metrics","protocol":"TCP"},{"containerPort":8443,"name":"dashboard","protocol":"TCP"}],"resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mgr-a-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mgr/ceph-a","name":"ceph-daemon-data"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash","/var/lib/ceph/mgr/ceph-a"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mgr-a-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mgr/ceph-a","name":"ceph-daemon-data"}]}],"restartPolicy":"Always","serviceAccountName":"rook-ceph-mgr","tolerations":[{"effect":"NoExecute","key":"node.kubernetes.io/unreachable","operator":"Exists","tolerationSeconds":5}],"volumes":[{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"name":"rook-ceph-mgr-a-keyring","secret":{"secretName":"rook-ceph-mgr-a-keyring"}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"emptyDir":{},"name":"ceph-daemon-data"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:08Z"
    generation: 1
    labels:
      app: rook-ceph-mgr
      ceph_daemon_id: a
      ceph_daemon_type: mgr
      instance: a
      mgr: a
      pod-template-hash: 65f9b6b9d6
      rook_cluster: rook-ceph
    name: rook-ceph-mgr-a-65f9b6b9d6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mgr-a
      uid: 8887887b-b04f-42b0-b14b-2f83fad5ecb0
    resourceVersion: "3876"
    uid: a49e5e8a-e39e-42ab-ae04-cc51ea5f909f
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mgr
        ceph_daemon_id: a
        instance: a
        mgr: a
        pod-template-hash: 65f9b6b9d6
        rook_cluster: rook-ceph
    template:
      metadata:
        annotations:
          prometheus.io/port: "9283"
          prometheus.io/scrape: "true"
        creationTimestamp: null
        labels:
          app: rook-ceph-mgr
          ceph_daemon_id: a
          ceph_daemon_type: mgr
          instance: a
          mgr: a
          pod-template-hash: 65f9b6b9d6
          rook_cluster: rook-ceph
        name: rook-ceph-mgr-a
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --client-mount-uid=0
          - --client-mount-gid=0
          - --foreground
          - --public-addr=$(ROOK_POD_IP)
          command:
          - ceph-mgr
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_OPERATOR_NAMESPACE
            value: rook-ceph
          - name: ROOK_CEPH_CLUSTER_CRD_VERSION
            value: v1
          - name: ROOK_CEPH_CLUSTER_CRD_NAME
            value: rook-ceph
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /
              port: 9283
              scheme: HTTP
            initialDelaySeconds: 60
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mgr
          ports:
          - containerPort: 6800
            name: mgr
            protocol: TCP
          - containerPort: 9283
            name: http-metrics
            protocol: TCP
          - containerPort: 8443
            name: dashboard
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /var/lib/ceph/mgr/ceph-a
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mgr-a-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mgr/ceph-a
            name: ceph-daemon-data
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-mgr
        serviceAccountName: rook-ceph-mgr
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mgr-a-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mgr-a-keyring
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-mon","ceph-version":"15.2.11-0","ceph_daemon_id":"a","ceph_daemon_type":"mon","mon":"a","mon_cluster":"rook-ceph","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-a","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-mon","ceph_daemon_id":"a","mon":"a","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-mon","ceph_daemon_id":"a","ceph_daemon_type":"mon","mon":"a","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-a","namespace":"rook-ceph"},"spec":{"containers":[{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=a","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--foreground","--public-addr=172.26.200.254","--setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db","--public-bind-addr=$(ROOK_POD_IP)"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}},{"name":"ROOK_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-mon.a.asok mon_status"]},"initialDelaySeconds":10},"name":"mon","ports":[{"containerPort":6789,"name":"tcp-msgr1","protocol":"TCP"}],"resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-a","name":"ceph-daemon-data"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash","/var/lib/ceph/mon/ceph-a"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-a","name":"ceph-daemon-data"}]},{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=a","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--public-addr=172.26.200.254","--mkfs"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}}],"image":"ceph/ceph:v15.2.11","name":"init-mon-fs","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-a","name":"ceph-daemon-data"}]}],"restartPolicy":"Always","volumes":[{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"name":"rook-ceph-mons-keyring","secret":{"secretName":"rook-ceph-mons-keyring"}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:28Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: a
      ceph_daemon_type: mon
      mon: a
      mon_cluster: rook-ceph
      pod-template-hash: 9db7d95f6
      rook_cluster: rook-ceph
    name: rook-ceph-mon-a-9db7d95f6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-a
      uid: 4119c5a6-7312-4852-ae2d-1c7530dd656a
    resourceVersion: "3540"
    uid: fe2d878b-4127-491f-ac23-4296c735d23e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: a
        mon: a
        mon_cluster: rook-ceph
        pod-template-hash: 9db7d95f6
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          ceph_daemon_id: a
          ceph_daemon_type: mon
          mon: a
          mon_cluster: rook-ceph
          pod-template-hash: 9db7d95f6
          rook_cluster: rook-ceph
        name: rook-ceph-mon-a
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --foreground
          - --public-addr=172.26.200.254
          - --setuser-match-path=/var/lib/ceph/mon/ceph-a/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-mon.a.asok mon_status
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mon
          ports:
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /var/lib/ceph/mon/ceph-a
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=a
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --public-addr=172.26.200.254
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-a
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-a/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-mon","ceph-version":"15.2.11-0","ceph_daemon_id":"b","ceph_daemon_type":"mon","mon":"b","mon_cluster":"rook-ceph","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-b","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-mon","ceph_daemon_id":"b","mon":"b","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-mon","ceph_daemon_id":"b","ceph_daemon_type":"mon","mon":"b","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-b","namespace":"rook-ceph"},"spec":{"containers":[{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=b","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--foreground","--public-addr=172.26.167.208","--setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db","--public-bind-addr=$(ROOK_POD_IP)"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}},{"name":"ROOK_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-mon.b.asok mon_status"]},"initialDelaySeconds":10},"name":"mon","ports":[{"containerPort":6789,"name":"tcp-msgr1","protocol":"TCP"}],"resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-b","name":"ceph-daemon-data"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash","/var/lib/ceph/mon/ceph-b"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-b","name":"ceph-daemon-data"}]},{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=b","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--public-addr=172.26.167.208","--mkfs"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}}],"image":"ceph/ceph:v15.2.11","name":"init-mon-fs","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-b","name":"ceph-daemon-data"}]}],"restartPolicy":"Always","volumes":[{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"name":"rook-ceph-mons-keyring","secret":{"secretName":"rook-ceph-mons-keyring"}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:40Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: b
      ceph_daemon_type: mon
      mon: b
      mon_cluster: rook-ceph
      pod-template-hash: 6778b88456
      rook_cluster: rook-ceph
    name: rook-ceph-mon-b-6778b88456
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-b
      uid: 1da32b15-dd43-453b-8589-60ed98934e0c
    resourceVersion: "3672"
    uid: 83203b8b-9ae8-4847-a0a4-77ae33974d27
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: b
        mon: b
        mon_cluster: rook-ceph
        pod-template-hash: 6778b88456
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          ceph_daemon_id: b
          ceph_daemon_type: mon
          mon: b
          mon_cluster: rook-ceph
          pod-template-hash: 6778b88456
          rook_cluster: rook-ceph
        name: rook-ceph-mon-b
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=b
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --foreground
          - --public-addr=172.26.167.208
          - --setuser-match-path=/var/lib/ceph/mon/ceph-b/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-mon.b.asok mon_status
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mon
          ports:
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-b
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /var/lib/ceph/mon/ceph-b
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-b
            name: ceph-daemon-data
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=b
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --public-addr=172.26.167.208
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-b
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-b/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-mon","ceph-version":"15.2.11-0","ceph_daemon_id":"c","ceph_daemon_type":"mon","mon":"c","mon_cluster":"rook-ceph","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-c","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-mon","ceph_daemon_id":"c","mon":"c","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-mon","ceph_daemon_id":"c","ceph_daemon_type":"mon","mon":"c","mon_cluster":"rook-ceph","rook_cluster":"rook-ceph"},"name":"rook-ceph-mon-c","namespace":"rook-ceph"},"spec":{"containers":[{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=c","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--foreground","--public-addr=172.26.84.185","--setuser-match-path=/var/lib/ceph/mon/ceph-c/store.db","--public-bind-addr=$(ROOK_POD_IP)"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}},{"name":"ROOK_POD_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-mon.c.asok mon_status"]},"initialDelaySeconds":10},"name":"mon","ports":[{"containerPort":6789,"name":"tcp-msgr1","protocol":"TCP"}],"resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-c","name":"ceph-daemon-data"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash","/var/lib/ceph/mon/ceph-c"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-c","name":"ceph-daemon-data"}]},{"args":["--fsid=742018bb-f74c-4681-bfea-4ab962753821","--keyring=/etc/ceph/keyring-store/keyring","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--mon-host=$(ROOK_CEPH_MON_HOST)","--mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)","--id=c","--setuser=ceph","--setgroup=ceph","--ms-bind-ipv4=true","--ms-bind-ipv6=false","--public-addr=172.26.84.185","--mkfs"],"command":["ceph-mon"],"env":[{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_NAMESPACE","valueFrom":{"fieldRef":{"fieldPath":"metadata.namespace"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"ROOK_CEPH_MON_INITIAL_MEMBERS","valueFrom":{"secretKeyRef":{"key":"mon_initial_members","name":"rook-ceph-config"}}}],"image":"ceph/ceph:v15.2.11","name":"init-mon-fs","resources":{},"volumeMounts":[{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/etc/ceph/keyring-store/","name":"rook-ceph-mons-keyring","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/var/lib/ceph/mon/ceph-c","name":"ceph-daemon-data"}]}],"restartPolicy":"Always","volumes":[{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"name":"rook-ceph-mons-keyring","secret":{"secretName":"rook-ceph-mons-keyring"}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:31:57Z"
    generation: 1
    labels:
      app: rook-ceph-mon
      ceph_daemon_id: c
      ceph_daemon_type: mon
      mon: c
      mon_cluster: rook-ceph
      pod-template-hash: 55ccc87ccb
      rook_cluster: rook-ceph
    name: rook-ceph-mon-c-55ccc87ccb
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-mon-c
      uid: 1bccac83-2b73-41dd-9aa5-172fee0eaad1
    resourceVersion: "3783"
    uid: fde81815-6cbc-48c0-bf0b-604790ecd453
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-mon
        ceph_daemon_id: c
        mon: c
        mon_cluster: rook-ceph
        pod-template-hash: 55ccc87ccb
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-mon
          ceph_daemon_id: c
          ceph_daemon_type: mon
          mon: c
          mon_cluster: rook-ceph
          pod-template-hash: 55ccc87ccb
          rook_cluster: rook-ceph
        name: rook-ceph-mon-c
        namespace: rook-ceph
      spec:
        affinity: {}
        containers:
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=c
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --foreground
          - --public-addr=172.26.84.185
          - --setuser-match-path=/var/lib/ceph/mon/ceph-c/store.db
          - --public-bind-addr=$(ROOK_POD_IP)
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          - name: ROOK_POD_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-mon.c.asok mon_status
            failureThreshold: 3
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: mon
          ports:
          - containerPort: 6789
            name: tcp-msgr1
            protocol: TCP
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          - /var/lib/ceph/mon/ceph-c
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
        - args:
          - --fsid=742018bb-f74c-4681-bfea-4ab962753821
          - --keyring=/etc/ceph/keyring-store/keyring
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --mon-host=$(ROOK_CEPH_MON_HOST)
          - --mon-initial-members=$(ROOK_CEPH_MON_INITIAL_MEMBERS)
          - --id=c
          - --setuser=ceph
          - --setgroup=ceph
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          - --public-addr=172.26.84.185
          - --mkfs
          command:
          - ceph-mon
          env:
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: ROOK_CEPH_MON_INITIAL_MEMBERS
            valueFrom:
              secretKeyRef:
                key: mon_initial_members
                name: rook-ceph-config
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: init-mon-fs
          resources: {}
          securityContext:
            privileged: false
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /etc/ceph/keyring-store/
            name: rook-ceph-mons-keyring
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /var/lib/ceph/mon/ceph-c
            name: ceph-daemon-data
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - name: rook-ceph-mons-keyring
          secret:
            defaultMode: 420
            secretName: rook-ceph-mons-keyring
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /var/lib/rook/mon-c/data
            type: ""
          name: ceph-daemon-data
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:28:35Z"
    generation: 1
    labels:
      app: rook-ceph-operator
      pod-template-hash: 84c85574d9
    name: rook-ceph-operator-84c85574d9
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-operator
      uid: 02df6b1e-2271-4cf7-be8a-89315e8c2fbb
    resourceVersion: "2533"
    uid: 68870733-2418-477e-b59b-06f5d3bae8b5
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-operator
        pod-template-hash: 84c85574d9
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-operator
          pod-template-hash: 84c85574d9
      spec:
        containers:
        - args:
          - ceph
          - operator
          env:
          - name: ROOK_CURRENT_NAMESPACE_ONLY
            value: "false"
          - name: ROOK_LOG_LEVEL
            value: INFO
          - name: ROOK_DISCOVER_DEVICES_INTERVAL
            value: 60m
          - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
            value: "false"
          - name: ROOK_ENABLE_SELINUX_RELABELING
            value: "true"
          - name: ROOK_ENABLE_FSGROUP
            value: "true"
          - name: ROOK_DISABLE_DEVICE_HOTPLUG
            value: "false"
          - name: DISCOVER_DAEMON_UDEV_BLACKLIST
            value: (?i)dm-[0-9]+,(?i)rbd[0-9]+,(?i)nbd[0-9]+
          - name: ROOK_UNREACHABLE_NODE_TOLERATION_SECONDS
            value: "5"
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          image: rook/ceph:master
          imagePullPolicy: IfNotPresent
          name: rook-ceph-operator
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-config
          - mountPath: /etc/ceph
            name: default-config-dir
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-system
        serviceAccountName: rook-ceph-system
        terminationGracePeriodSeconds: 30
        volumes:
        - emptyDir: {}
          name: rook-config
        - emptyDir: {}
          name: default-config-dir
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"0","ceph-version":"15.2.11-0","ceph_daemon_id":"0","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"0","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-0","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"0","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"0","ceph-version":"15.2.11-0","ceph_daemon_id":"0","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"0","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","0","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"f8e31292-e88c-48ef-9806-f45a7280254f"},{"name":"ROOK_OSD_ID","value":"0"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.0.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-0","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=0\nOSD_UUID=f8e31292-e88c-48ef-9806-f45a7280254f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-0","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-0","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sda","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "0"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "0"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "0"
      pod-template-hash: 56c4fc758d
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-0-56c4fc758d
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-0
      uid: bc19eb13-184d-489f-84df-dbef72814dca
    resourceVersion: "4377"
    uid: 4886b261-e1aa-41cf-a1df-39b6a6a3e92d
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "0"
        pod-template-hash: 56c4fc758d
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "0"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "0"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          osd: "0"
          pod-template-hash: 56c4fc758d
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "0"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: f8e31292-e88c-48ef-9806-f45a7280254f
          - name: ROOK_OSD_ID
            value: "0"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.0.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=0\nOSD_UUID=f8e31292-e88c-48ef-9806-f45a7280254f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-0
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sda
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"1","ceph-version":"15.2.11-0","ceph_daemon_id":"1","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"1","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-1","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"1","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"1","ceph-version":"15.2.11-0","ceph_daemon_id":"1","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"1","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","1","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"cbf296c6-1064-405f-b8a2-fd4ede68c69f"},{"name":"ROOK_OSD_ID","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.1.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-1","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=1\nOSD_UUID=cbf296c6-1064-405f-b8a2-fd4ede68c69f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-1","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-1","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sda","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "1"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "1"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "1"
      pod-template-hash: 6c7bbc6996
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-1-6c7bbc6996
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-1
      uid: df854435-76b7-4e2f-877f-3b496c80a12c
    resourceVersion: "4428"
    uid: 42738293-12f9-4480-80a7-598e5c25003d
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "1"
        pod-template-hash: 6c7bbc6996
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "1"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "1"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          osd: "1"
          pod-template-hash: 6c7bbc6996
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "1"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: cbf296c6-1064-405f-b8a2-fd4ede68c69f
          - name: ROOK_OSD_ID
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.1.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=1\nOSD_UUID=cbf296c6-1064-405f-b8a2-fd4ede68c69f\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-1
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sda
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"2","ceph-version":"15.2.11-0","ceph_daemon_id":"2","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"2","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-2","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"2","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"2","ceph-version":"15.2.11-0","ceph_daemon_id":"2","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"2","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","2","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"0794c043-8b9a-4600-9e56-e27358665ea5"},{"name":"ROOK_OSD_ID","value":"2"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.2.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-2","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=2\nOSD_UUID=0794c043-8b9a-4600-9e56-e27358665ea5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sda"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-2","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-2","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sda","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:25Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "2"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "2"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "2"
      pod-template-hash: 64c7556b68
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-2-64c7556b68
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-2
      uid: d07ee528-35c2-4c72-a258-ac2da6cb70ff
    resourceVersion: "4407"
    uid: bc550f2b-aea5-47f0-a90b-e7d050bdc7c9
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "2"
        pod-template-hash: 64c7556b68
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "2"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "2"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          osd: "2"
          pod-template-hash: 64c7556b68
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "2"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: 0794c043-8b9a-4600-9e56-e27358665ea5
          - name: ROOK_OSD_ID
            value: "2"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.2.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=2\nOSD_UUID=0794c043-8b9a-4600-9e56-e27358665ea5\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sda
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-2
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sda
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"3","ceph-version":"15.2.11-0","ceph_daemon_id":"3","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"3","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-3","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"3","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"3","ceph-version":"15.2.11-0","ceph_daemon_id":"3","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"3","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","3","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"0439f447-d0de-4e7d-8b78-8e112a4181ce"},{"name":"ROOK_OSD_ID","value":"3"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdb"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.3.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-3","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=3\nOSD_UUID=0439f447-d0de-4e7d-8b78-8e112a4181ce\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdb"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-3","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-3","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdb","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "3"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "3"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "3"
      pod-template-hash: 6c56cdc4cc
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-3-6c56cdc4cc
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-3
      uid: eae35e70-bb08-4d55-b654-f02f2fdf93a4
    resourceVersion: "4370"
    uid: c7fd58e7-cf00-4956-a75d-c006990bc7d3
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "3"
        pod-template-hash: 6c56cdc4cc
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "3"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "3"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          osd: "3"
          pod-template-hash: 6c56cdc4cc
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "3"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: 0439f447-d0de-4e7d-8b78-8e112a4181ce
          - name: ROOK_OSD_ID
            value: "3"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.3.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=3\nOSD_UUID=0439f447-d0de-4e7d-8b78-8e112a4181ce\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-3
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdb
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"4","ceph-version":"15.2.11-0","ceph_daemon_id":"4","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"4","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-4","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"4","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"4","ceph-version":"15.2.11-0","ceph_daemon_id":"4","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"4","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","4","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"a82f8fcd-16ea-47aa-9cf1-634534f103cc"},{"name":"ROOK_OSD_ID","value":"4"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdb"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.4.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-4","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=4\nOSD_UUID=a82f8fcd-16ea-47aa-9cf1-634534f103cc\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdb"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-4","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-4","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdb","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "4"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "4"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "4"
      pod-template-hash: 8fb498649
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-4-8fb498649
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-4
      uid: 5da42462-c070-4a38-a58d-6a37c80964db
    resourceVersion: "4453"
    uid: 8bebd2e0-7d90-4711-a3db-6ebf4f85d6b0
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "4"
        pod-template-hash: 8fb498649
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "4"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "4"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          osd: "4"
          pod-template-hash: 8fb498649
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "4"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: a82f8fcd-16ea-47aa-9cf1-634534f103cc
          - name: ROOK_OSD_ID
            value: "4"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.4.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-4
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=4\nOSD_UUID=a82f8fcd-16ea-47aa-9cf1-634534f103cc\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdb
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-4
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-4
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdb
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"5","ceph-version":"15.2.11-0","ceph_daemon_id":"5","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"5","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-5","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"5","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"5","ceph-version":"15.2.11-0","ceph_daemon_id":"5","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"5","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","5","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"ba3f17ca-a4d6-4a31-adbb-68c70a0a6196"},{"name":"ROOK_OSD_ID","value":"5"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.5.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-5","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=5\nOSD_UUID=ba3f17ca-a4d6-4a31-adbb-68c70a0a6196\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-5","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-5","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdc","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:25Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "5"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "5"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "5"
      pod-template-hash: 6df5f5cc94
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-5-6df5f5cc94
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-5
      uid: 17f19c59-7048-4926-9450-6921c06fb06e
    resourceVersion: "4401"
    uid: 04b6e554-879d-4f43-8797-b5ea83165933
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "5"
        pod-template-hash: 6df5f5cc94
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "5"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "5"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          osd: "5"
          pod-template-hash: 6df5f5cc94
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "5"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: ba3f17ca-a4d6-4a31-adbb-68c70a0a6196
          - name: ROOK_OSD_ID
            value: "5"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.5.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-5
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=5\nOSD_UUID=ba3f17ca-a4d6-4a31-adbb-68c70a0a6196\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-5
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-5
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdc
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"6","ceph-version":"15.2.11-0","ceph_daemon_id":"6","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"6","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-6","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"6","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"6","ceph-version":"15.2.11-0","ceph_daemon_id":"6","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","osd":"6","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","6","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"1e2abf51-b10e-4d8b-a409-989eed9ff755"},{"name":"ROOK_OSD_ID","value":"6"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.6.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-6","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=6\nOSD_UUID=1e2abf51-b10e-4d8b-a409-989eed9ff755\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-6","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-6","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdc","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "6"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "6"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      osd: "6"
      pod-template-hash: c97896bb6
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-6-c97896bb6
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-6
      uid: 65b23d19-3484-4eb5-af61-140981a03694
    resourceVersion: "4374"
    uid: f8a2a47a-5c61-4431-b633-d127eb6afd2e
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "6"
        pod-template-hash: c97896bb6
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "6"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "6"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          osd: "6"
          pod-template-hash: c97896bb6
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "6"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: 1e2abf51-b10e-4d8b-a409-989eed9ff755
          - name: ROOK_OSD_ID
            value: "6"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.6.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-6
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=6\nOSD_UUID=1e2abf51-b10e-4d8b-a409-989eed9ff755\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-6
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-6
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdc
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"7","ceph-version":"15.2.11-0","ceph_daemon_id":"7","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"7","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-7","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"7","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"7","ceph-version":"15.2.11-0","ceph_daemon_id":"7","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","osd":"7","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","7","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"a71d8128-94c0-494b-9e3a-e47b2341d1d7"},{"name":"ROOK_OSD_ID","value":"7"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.7.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-7","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=7\nOSD_UUID=a71d8128-94c0-494b-9e3a-e47b2341d1d7\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdc"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-7","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-7","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdc","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:24Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "7"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "7"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      osd: "7"
      pod-template-hash: 75b5fcc6b
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-7-75b5fcc6b
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-7
      uid: 859bf6a0-6b19-4e05-91d5-574a92a090e3
    resourceVersion: "4433"
    uid: 84f359c4-c1c3-4800-a612-d5ec2be24f57
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "7"
        pod-template-hash: 75b5fcc6b
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "7"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "7"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          osd: "7"
          pod-template-hash: 75b5fcc6b
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "7"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: a71d8128-94c0-494b-9e3a-e47b2341d1d7
          - name: ROOK_OSD_ID
            value: "7"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.7.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-7
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=7\nOSD_UUID=a71d8128-94c0-494b-9e3a-e47b2341d1d7\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdc
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-7
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-7
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdc
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      banzaicloud.com/last-applied: '{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"8","ceph-version":"15.2.11-0","ceph_daemon_id":"8","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"8","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd-8","namespace":"rook-ceph","ownerReferences":[{"apiVersion":"ceph.rook.io/v1","blockOwnerDeletion":true,"controller":true,"kind":"CephCluster","name":"rook-ceph","uid":"3d3233b3-387a-4444-85cf-8ae39d38e989"}]},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"rook-ceph-osd","ceph-osd-id":"8","rook_cluster":"rook-ceph"}},"strategy":{"type":"Recreate"},"template":{"metadata":{"labels":{"app":"rook-ceph-osd","ceph-osd-id":"8","ceph-version":"15.2.11-0","ceph_daemon_id":"8","ceph_daemon_type":"osd","failure-domain":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","osd":"8","portable":"false","rook-version":"v1.6.0-alpha.0.219.g512d77d","rook_cluster":"rook-ceph","topology-location-host":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs","topology-location-region":"fr2","topology-location-root":"default"},"name":"rook-ceph-osd"},"spec":{"affinity":{},"containers":[{"args":["--foreground","--id","8","--fsid","742018bb-f74c-4681-bfea-4ab962753821","--setuser","ceph","--setgroup","ceph","--crush-location=root=default
        host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs region=fr2","--log-to-stderr=true","--err-to-stderr=true","--mon-cluster-log-to-stderr=true","--log-stderr-prefix=debug
        ","--default-log-to-file=false","--default-mon-cluster-log-to-file=false","--ms-learn-addr-from-peer=false","--ms-bind-ipv4=true","--ms-bind-ipv6=false"],"command":["ceph-osd"],"env":[{"name":"ROOK_NODE_NAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"ROOK_CLUSTER_ID","value":"3d3233b3-387a-4444-85cf-8ae39d38e989"},{"name":"ROOK_CLUSTER_NAME","value":"rook-ceph"},{"name":"ROOK_PRIVATE_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"ROOK_PUBLIC_IP","valueFrom":{"fieldRef":{"fieldPath":"status.podIP"}}},{"name":"POD_NAMESPACE","value":"rook-ceph"},{"name":"ROOK_MON_ENDPOINTS","valueFrom":{"configMapKeyRef":{"key":"data","name":"rook-ceph-mon-endpoints"}}},{"name":"ROOK_MON_SECRET","valueFrom":{"secretKeyRef":{"key":"mon-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_USERNAME","valueFrom":{"secretKeyRef":{"key":"ceph-username","name":"rook-ceph-mon"}}},{"name":"ROOK_CEPH_SECRET","valueFrom":{"secretKeyRef":{"key":"ceph-secret","name":"rook-ceph-mon"}}},{"name":"ROOK_CONFIG_DIR","value":"/var/lib/rook"},{"name":"ROOK_CEPH_CONFIG_OVERRIDE","value":"/etc/rook/config/override.conf"},{"name":"ROOK_FSID","valueFrom":{"secretKeyRef":{"key":"fsid","name":"rook-ceph-mon"}}},{"name":"NODE_NAME","valueFrom":{"fieldRef":{"fieldPath":"spec.nodeName"}}},{"name":"ROOK_CRUSHMAP_ROOT","value":"default"},{"name":"ROOK_CRUSHMAP_HOSTNAME","value":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_OSDS_PER_DEVICE","value":"1"},{"name":"TINI_SUBREAPER"},{"name":"CONTAINER_IMAGE","value":"ceph/ceph:v15.2.11"},{"name":"POD_NAME","valueFrom":{"fieldRef":{"fieldPath":"metadata.name"}}},{"name":"POD_MEMORY_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"limits.memory"}}},{"name":"POD_MEMORY_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.memory"}}},{"name":"POD_CPU_LIMIT","valueFrom":{"resourceFieldRef":{"divisor":"1","resource":"limits.cpu"}}},{"name":"POD_CPU_REQUEST","valueFrom":{"resourceFieldRef":{"divisor":"0","resource":"requests.cpu"}}},{"name":"ROOK_OSD_UUID","value":"af0ac0df-b3d4-425b-ac37-b28f7e40a590"},{"name":"ROOK_OSD_ID","value":"8"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdd"},{"name":"ROOK_CV_MODE","value":"raw"}],"image":"ceph/ceph:v15.2.11","livenessProbe":{"exec":{"command":["env","-i","sh","-c","ceph
        --admin-daemon /run/ceph/ceph-osd.8.asok status"]},"initialDelaySeconds":45},"name":"osd","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-8","name":"activate-osd"}],"workingDir":"/var/log/ceph"}],"initContainers":[{"command":["/bin/bash","-c","\nset
        -ex\n\nOSD_ID=8\nOSD_UUID=af0ac0df-b3d4-425b-ac37-b28f7e40a590\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
        active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
        -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
        \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
        to a temporary directory\n\t# this is needed because when the init container
        exits, the tmpfs goes away and its content with it\n\t# this will result in
        the emptydir to be empty when accessed by the main osd container\n\tcp --verbose
        --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount the tmpfs
        since we don''t need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t# copy back
        the content of the tmpfs into the original osd directory\n\tcp --verbose --no-dereference
        \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain ownership of files to the ceph
        user/group\n\tchown --verbose --recursive ceph:ceph \"$OSD_DATA_DIR\"\n\n\t#
        remove the temporary directory\n\trm --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device
        ${DEVICE} --no-systemd --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
        ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
        ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore so
        we don''t need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate \"${ARGS[@]}\"\nfi\n\n"],"env":[{"name":"CEPH_VOLUME_DEBUG","value":"1"},{"name":"CEPH_VOLUME_SKIP_RESTORECON","value":"1"},{"name":"DM_DISABLE_UDEV","value":"1"},{"name":"ROOK_CEPH_MON_HOST","valueFrom":{"secretKeyRef":{"key":"mon_host","name":"rook-ceph-config"}}},{"name":"CEPH_ARGS","value":"-m
        $(ROOK_CEPH_MON_HOST)"},{"name":"ROOK_BLOCK_PATH","value":"/dev/sdd"},{"name":"ROOK_METADATA_DEVICE"},{"name":"ROOK_WAL_DEVICE"}],"image":"ceph/ceph:v15.2.11","name":"activate","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/ceph/osd/ceph-8","name":"activate-osd"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true}]},{"args":["--verbose","--recursive","ceph:ceph","/var/log/ceph","/var/lib/ceph/crash"],"command":["chown"],"image":"ceph/ceph:v15.2.11","name":"chown-container-data-dir","resources":{},"securityContext":{"privileged":true},"volumeMounts":[{"mountPath":"/var/lib/rook","name":"rook-data"},{"mountPath":"/etc/ceph","name":"rook-config-override","readOnly":true},{"mountPath":"/var/log/ceph","name":"rook-ceph-log"},{"mountPath":"/var/lib/ceph/crash","name":"rook-ceph-crash"},{"mountPath":"/dev","name":"devices"},{"mountPath":"/run/udev","name":"run-udev"},{"mountPath":"/var/lib/ceph/osd/ceph-8","name":"activate-osd"}]}],"nodeSelector":{"kubernetes.io/hostname":"kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs"},"restartPolicy":"Always","serviceAccountName":"rook-ceph-osd","volumes":[{"hostPath":{"path":"/var/lib/rook"},"name":"rook-data"},{"name":"rook-config-override","projected":{"sources":[{"configMap":{"items":[{"key":"config","mode":292,"path":"ceph.conf"}],"name":"rook-config-override"}}]}},{"hostPath":{"path":"/var/lib/rook/rook-ceph/log"},"name":"rook-ceph-log"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/crash"},"name":"rook-ceph-crash"},{"hostPath":{"path":"/dev"},"name":"devices"},{"hostPath":{"path":"/run/udev"},"name":"run-udev"},{"hostPath":{"path":"/var/lib/rook/rook-ceph/_dev_sdd","type":"DirectoryOrCreate"},"name":"activate-osd"}]}}},"status":{}}'
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "1"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:32:25Z"
    generation: 1
    labels:
      app: rook-ceph-osd
      ceph-osd-id: "8"
      ceph-version: 15.2.11-0
      ceph_daemon_id: "8"
      ceph_daemon_type: osd
      failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      osd: "8"
      pod-template-hash: 69865cc7ff
      portable: "false"
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
      topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
      topology-location-region: fr2
      topology-location-root: default
    name: rook-ceph-osd-8-69865cc7ff
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-osd-8
      uid: 43daa347-b4da-4501-9239-17a2ce38c1b6
    resourceVersion: "4404"
    uid: 7a9a6d7d-56f6-48e2-a6fa-a5b060efd353
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-osd
        ceph-osd-id: "8"
        pod-template-hash: 69865cc7ff
        rook_cluster: rook-ceph
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd
          ceph-osd-id: "8"
          ceph-version: 15.2.11-0
          ceph_daemon_id: "8"
          ceph_daemon_type: osd
          failure-domain: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          osd: "8"
          pod-template-hash: 69865cc7ff
          portable: "false"
          rook-version: v1.6.0-alpha.0.219.g512d77d
          rook_cluster: rook-ceph
          topology-location-host: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          topology-location-region: fr2
          topology-location-root: default
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --foreground
          - --id
          - "8"
          - --fsid
          - 742018bb-f74c-4681-bfea-4ab962753821
          - --setuser
          - ceph
          - --setgroup
          - ceph
          - --crush-location=root=default host=kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
            region=fr2
          - --log-to-stderr=true
          - --err-to-stderr=true
          - --mon-cluster-log-to-stderr=true
          - '--log-stderr-prefix=debug '
          - --default-log-to-file=false
          - --default-mon-cluster-log-to-file=false
          - --ms-learn-addr-from-peer=false
          - --ms-bind-ipv4=true
          - --ms-bind-ipv6=false
          command:
          - ceph-osd
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: TINI_SUBREAPER
          - name: CONTAINER_IMAGE
            value: ceph/ceph:v15.2.11
          - name: POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: POD_MEMORY_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: limits.memory
          - name: POD_MEMORY_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.memory
          - name: POD_CPU_LIMIT
            valueFrom:
              resourceFieldRef:
                divisor: "1"
                resource: limits.cpu
          - name: POD_CPU_REQUEST
            valueFrom:
              resourceFieldRef:
                divisor: "0"
                resource: requests.cpu
          - name: ROOK_OSD_UUID
            value: af0ac0df-b3d4-425b-ac37-b28f7e40a590
          - name: ROOK_OSD_ID
            value: "8"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdd
          - name: ROOK_CV_MODE
            value: raw
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - env
              - -i
              - sh
              - -c
              - ceph --admin-daemon /run/ceph/ceph-osd.8.asok status
            failureThreshold: 3
            initialDelaySeconds: 45
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          name: osd
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-8
            name: activate-osd
          workingDir: /var/log/ceph
        dnsPolicy: ClusterFirst
        initContainers:
        - command:
          - /bin/bash
          - -c
          - "\nset -ex\n\nOSD_ID=8\nOSD_UUID=af0ac0df-b3d4-425b-ac37-b28f7e40a590\nOSD_STORE_FLAG=\"--bluestore\"\nOSD_DATA_DIR=/var/lib/ceph/osd/ceph-\"$OSD_ID\"\nCV_MODE=raw\nDEVICE=\"$ROOK_BLOCK_PATH\"\nMETADATA_DEVICE=\"$ROOK_METADATA_DEVICE\"\nWAL_DEVICE=\"$ROOK_WAL_DEVICE\"\n\n#
            active the osd with ceph-volume\nif [[ \"$CV_MODE\" == \"lvm\" ]]; then\n\tTMP_DIR=$(mktemp
            -d)\n\n\t# activate osd\n\tceph-volume \"$CV_MODE\" activate --no-systemd
            \"$OSD_STORE_FLAG\" \"$OSD_ID\" \"$OSD_UUID\"\n\n\t# copy the tmpfs directory
            to a temporary directory\n\t# this is needed because when the init container
            exits, the tmpfs goes away and its content with it\n\t# this will result
            in the emptydir to be empty when accessed by the main osd container\n\tcp
            --verbose --no-dereference \"$OSD_DATA_DIR\"/* \"$TMP_DIR\"/\n\n\t# unmount
            the tmpfs since we don't need it anymore\n\tumount \"$OSD_DATA_DIR\"\n\n\t#
            copy back the content of the tmpfs into the original osd directory\n\tcp
            --verbose --no-dereference \"$TMP_DIR\"/* \"$OSD_DATA_DIR\"\n\n\t# retain
            ownership of files to the ceph user/group\n\tchown --verbose --recursive
            ceph:ceph \"$OSD_DATA_DIR\"\n\n\t# remove the temporary directory\n\trm
            --recursive --force \"$TMP_DIR\"\nelse\n\tARGS=(--device ${DEVICE} --no-systemd
            --no-tmpfs)\n\tif [ -n \"$METADATA_DEVICE\" ]; then\n\t\tARGS+=(--block.db
            ${METADATA_DEVICE})\n\tfi\n\tif [ -n \"$WAL_DEVICE\" ]; then\n\t\tARGS+=(--block.wal
            ${WAL_DEVICE})\n\tfi\n\t# ceph-volume raw mode only supports bluestore
            so we don't need to pass a store flag\n\tceph-volume \"$CV_MODE\" activate
            \"${ARGS[@]}\"\nfi\n\n"
          env:
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_CEPH_MON_HOST
            valueFrom:
              secretKeyRef:
                key: mon_host
                name: rook-ceph-config
          - name: CEPH_ARGS
            value: -m $(ROOK_CEPH_MON_HOST)
          - name: ROOK_BLOCK_PATH
            value: /dev/sdd
          - name: ROOK_METADATA_DEVICE
          - name: ROOK_WAL_DEVICE
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: activate
          resources: {}
          securityContext:
            privileged: true
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/ceph/osd/ceph-8
            name: activate-osd
          - mountPath: /dev
            name: devices
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
        - args:
          - --verbose
          - --recursive
          - ceph:ceph
          - /var/log/ceph
          - /var/lib/ceph/crash
          command:
          - chown
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: chown-container-data-dir
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: rook-config-override
            readOnly: true
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: run-udev
          - mountPath: /var/lib/ceph/osd/ceph-8
            name: activate-osd
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - name: rook-config-override
          projected:
            defaultMode: 420
            sources:
            - configMap:
                items:
                - key: config
                  mode: 292
                  path: ceph.conf
                name: rook-config-override
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: run-udev
        - hostPath:
            path: /var/lib/rook/rook-ceph/_dev_sdd
            type: DirectoryOrCreate
          name: activate-osd
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: ReplicaSet
  metadata:
    annotations:
      deployment.kubernetes.io/desired-replicas: "1"
      deployment.kubernetes.io/max-replicas: "2"
      deployment.kubernetes.io/revision: "1"
    creationTimestamp: "2021-05-03T13:28:39Z"
    generation: 1
    labels:
      app: rook-ceph-tools
      pod-template-hash: 78cdfd976c
    name: rook-ceph-tools-78cdfd976c
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: apps/v1
      blockOwnerDeletion: true
      controller: true
      kind: Deployment
      name: rook-ceph-tools
      uid: 7fa66cc3-7a79-41cf-831a-ad2c52a5c827
    resourceVersion: "3309"
    uid: 2bd47236-e82b-413a-b01c-3d4278380da0
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: rook-ceph-tools
        pod-template-hash: 78cdfd976c
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-tools
          pod-template-hash: 78cdfd976c
      spec:
        containers:
        - args:
          - -g
          - --
          - /usr/local/bin/toolbox.sh
          command:
          - /tini
          env:
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          image: rook/ceph:master
          imagePullPolicy: IfNotPresent
          name: rook-ceph-tools
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /etc/ceph
            name: ceph-config
          - mountPath: /etc/rook
            name: mon-endpoint-volume
        dnsPolicy: ClusterFirstWithHostNet
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        tolerations:
        - effect: NoExecute
          key: node.kubernetes.io/unreachable
          operator: Exists
          tolerationSeconds: 5
        volumes:
        - configMap:
            defaultMode: 420
            items:
            - key: data
              path: mon-endpoints
            name: rook-ceph-mon-endpoints
          name: mon-endpoint-volume
        - emptyDir: {}
          name: ceph-config
  status:
    availableReplicas: 1
    fullyLabeledReplicas: 1
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
- apiVersion: apps/v1
  kind: StatefulSet
  metadata:
    annotations:
      kubectl.kubernetes.io/last-applied-configuration: |
        {"apiVersion":"apps/v1","kind":"StatefulSet","metadata":{"annotations":{},"name":"postgresql","namespace":"default"},"spec":{"replicas":1,"selector":{"matchLabels":{"app":"postgresql"}},"serviceName":"postgresql","template":{"metadata":{"labels":{"app":"postgresql"}},"spec":{"containers":[{"env":[{"name":"POSTGRES_USER","value":"postgres"},{"name":"POSTGRES_DB","value":"klustered"},{"name":"POSTGRES_PASSWORD","value":"postgresql123"}],"image":"postgres:13-alpine","imagePullPolicy":"IfNotPresent","livenessProbe":{"exec":{"command":["/bin/sh","-c","exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432"]},"failureThreshold":2,"initialDelaySeconds":5,"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"name":"postgresql","ports":[{"containerPort":5432,"name":"psql"}],"readinessProbe":{"exec":{"command":["/bin/sh","-c","exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432"]},"failureThreshold":2,"initialDelaySeconds":5,"periodSeconds":5,"successThreshold":1,"timeoutSeconds":5},"volumeMounts":[{"mountPath":"/docker-entrypoint-initdb.d","name":"init"}]}],"volumes":[{"configMap":{"name":"postgresql"},"name":"init"}]}}}}
    creationTimestamp: "2021-05-03T13:28:46Z"
    generation: 1
    name: postgresql
    namespace: default
    resourceVersion: "2455"
    uid: 2c013aca-3caa-4ea4-8527-389419186036
  spec:
    podManagementPolicy: OrderedReady
    replicas: 1
    revisionHistoryLimit: 10
    selector:
      matchLabels:
        app: postgresql
    serviceName: postgresql
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: postgresql
      spec:
        containers:
        - env:
          - name: POSTGRES_USER
            value: postgres
          - name: POSTGRES_DB
            value: klustered
          - name: POSTGRES_PASSWORD
            value: postgresql123
          image: postgres:13-alpine
          imagePullPolicy: IfNotPresent
          livenessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
            failureThreshold: 2
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          name: postgresql
          ports:
          - containerPort: 5432
            name: psql
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -c
              - exec pg_isready -U "postgres" -h 127.0.0.1 -p 5432
            failureThreshold: 2
            initialDelaySeconds: 5
            periodSeconds: 5
            successThreshold: 1
            timeoutSeconds: 5
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /docker-entrypoint-initdb.d
            name: init
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - configMap:
            defaultMode: 420
            name: postgresql
          name: init
    updateStrategy:
      rollingUpdate:
        partition: 0
      type: RollingUpdate
  status:
    collisionCount: 0
    currentReplicas: 1
    currentRevision: postgresql-fff6bd679
    observedGeneration: 1
    readyReplicas: 1
    replicas: 1
    updateRevision: postgresql-fff6bd679
    updatedReplicas: 1
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2021-05-03T13:32:44Z"
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 15.2.11-0
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-6f643bc7106c83d1bb0a9274a2947bc0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4602"
    uid: 85d20e1d-7757-4238-a7a3-401aeeaa7366
  spec:
    backoffLimit: 6
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 85d20e1d-7757-4238-a7a3-401aeeaa7366
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          ceph.rook.io/pvc: ""
          controller-uid: 85d20e1d-7757-4238-a7a3-401aeeaa7366
          job-name: rook-ceph-osd-prepare-6f643bc7106c83d1bb0a9274a2947bc0
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --
          - /rook/rook
          - ceph
          - osd
          - provision
          command:
          - /rook/tini
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: all
          - name: ROOK_CEPH_VERSION
            value: ceph version 15.2.11-0 octopus
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: provision
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - copy-binaries
          - --copy-to-dir
          - /rook
          image: rook/ceph:master
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-9fz6f
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2021-05-03T13:32:48Z"
    conditions:
    - lastProbeTime: "2021-05-03T13:32:48Z"
      lastTransitionTime: "2021-05-03T13:32:48Z"
      status: "True"
      type: Complete
    startTime: "2021-05-03T13:32:44Z"
    succeeded: 1
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2021-05-03T13:32:48Z"
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 15.2.11-0
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-ceeab05259c8d89b0904c022bc5f8be0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4652"
    uid: 4b4bff43-200a-4b1d-b614-d503218be780
  spec:
    backoffLimit: 6
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 4b4bff43-200a-4b1d-b614-d503218be780
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          ceph.rook.io/pvc: ""
          controller-uid: 4b4bff43-200a-4b1d-b614-d503218be780
          job-name: rook-ceph-osd-prepare-ceeab05259c8d89b0904c022bc5f8be0
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --
          - /rook/rook
          - ceph
          - osd
          - provision
          command:
          - /rook/tini
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: all
          - name: ROOK_CEPH_VERSION
            value: ceph version 15.2.11-0 octopus
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: provision
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - copy-binaries
          - --copy-to-dir
          - /rook
          image: rook/ceph:master
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-frbvs
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2021-05-03T13:32:51Z"
    conditions:
    - lastProbeTime: "2021-05-03T13:32:51Z"
      lastTransitionTime: "2021-05-03T13:32:51Z"
      status: "True"
      type: Complete
    startTime: "2021-05-03T13:32:48Z"
    succeeded: 1
- apiVersion: batch/v1
  kind: Job
  metadata:
    creationTimestamp: "2021-05-03T13:32:46Z"
    labels:
      app: rook-ceph-osd-prepare
      ceph-version: 15.2.11-0
      rook-version: v1.6.0-alpha.0.219.g512d77d
      rook_cluster: rook-ceph
    name: rook-ceph-osd-prepare-fb5b167868730b399f51c41bfe43cdc0
    namespace: rook-ceph
    ownerReferences:
    - apiVersion: ceph.rook.io/v1
      blockOwnerDeletion: true
      controller: true
      kind: CephCluster
      name: rook-ceph
      uid: 3d3233b3-387a-4444-85cf-8ae39d38e989
    resourceVersion: "4640"
    uid: 287f95e9-0081-4b46-965e-33e4f7634769
  spec:
    backoffLimit: 6
    completions: 1
    parallelism: 1
    selector:
      matchLabels:
        controller-uid: 287f95e9-0081-4b46-965e-33e4f7634769
    template:
      metadata:
        creationTimestamp: null
        labels:
          app: rook-ceph-osd-prepare
          ceph.rook.io/pvc: ""
          controller-uid: 287f95e9-0081-4b46-965e-33e4f7634769
          job-name: rook-ceph-osd-prepare-fb5b167868730b399f51c41bfe43cdc0
          rook_cluster: rook-ceph
        name: rook-ceph-osd
      spec:
        affinity: {}
        containers:
        - args:
          - --
          - /rook/rook
          - ceph
          - osd
          - provision
          command:
          - /rook/tini
          env:
          - name: ROOK_NODE_NAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: ROOK_CLUSTER_ID
            value: 3d3233b3-387a-4444-85cf-8ae39d38e989
          - name: ROOK_CLUSTER_NAME
            value: rook-ceph
          - name: ROOK_PRIVATE_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: ROOK_PUBLIC_IP
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: status.podIP
          - name: POD_NAMESPACE
            value: rook-ceph
          - name: ROOK_MON_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                key: data
                name: rook-ceph-mon-endpoints
          - name: ROOK_MON_SECRET
            valueFrom:
              secretKeyRef:
                key: mon-secret
                name: rook-ceph-mon
          - name: ROOK_CEPH_USERNAME
            valueFrom:
              secretKeyRef:
                key: ceph-username
                name: rook-ceph-mon
          - name: ROOK_CEPH_SECRET
            valueFrom:
              secretKeyRef:
                key: ceph-secret
                name: rook-ceph-mon
          - name: ROOK_CONFIG_DIR
            value: /var/lib/rook
          - name: ROOK_CEPH_CONFIG_OVERRIDE
            value: /etc/rook/config/override.conf
          - name: ROOK_FSID
            valueFrom:
              secretKeyRef:
                key: fsid
                name: rook-ceph-mon
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: ROOK_CRUSHMAP_ROOT
            value: default
          - name: ROOK_CRUSHMAP_HOSTNAME
            value: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
          - name: CEPH_VOLUME_DEBUG
            value: "1"
          - name: CEPH_VOLUME_SKIP_RESTORECON
            value: "1"
          - name: DM_DISABLE_UDEV
            value: "1"
          - name: ROOK_OSDS_PER_DEVICE
            value: "1"
          - name: ROOK_LOG_LEVEL
            value: DEBUG
          - name: ROOK_DATA_DEVICE_FILTER
            value: all
          - name: ROOK_CEPH_VERSION
            value: ceph version 15.2.11-0 octopus
          - name: ROOK_OSD_CRUSH_DEVICE_CLASS
          - name: ROOK_OSD_CRUSH_INITIAL_WEIGHT
          image: ceph/ceph:v15.2.11
          imagePullPolicy: IfNotPresent
          name: provision
          resources: {}
          securityContext:
            privileged: true
            readOnlyRootFilesystem: false
            runAsNonRoot: false
            runAsUser: 0
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /var/lib/rook
            name: rook-data
          - mountPath: /etc/ceph
            name: ceph-conf-emptydir
          - mountPath: /var/log/ceph
            name: rook-ceph-log
          - mountPath: /var/lib/ceph/crash
            name: rook-ceph-crash
          - mountPath: /dev
            name: devices
          - mountPath: /run/udev
            name: udev
          - mountPath: /rook
            name: rook-binaries
          - mountPath: /rootfs
            name: rootfs
            readOnly: true
        dnsPolicy: ClusterFirst
        initContainers:
        - args:
          - copy-binaries
          - --copy-to-dir
          - /rook
          image: rook/ceph:master
          imagePullPolicy: IfNotPresent
          name: copy-bins
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          volumeMounts:
          - mountPath: /rook
            name: rook-binaries
        nodeSelector:
          kubernetes.io/hostname: kluster-022-krisnova-kubecon-eu-21-worker-a-d9glg
        restartPolicy: OnFailure
        schedulerName: default-scheduler
        securityContext: {}
        serviceAccount: rook-ceph-osd
        serviceAccountName: rook-ceph-osd
        terminationGracePeriodSeconds: 30
        volumes:
        - hostPath:
            path: /var/lib/rook
            type: ""
          name: rook-data
        - emptyDir: {}
          name: ceph-conf-emptydir
        - hostPath:
            path: /var/lib/rook/rook-ceph/log
            type: ""
          name: rook-ceph-log
        - hostPath:
            path: /var/lib/rook/rook-ceph/crash
            type: ""
          name: rook-ceph-crash
        - emptyDir: {}
          name: rook-binaries
        - hostPath:
            path: /dev
            type: ""
          name: devices
        - hostPath:
            path: /run/udev
            type: ""
          name: udev
        - hostPath:
            path: /
            type: ""
          name: rootfs
  status:
    completionTime: "2021-05-03T13:32:49Z"
    conditions:
    - lastProbeTime: "2021-05-03T13:32:49Z"
      lastTransitionTime: "2021-05-03T13:32:49Z"
      status: "True"
      type: Complete
    startTime: "2021-05-03T13:32:46Z"
    succeeded: 1
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
